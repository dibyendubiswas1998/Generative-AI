{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMVaSx5oo7fnP/I2ptfxo/7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **LLM Hallucination:**\n","In the context of Large Language Models (LLMs) like GPT, <u>**hallucinations** refer to instances when the model generates responses that are **factually incorrect**, **fabricated**, or **nonsensical**, **even though the response appears plausible and confident**.</u> These hallucinations often stem from the model's architecture and training process, which involve predicting the most likely next word in a sequence based on patterns in its training data.<br><br>\n","\n","**Why Do LLM Hallucination Happens:**<br>\n","\n","1. **Training Data Limitation:**\n","  * LLMs are trained on vast datasets from the internet, which may include incomplete, out-dated, or inaccurate information.\n","  * If the data is flawed, the model learns and reproduces those flaws.\n","\n","2. **Lack of True Understanding:**\n","  * LLMs generate text based on patterns and probabilities rather than a deep understanding of the words.\n","  * This statistical approach can lead to plausible-sounding but incorrect answers.\n","\n","3. **Prompt Ambiguity:**\n","  * If the input prompt is unclear or open-ended, the model might guess and generate hallucinated content.\n","\n","4. **Over Generation:**\n","  * LLMs are designed to generalize patterns from the training data. In doing so, they might generate content that doesn’t match the specific facts.\n","\n","5. **Context Truncation:**\n","  * If the input or conversational context exceeds the token limit, earlier information might be omitted, leading to incoherent or hallucinated responses.\n","<br><br>\n","\n","**How to Prevent LLM Hallucination:**<br>\n","1. **Use High Quality Data:** Generative AI models thrive on vast amounts of input data, but their outputs depend heavily on the **quality**, **relevance**, and **structure** of this data.<br>\n","  **Ex:** Consider training a language model to generate medical advice. If the dataset predominantly contains data about general health but lack specialized information on rare diseases, the model might generate plausible-sounding but incorrect advice for queries on those diseases.\n","\n","  Balanced datasets that cover a wide range of contexts and nuances equip the model to handle diverse inputs more effectively.\n","\n","2. **Data Templates:** An effective way to curb AI hallucinations is through data templates—structured guides that outline the expected format and permissible range of responses. By enforcing these predefined patterns, templates ensure consistency, accuracy, and adherence to domain-specific requirements.<br>\n","\n","  **Ex:** In financial reporting, templates might define the structure of balance sheets, including mandatory fields like assets, liabilities, and net income.\n","\n","3. **Parameter Tuning:** Fine-tuning inference parameters is a powerful, cost-effective way to refine the output of language models, allowing users to balance randomness, creativity, and consistency. By adjusting key settings like temperature, frequency penalty, presence penalty, and top P, you can achieve more tailored responses based on specific needs.<br>\n","\n","  **Ex:** For generating creative content like poems, setting a **high temperature** (**0.9**) and a **low frequency penalty** can produce imaginative outputs. Conversely, for technical documentation, a **low temperature** (**0.3**) and **higher frequency** penalty ensure factual accuracy and consistency.\n","\n","4. **Prompt Engineering:** Prompt engineering is a method of crafting precise and effective prompts to guide language models (LLMs) in generating accurate and relevant outputs. This is a cost effective approach to improving the quality of responses and mitigating issues like hallucinations and biases.\n","\n","5. **RAG & Tools-Agent:** Retrieval Augmented Generation (RAG) is a powerful technique that enhances a language model's ability to provide accurate answers by integrating additional, external knowledge. Tools also help to extract the real-world information.<br>\n","\n","  **Ex:** For a technical support chatbot, RAG allows the model to reference a product’s user manual to answer queries like \"How do I reset my password?\" instead of relying on generic training data.\n","\n","  By grounding responses in curated, domain-specific documentation, RAG reduces the influence of training data biases.\n","\n","6. **Human Fact Checking:** Despite the progress in AI, adding a human review layer is still one of the most reliable ways to prevent hallucinations. Human fact-checkers play a key role in identifying and correcting inaccuracies that the AI might miss, ensuring the accuracy of the output.<br> Human reviewers regularly assess AI-generated content, flagging errors or fabrications. This feedback is then used to refine the AI’s training data, improving its accuracy over time.\n","\n","  **Ex:** In a news generation system, human editors verify the facts before publishing to prevent the spread of false information."],"metadata":{"id":"i3gQ9oKXwb0-"}},{"cell_type":"markdown","source":["## **Parameter Tuning during LLM Calling:**"],"metadata":{"id":"9bz84lny4yln"}},{"cell_type":"markdown","source":["### **1. Temperature:**\n","\n","* **Definition:** A parameter that **controls the randomness of the model's output** by adjusting the probability distribution of token generation.\n","\n","\n","* **Range:** Typically between **`0`** and **`1`**.\n","  * Lower values (e.g., **`0.2`**) make the output more **deterministic** (focused on the most likely tokens).\n","  * Higher values (e.g., **`0.8`**) make the output more **diverse and creative**.\n","\n","\n","* **Effect of Hallucination:**\n","  * **Low Temperature:** Reduces hallucinations by focusing on the most probable answers.\n","\n","  * **High Temperature:** May increase hallucinations due to higher variability in token selection.\n"],"metadata":{"id":"YqoJOAJU9ukg"}},{"cell_type":"markdown","source":["### **2. Top_k:**\n","\n","\n","* **Definition:** Limits the number of tokens the model considers when generating the next token to the top **`k`** most probable tokens.\n","\n","\n","* **Range:** **`1`** to the vocabulary size (e.g., **`50`** or **`12000`**)\n","  * **Low k:** Considers only the most likely tokens.\n","  * **High k:** Allows the model to consider more tokens, increasing diversity.\n","\n","\n","* **Effect on Hallucinations:**\n","  * A **`low top_k`** value **reduces hallucinations** by narrowing the token choices to the most probable ones.\n","  * A **`high top_k`** can **cause hallucinations** by introducing less relevant or improbable tokens.\n","\n","\n","* **Example:**\n","  * **Query:** \"Who discovered gravity?\"\n","    * **Low Top_k (5):** \"Isaac Newton.\"\n","    * **High Top_k (50):** \"Isaac Newton, Galileo, or even Aristotle.\""],"metadata":{"id":"vzGimjXr_Xhg"}},{"cell_type":"markdown","source":["### **3. Top_p (Nucleus Sampling):**\n","\n","* **Definition:** Controls the probability distribution of token generation by considering the cumulative probability (**`p`**) of the top tokens. Limits token selection to a probability threshold, focusing on likely and relevant tokens.\n","\n","\n","* **Range:** **`0.0`** to **`1.0`**\n","  * **Low Top_p (e.g., `0.3`):** Considers only tokens that collectively have a high probability (<u>focused on the most certain answers</u>).\n","  * **High Top_p (e.g., `0.9`):** Expands token choices, allowing more **diversity**.\n","\n","\n","* **Effect on Hallucinations:**\n","  * **Low Top_p** reduces hallucinations by constraining token selection to highly probable options.\n","  * **High Top_p** may increase hallucinations by introducing more variability.\n","\n","\n","* **Example:**\n","  * **Query:** \"What is the capital of Germany?\"\n","    * **Low Top_p (`0.2`):** \"Berlin.\"\n","    * **High Top_p (`0.8`):** \"Berlin or possibly Frankfurt.\""],"metadata":{"id":"Yox0XXnWBNni"}},{"cell_type":"markdown","source":["### **4. Frequency Penalty:**\n","\n","* **Definition:** Penalizes tokens that appear frequently in the generated text, **reducing repetition**.\n","\n","\n","* **Range:** Typically between **`0.0`** and **`2.0`**.\n","  * **Higher Values:** Reduces the likelihood of repeating the same words or phrases.\n","\n","\n","* **Effect on Hallucination:**\n","  * Helps avoid repetitive hallucinations (e.g., repeating incorrect information multiple times).\n","\n","\n","* **Example:**\n","  * **Query:** \"Explain photosynthesis.\"\n","    * **No Penalty (`0.0`):** \"Photosynthesis is a process... photosynthesis is a process...\"\n","    * **High Penalty (`1.5`):** \"Photosynthesis is a process where plants convert sunlight into energy.\""],"metadata":{"id":"osClpfA0CykJ"}},{"cell_type":"markdown","source":["### **5. Presence Penalty:**\n","\n","* **Definition:** Penalizes tokens based on whether they have already appeared in the generated text, encouraging the introduction of new topics.\n","\n","\n","* **Range:** Typically between **`0.0`** and **`2.0`**.\n","  * **Higher Values:** Encourages more diverse content by reducing repetition.\n","\n","\n","* **Effect on Hallucination:**\n","  * Can reduce hallucinations caused by the model sticking to already mentioned but incorrect details.\n","\n","\n","* **Example:**\n","  * **Query:** \"Describe the Eiffel Tower.\"\n","    * **No Penalty (`0.0`):** \"The Eiffel Tower is a tower... The Eiffel Tower is a tower...\"\n","    * **High Penalty (`1.5`):** \"The Eiffel Tower is a landmark in Paris, known for its iron structure.\"\n"],"metadata":{"id":"rwDfMZPtD5SV"}},{"cell_type":"markdown","source":["### **Example of Combined Usage:**\n","\n","**Query:** \"Who discovered electricity?\"<br>\n","\n","* **Parameters:**\n","  * **Temperature:** **`0.2`** (deterministic output).\n","  * **Top_k:** **`5`** (only considers highly likely answers).\n","  * **Top_p:** **`0.3`** (restricts token selection to high-probability tokens).\n","  * **Frequency Penalty:** **`1.0`** (reduces repetitive phrases).\n","  * **Presence Penalty:** **`0.5`** (avoids excessive repetition of specific tokens).\n","\n","**Output:**<br>\n","\"Electricity was not discovered by a single individual. However, Benjamin Franklin's experiments in the 18th century contributed significantly to its understanding.\""],"metadata":{"id":"LtVTR9zkGGIm"}},{"cell_type":"markdown","source":["**Note:**<br>\n","When you load any **llm** then check the documentation of that libary."],"metadata":{"id":"sDn8wFArGzdp"}},{"cell_type":"code","source":[],"metadata":{"id":"3VXfxPp2wWpk","executionInfo":{"status":"ok","timestamp":1735017385535,"user_tz":-330,"elapsed":3,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RrtUErG_wWsQ"},"execution_count":null,"outputs":[]}]}