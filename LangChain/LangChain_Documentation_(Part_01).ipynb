{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LangChain**\n",
        "\n",
        "LangChain is an open-source framework that simplifies building applications powered by large language models (LLMs).  <br><br>\n",
        "\n",
        "**Here's why something like LangChain is helpful:**<br>\n",
        "* **LLMs are powerful but complex:** LLMs are incredibly good at understanding and generating human-like text, but working with them directly requires a lot of programming expertise. LangChain provides pre-built tools and abstractions that make it easier to interact with LLMs.\n",
        "\n",
        "* **Improves accuracy and customization:**LangChain offers functionalities like prompt templates and retrieval modules. These allow developers to fine-tune how LLMs are prompted and what information they access, ultimately leading to more accurate and relevant outputs.\n",
        "\n",
        "* **Faster development:** By providing building blocks like retrievers and parsers, LangChain saves developers time from writing complex code from scratch. This allows for faster experimentation and prototyping of LLM-based applications.\n",
        "\n",
        "* **Empowers a wider range of developers:** LangChain's abstractions make it easier for developers with less expertise to get started with building LLM applications. This opens up the field to a broader range of innovators."
      ],
      "metadata": {
        "id": "ZUkyZc2apYi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the necessary module or packages:\n",
        "\n",
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "Btt2Dn7_pWfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Model & Predict using LangChain:**"
      ],
      "metadata": {
        "id": "o0Ldp1lFhUuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LangChain with HuggingFace Hub:**\n"
      ],
      "metadata": {
        "id": "i5_VDe3_hdvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain import HuggingFaceHub\n",
        "\n",
        "HuggingFace_API_Token = userdata.get('huggingface_token_1') # get the HuggingFace API Tokens"
      ],
      "metadata": {
        "id": "InNAO7jopWiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 01**"
      ],
      "metadata": {
        "id": "k_EDJoEFiuWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model from HuggingFace Hub:\n",
        "llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", huggingfacehub_api_token=HuggingFace_API_Token)\n",
        "llm"
      ],
      "metadata": {
        "id": "Z57qPS15pWkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the output using llm model:\n",
        "\n",
        "input_text = \"Can you tell me about India?\"\n",
        "output = llm.invoke(input=input_text)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "a_efN3ZnpWnE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88adb46e-5238-4c74-e743-dc07a3987d57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "India is a country in the subcontinent of Asia .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 02**"
      ],
      "metadata": {
        "id": "_R2dJm72i0rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceHub(repo_id=\"facebook/bart-large-cnn\",\n",
        "                     huggingfacehub_api_token=HuggingFace_API_Token,\n",
        "                     model_kwargs={\"temperature\":0})\n",
        "\n",
        "# temperature 0 means perfectly get the accurate result.\n",
        "# More towared the 0 menas, perfectly get the accurate result or model is very safe it is not taking any bets.\n",
        "# More towared the 1 menas, not that much perfectly get the accurate result or it will take risk it might generate wrong output but it is very creative\n",
        "\n",
        "llm"
      ],
      "metadata": {
        "id": "qtEZIaDLpWpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"Prime Minister Narendra Modi today announced the development of \"Mission Divyastra\" -- an indigenously developed, highly advanced\n",
        "                weapons system that alters the country's geopolitic position. The Agni-5 MIRV missile, developed by the defence research centre\n",
        "                DRDO for over a decade, took its first flight today. The new weapon system has Multiple Independently Targetable Re-entry Vehicle (MIRV)\n",
        "                technology, which ensures that a single missile can deploy multiple war heads at different locations. This is a technology currently\n",
        "                possessed by a handful of nations and with its test, India has joined a select group of nations, said government sources.\n",
        "                Proud of our DRDO scientists for Mission Divyastra, the first flight test of indigenously developed Agni-5 missile with Multiple\n",
        "                Independently Targetable Re-entry Vehicle (MIRV) technology,' PM Modi posted on X, formerly Twitter.\"\"\"\n",
        "\n",
        "output = llm.invoke(input=input_text)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "8H0uj2e7pWrl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "835ef8f6-8858-49d3-d709-686e420c585f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Agni-5 MIRV missile, developed by the defence research centre DRDO, took its first flight today. The new weapon system has Multiple Independently Targetable Re-entry Vehicle (MIRV) technology, which ensures that a single missile can deploy multiple war heads at different locations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 03**"
      ],
      "metadata": {
        "id": "qX1nNZ6zlzAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\",\n",
        "                     huggingfacehub_api_token=HuggingFace_API_Token,\n",
        "                     model_kwargs={\"temperature\":0, \"max_length\":64})\n",
        "\n",
        "llm"
      ],
      "metadata": {
        "id": "YMJ8lX4RpWt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"I want to open a restaurant for Indian food. Suggest a fency 5 name for this.\"\n",
        "\n",
        "output = llm.invoke(input=input_text)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "9KykcIZtpWwQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "352ca513-7778-42d1-f857-79029b1374ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indian restaurant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 04**"
      ],
      "metadata": {
        "id": "zmLrhLjgmt67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\",\n",
        "                     huggingfacehub_api_token=HuggingFace_API_Token,\n",
        "                     model_kwargs={\"temperature\":0, \"max_length\":64})\n",
        "\n",
        "llm"
      ],
      "metadata": {
        "id": "8j0phvUQpW0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"translate English to German: How old are you?\"\n",
        "\n",
        "output = llm.invoke(input=input_text)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "zPxvBzi_pW3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d54ee018-e7a3-4217-8e17-1bd1826efc4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wie alte sind Sie?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LangChain with Gemini:**"
      ],
      "metadata": {
        "id": "InQGXhFgnw1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "id": "PPqaoDOSoTcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "Gemini_API_Key = userdata.get('gimini_api_key_1') # get the Gemini API Key"
      ],
      "metadata": {
        "id": "jTaMkGEIpW56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Get The List of Gemini Models Info.:**"
      ],
      "metadata": {
        "id": "hwiGSp_Yvup2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "genai.configure(api_key=Gemini_API_Key) # configure Gemini_API"
      ],
      "metadata": {
        "id": "5F_Ky7awvq5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for models in genai.list_models():\n",
        "  print(models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2170
        },
        "id": "3CkigZ8qv1s8",
        "outputId": "44aabc27-4500-4878-b9dd-8d1d4266dac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/chat-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 Chat (Legacy)',\n",
            "      description='A legacy text-only model optimized for chat conversations',\n",
            "      input_token_limit=4096,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
            "      temperature=0.25,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/text-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 (Legacy)',\n",
            "      description='A legacy model that understands text and generates text as an output',\n",
            "      input_token_limit=8196,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
            "      temperature=0.7,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/embedding-gecko-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding Gecko',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=1024,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
            "      temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      top_p=1.0,\n",
            "      top_k=1)\n",
            "Model(name='models/gemini-1.0-pro-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
            "      description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
            "                   'model that supports tuning.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
            "      temperature=0.9,\n",
            "      top_p=1.0,\n",
            "      top_k=1)\n",
            "Model(name='models/gemini-1.0-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Latest',\n",
            "      description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
            "                   'model.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      top_p=1.0,\n",
            "      top_k=1)\n",
            "Model(name='models/gemini-1.0-pro-vision-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description='The best image understanding model to handle a broad range of applications',\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/gemini-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      top_p=1.0,\n",
            "      top_k=1)\n",
            "Model(name='models/gemini-pro-vision',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description='The best image understanding model to handle a broad range of applications',\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/embedding-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding 001',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/aqa',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Model that performs Attributed Question Answering.',\n",
            "      description=('Model trained to return answers to questions that are grounded in provided '\n",
            "                   'sources, along with estimating answerable probability.'),\n",
            "      input_token_limit=7168,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateAnswer'],\n",
            "      temperature=0.2,\n",
            "      top_p=1.0,\n",
            "      top_k=40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print all Content Generation models:\n",
        "\n",
        "for models in genai.list_models():\n",
        "  if \"generateContent\" in models.supported_generation_methods:\n",
        "    print(models.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "3l46ZdY_v_c9",
        "outputId": "fbcf0c18-8092-4d26-e460-230e0b3542e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "HsT3k251vrvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 01**"
      ],
      "metadata": {
        "id": "FoPefs4mprl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=Gemini_API_Key) # load the gemini-pro model\n",
        "llm"
      ],
      "metadata": {
        "id": "Jr_dZTNJpW8j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf94c86f-9708-4e87-d9c4-9158d491bc88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGoogleGenerativeAI(model='gemini-pro', google_api_key=SecretStr('**********'), client= genai.GenerativeModel(\n",
              "   model_name='models/gemini-pro',\n",
              "   generation_config={}.\n",
              "   safety_settings={}\n",
              "))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Tell me something about India?\"\n",
        "output = llm.invoke(input=input_text)\n",
        "print(output.content)"
      ],
      "metadata": {
        "id": "nByBqvG8pW-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54de1c5c-6a42-4567-a5c7-45fe1cf96ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**General Information:**\n",
            "\n",
            "* **Official Name:** Republic of India\n",
            "* **Capital:** New Delhi\n",
            "* **Area:** 3.287 million square kilometers (7th largest country in the world)\n",
            "* **Population:** 1.4 billion (second most populous country in the world)\n",
            "* **Currency:** Indian Rupee (INR)\n",
            "* **Official Languages:** Hindi and English (22 scheduled languages in total)\n",
            "\n",
            "**History and Culture:**\n",
            "\n",
            "* **Ancient Civilization:** India is home to one of the oldest civilizations in the world, with roots in the Indus Valley Civilization (c. 3300-1700 BCE).\n",
            "* **Religious Diversity:** India is a melting pot of religions, including Hinduism, Buddhism, Jainism, Sikhism, Islam, Christianity, and Zoroastrianism.\n",
            "* **Rich Heritage:** India has a rich cultural heritage, with ancient traditions in art, music, dance, literature, and architecture.\n",
            "* **Independence:** India gained independence from British rule in 1947.\n",
            "\n",
            "**Geography:**\n",
            "\n",
            "* **Northern Mountains:** The Himalayas, the highest mountain range in the world, forms India's northern border.\n",
            "* **Central Plateau:** The Deccan Plateau covers much of central India.\n",
            "* **Coastal Plains:** India has long coastlines along the Arabian Sea to the west and the Bay of Bengal to the east.\n",
            "* **Major Rivers:** The Ganges, Indus, and Brahmaputra are the three major rivers of India.\n",
            "\n",
            "**Economy:**\n",
            "\n",
            "* **Emerging Market:** India is one of the fastest-growing major economies in the world.\n",
            "* **Services Sector:** The services sector, including IT, finance, and tourism, is a major driver of the economy.\n",
            "* **Agriculture:** India is a major producer of agricultural products such as rice, wheat, cotton, and sugar.\n",
            "* **Manufacturing:** India has a growing manufacturing sector, particularly in textiles, automobiles, and pharmaceuticals.\n",
            "\n",
            "**Other Notable Facts:**\n",
            "\n",
            "* **Cricket:** Cricket is the national sport of India and enjoys immense popularity.\n",
            "* **Bollywood:** India's film industry, Bollywood, is one of the largest and most prolific in the world.\n",
            "* **Space Program:** India has a successful space program and has launched numerous satellites and spacecraft.\n",
            "* **Yoga and Ayurveda:** India is the birthplace of yoga and Ayurveda, ancient systems of health and well-being.\n",
            "* **Diversity:** India is a diverse country with a wide range of languages, ethnicities, and cultures.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 02**"
      ],
      "metadata": {
        "id": "WBb781Rrqp89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"Prime Minister Narendra Modi today announced the development of \"Mission Divyastra\" -- an indigenously developed, highly advanced\n",
        "                weapons system that alters the country's geopolitic position. The Agni-5 MIRV missile, developed by the defence research centre\n",
        "                DRDO for over a decade, took its first flight today. The new weapon system has Multiple Independently Targetable Re-entry Vehicle (MIRV)\n",
        "                technology, which ensures that a single missile can deploy multiple war heads at different locations. This is a technology currently\n",
        "                possessed by a handful of nations and with its test, India has joined a select group of nations, said government sources.\n",
        "                Proud of our DRDO scientists for Mission Divyastra, the first flight test of indigenously developed Agni-5 missile with Multiple\n",
        "                Independently Targetable Re-entry Vehicle (MIRV) technology,' PM Modi posted on X, formerly Twitter.\n",
        "                Generate the summary within 5 lines.\n",
        "                \"\"\"\n",
        "\n",
        "output = llm.invoke(input=input_text)\n",
        "print(output.content)"
      ],
      "metadata": {
        "id": "vKIrgp2IpXBJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94344314-445c-4749-e56d-bbc156c3d1ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prime Minister Modi announced the development of \"Mission Divyastra,\" an indigenous weapons system that enhances India's geopolitical standing. The Agni-5 MIRV missile, developed by DRDO, successfully completed its maiden flight. The system incorporates Multiple Independently Targetable Re-entry Vehicle (MIRV) technology, allowing a single missile to deploy multiple warheads at different locations. India joins an exclusive group of nations with this technology, currently possessed by only a few. PM Modi expressed pride in DRDO scientists for this achievement.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 03**"
      ],
      "metadata": {
        "id": "zh8pz4ThsQ5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"How are you? translate into Bengali language\"\n",
        "output = llm.invoke(input=input_text)\n",
        "print(output.content)"
      ],
      "metadata": {
        "id": "MCuQiGB9pXDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d9406a-5459-4fb8-f9cd-3cd9749745b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "তুমি কেমন আছ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LangChain with OpenAI:**"
      ],
      "metadata": {
        "id": "4IZmEqgsspOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "OpenAI_API_Key = userdata.get('open_ai_api_key_1') # get the OpenAI key"
      ],
      "metadata": {
        "id": "0ss0OHwwpXGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Get The List of OpenAI models:**"
      ],
      "metadata": {
        "id": "LCK9gyKvvMqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "oH8T2DCDunfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=OpenAI_API_Key)\n",
        "client"
      ],
      "metadata": {
        "id": "dSnfNPlipXKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8797ac06-dba0-4feb-b4c7-5f19c4508893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<openai.OpenAI at 0x7b609bcba140>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.models.list().data"
      ],
      "metadata": {
        "id": "90F6NBKgpXQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b6e6d29-e864-4c4f-a46b-800f7800549d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(id='gpt-3.5-turbo-16k-0613', created=1685474247, object='model', owned_by='openai'),\n",
              " Model(id='dall-e-3', created=1698785189, object='model', owned_by='system'),\n",
              " Model(id='text-embedding-3-large', created=1705953180, object='model', owned_by='system'),\n",
              " Model(id='dall-e-2', created=1698798177, object='model', owned_by='system'),\n",
              " Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal'),\n",
              " Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system'),\n",
              " Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'),\n",
              " Model(id='gpt-3.5-turbo-0125', created=1706048358, object='model', owned_by='system'),\n",
              " Model(id='text-embedding-3-small', created=1705948997, object='model', owned_by='system'),\n",
              " Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'),\n",
              " Model(id='gpt-3.5-turbo-0613', created=1686587434, object='model', owned_by='openai'),\n",
              " Model(id='gpt-3.5-turbo-0301', created=1677649963, object='model', owned_by='openai'),\n",
              " Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system'),\n",
              " Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal'),\n",
              " Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system'),\n",
              " Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal'),\n",
              " Model(id='davinci-002', created=1692634301, object='model', owned_by='system'),\n",
              " Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system'),\n",
              " Model(id='babbage-002', created=1692634615, object='model', owned_by='system'),\n",
              " Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system'),\n",
              " Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "openai_models = client.models.list().data\n",
        "pd.DataFrame(openai_models, columns=['id', 'created', 'object', 'owned_by'])"
      ],
      "metadata": {
        "id": "_7qimUAgpXSg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "outputId": "a8c221fe-ee4f-445f-ba7b-e2a7fdf1091e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                   id                created           object  \\\n",
              "0        (id, gpt-3.5-turbo-16k-0613)  (created, 1685474247)  (object, model)   \n",
              "1                      (id, dall-e-3)  (created, 1698785189)  (object, model)   \n",
              "2        (id, text-embedding-3-large)  (created, 1705953180)  (object, model)   \n",
              "3                      (id, dall-e-2)  (created, 1698798177)  (object, model)   \n",
              "4                     (id, whisper-1)  (created, 1677532384)  (object, model)   \n",
              "5                 (id, tts-1-hd-1106)  (created, 1699053533)  (object, model)   \n",
              "6                      (id, tts-1-hd)  (created, 1699046015)  (object, model)   \n",
              "7            (id, gpt-3.5-turbo-0125)  (created, 1706048358)  (object, model)   \n",
              "8        (id, text-embedding-3-small)  (created, 1705948997)  (object, model)   \n",
              "9                 (id, gpt-3.5-turbo)  (created, 1677610602)  (object, model)   \n",
              "10           (id, gpt-3.5-turbo-0613)  (created, 1686587434)  (object, model)   \n",
              "11           (id, gpt-3.5-turbo-0301)  (created, 1677649963)  (object, model)   \n",
              "12           (id, gpt-3.5-turbo-1106)  (created, 1698959748)  (object, model)   \n",
              "13            (id, gpt-3.5-turbo-16k)  (created, 1683758102)  (object, model)   \n",
              "14  (id, gpt-3.5-turbo-instruct-0914)  (created, 1694122472)  (object, model)   \n",
              "15                        (id, tts-1)  (created, 1681940951)  (object, model)   \n",
              "16                  (id, davinci-002)  (created, 1692634301)  (object, model)   \n",
              "17       (id, gpt-3.5-turbo-instruct)  (created, 1692901427)  (object, model)   \n",
              "18                  (id, babbage-002)  (created, 1692634615)  (object, model)   \n",
              "19                   (id, tts-1-1106)  (created, 1699053241)  (object, model)   \n",
              "20       (id, text-embedding-ada-002)  (created, 1671217299)  (object, model)   \n",
              "\n",
              "                       owned_by  \n",
              "0            (owned_by, openai)  \n",
              "1            (owned_by, system)  \n",
              "2            (owned_by, system)  \n",
              "3            (owned_by, system)  \n",
              "4   (owned_by, openai-internal)  \n",
              "5            (owned_by, system)  \n",
              "6            (owned_by, system)  \n",
              "7            (owned_by, system)  \n",
              "8            (owned_by, system)  \n",
              "9            (owned_by, openai)  \n",
              "10           (owned_by, openai)  \n",
              "11           (owned_by, openai)  \n",
              "12           (owned_by, system)  \n",
              "13  (owned_by, openai-internal)  \n",
              "14           (owned_by, system)  \n",
              "15  (owned_by, openai-internal)  \n",
              "16           (owned_by, system)  \n",
              "17           (owned_by, system)  \n",
              "18           (owned_by, system)  \n",
              "19           (owned_by, system)  \n",
              "20  (owned_by, openai-internal)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fca7a32e-1b4f-4032-b073-e3882fbf284e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>created</th>\n",
              "      <th>object</th>\n",
              "      <th>owned_by</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(id, gpt-3.5-turbo-16k-0613)</td>\n",
              "      <td>(created, 1685474247)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, openai)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(id, dall-e-3)</td>\n",
              "      <td>(created, 1698785189)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, system)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(id, text-embedding-3-large)</td>\n",
              "      <td>(created, 1705953180)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, system)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(id, dall-e-2)</td>\n",
              "      <td>(created, 1698798177)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, system)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(id, whisper-1)</td>\n",
              "      <td>(created, 1677532384)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, openai-internal)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>(id, tts-1-hd-1106)</td>\n",
              "      <td>(created, 1699053533)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, system)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>(id, tts-1-hd)</td>\n",
              "      <td>(created, 1699046015)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, system)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>(id, gpt-3.5-turbo-0125)</td>\n",
              "      <td>(created, 1706048358)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, system)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>(id, text-embedding-3-small)</td>\n",
              "      <td>(created, 1705948997)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, system)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>(id, gpt-3.5-turbo)</td>\n",
              "      <td>(created, 1677610602)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, openai)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>(id, gpt-3.5-turbo-0613)</td>\n",
              "      <td>(created, 1686587434)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, openai)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>(id, gpt-3.5-turbo-0301)</td>\n",
              "      <td>(created, 1677649963)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, openai)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>(id, gpt-3.5-turbo-1106)</td>\n",
              "      <td>(created, 1698959748)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, system)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>(id, gpt-3.5-turbo-16k)</td>\n",
              "      <td>(created, 1683758102)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, openai-internal)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>(id, gpt-3.5-turbo-instruct-0914)</td>\n",
              "      <td>(created, 1694122472)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, system)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>(id, tts-1)</td>\n",
              "      <td>(created, 1681940951)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, openai-internal)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>(id, davinci-002)</td>\n",
              "      <td>(created, 1692634301)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, system)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>(id, gpt-3.5-turbo-instruct)</td>\n",
              "      <td>(created, 1692901427)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, system)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>(id, babbage-002)</td>\n",
              "      <td>(created, 1692634615)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, system)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>(id, tts-1-1106)</td>\n",
              "      <td>(created, 1699053241)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, system)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>(id, text-embedding-ada-002)</td>\n",
              "      <td>(created, 1671217299)</td>\n",
              "      <td>(object, model)</td>\n",
              "      <td>(owned_by, openai-internal)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fca7a32e-1b4f-4032-b073-e3882fbf284e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fca7a32e-1b4f-4032-b073-e3882fbf284e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fca7a32e-1b4f-4032-b073-e3882fbf284e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-02081f5f-096b-4559-b457-6d1ba2d9993d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-02081f5f-096b-4559-b457-6d1ba2d9993d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-02081f5f-096b-4559-b457-6d1ba2d9993d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 21,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21,\n        \"samples\": [\n          [\n            \"id\",\n            \"gpt-3.5-turbo-16k-0613\"\n          ],\n          [\n            \"id\",\n            \"gpt-3.5-turbo-instruct\"\n          ],\n          [\n            \"id\",\n            \"tts-1\"\n          ]\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"created\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21,\n        \"samples\": [\n          [\n            \"created\",\n            1685474247\n          ],\n          [\n            \"created\",\n            1692901427\n          ],\n          [\n            \"created\",\n            1681940951\n          ]\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"object\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          [\n            \"object\",\n            \"model\"\n          ]\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"owned_by\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          [\n            \"owned_by\",\n            \"openai\"\n          ]\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "43kf0rqupXaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PromptTemplate:**\n",
        "\n",
        "**Definition:**<br>\n",
        "**Prompt templates are predefined recipes for generating prompts for language models.** A PromptTemplate in LangChain is like that instruction for a large language model. It's a pre-built message with blanks that you can fill in to tell the LLM exactly what you want it to do.<br>\n",
        "\n",
        "**For instance, a simple template might be:** \"Write a story about a brave **{animal}** who goes on an adventure.\" You could replace \"**{animal}**\" with \"dog\", \"cat\", or anything else to create different stories.<br>\n",
        "\n",
        "Currently in the above applications we are writing an entire prompt, if you are creating a user directed application then this is not an ideal case. LangChain faciliates prompt management and optimization. **Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you need to take the user input and construct a prompt, and only then send that to the LLM.**\n",
        "<br><br>\n",
        "\n",
        "**Here's why PromptTemplates are helpful:**<br>\n",
        "* **Easy to understand:** They make it clear what you want the LLM to do.\n",
        "* **Flexible:** You can change the details to fit your needs.\n",
        "* **Less mistakes:** Using a template reduces types and errors in your instructions.\n",
        "* **Faster work:** You don't have to write the whole message every time.\n",
        "<br><br>\n",
        "\n",
        "**LangChain offers two main types of PromptTemplates:**<br>\n",
        "* **PromptTemplate:** This is for general prompts where you might have instructions or context for the LLM.\n",
        "* **ChatPromptTemplate:** This is specifically designed for crafting prompts within a chat-like conversation.\n",
        "<br><br>\n",
        "\n"
      ],
      "metadata": {
        "id": "PxI_5Y0QwaPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 01**"
      ],
      "metadata": {
        "id": "TWeWLQYJ1ZD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These examples is very much convenient way:\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
        ")\n",
        "p = prompt_template_name.format(cuisine=\"Italian\")\n",
        "print(p)"
      ],
      "metadata": {
        "id": "xxkjLyt2pXhM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799208ba-3113-451b-b980-4c9119360080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want to open a restaurant for Italian food. Suggest a fency name for this.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 02**"
      ],
      "metadata": {
        "id": "uVdHbZ5n1xAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\"Give me some {company_name} products name.\")\n",
        "prompt_template.format(company_name=\"Apple\")"
      ],
      "metadata": {
        "id": "PPB4VEAypXkf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1c1ecff5-9444-4130-db19-b25df3748539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Give me some Apple products name.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uJwOy3R-2TpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Chain:**\n",
        "\n",
        "\n",
        "In LangChain, a Chain is the core concept that ties everything together. It acts like a recipe that outlines the specific steps to be taken for a particular task.<br><br>\n",
        "\n",
        "**What is a Chain?**<br>\n",
        "Imagine a recipe for a cake. It outlines the ingredients and steps needed to bake it. Similarly, **a Chain defines a sequence of actions**, which can involve:<br>\n",
        "\n",
        "* Calling a Large Language Model (LLM) with a specific prompt.\n",
        "* Retrieving information from a data source.\n",
        "* Processing information using built-in LangChain tools.\n",
        "* Performing other actions as needed.\n",
        "<br><br>\n",
        "\n",
        "**Chains are essential in LangChain for several reasons:**<br>\n",
        "* **Structured Workflows:** Chains provide a clear and organized way to define the steps involved in a task. This makes your code more readable and maintainable.\n",
        "\n",
        "* **Modular Design:** By breaking down tasks into smaller steps within a Chain, you can easily reuse components for different applications.\n",
        "\n",
        "* **Flexibility and Customization:** Chains allow you to combine different functionalities to create complex workflows tailored to your specific needs.\n",
        "\n",
        "* **Improved Efficiency:** Pre-built chain functionalities save you time from writing complex code from scratch.\n",
        "<br><br>\n",
        "\n",
        "**LangChain offers two main types of Chains:**<br>\n",
        "* **LLM Chains:** These focus on interacting with one or more LLMs for tasks like text generation, translation, or summarization.\n",
        "\n",
        "* **Utility Chains:** These are more specialized and might involve a combination of LLMs, data processing steps, and other tools to achieve a specific goal.\n",
        "<br><br>\n",
        "\n",
        "**Note:**\n",
        "* Chain is multi-step workflows.\n",
        "* Using Chains we will link together model and the PromptTemplate and other Chains.\n",
        "* The simplest and most common type of Chain is LLMChain, which passes the input first to Prompt Template and then to Large Language Model.\n",
        "* LLMChain is responsible to execute the PromptTemplate, For every PromptTemplate we will specifically have an LLMChain.\n"
      ],
      "metadata": {
        "id": "1mUbuoT93Oea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LLMChain:**"
      ],
      "metadata": {
        "id": "MWNJ2dcWExPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=Gemini_API_Key) # load the gemini-pro model\n",
        "llm"
      ],
      "metadata": {
        "id": "En_PS7f767FD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf51d3f-7e37-4ef2-e2cb-f3b2c9bf7ba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGoogleGenerativeAI(model='gemini-pro', google_api_key=SecretStr('**********'), client= genai.GenerativeModel(\n",
              "   model_name='models/gemini-pro',\n",
              "   generation_config={}.\n",
              "   safety_settings={}\n",
              "))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 01**"
      ],
      "metadata": {
        "id": "roqyfMNMBjQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the PromptTemplate:\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "get_cuisine_template = PromptTemplate(\n",
        "    input_variables=['cuisine'],\n",
        "    template=\"Please give top 5 {cuisine} food.\"\n",
        ")"
      ],
      "metadata": {
        "id": "R3pn0eDQAL-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a chain:\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=get_cuisine_template)\n",
        "response = chain.invoke('Indian')\n",
        "print(\"Cuisine Name: \", response['cuisine'], \"\\n\")\n",
        "\n",
        "print(\"List of Cuisines:\")\n",
        "print(response['text'])"
      ],
      "metadata": {
        "id": "GAqvNvHHAMBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c13a256-1fd8-4e80-a96d-f4643ad973d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuisine Name:  Indian \n",
            "\n",
            "List of Cuisines:\n",
            "1. Butter Chicken\n",
            "2. Chicken Tikka Masala\n",
            "3. Palak Paneer\n",
            "4. Biryani\n",
            "5. Aloo Gobi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 02**"
      ],
      "metadata": {
        "id": "6ccaOwfND9ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=llm, prompt=get_cuisine_template, verbose=True) # verbose=True, means get info step wise, how chain is work\n",
        "response = chain.invoke('Bengali')\n",
        "print(\"Cuisine Name: \", response['cuisine'], \"\\n\")\n",
        "\n",
        "print(\"List of Cuisines:\")\n",
        "print(response['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyh3THqBCYe1",
        "outputId": "52fc0a9e-7a2b-41c0-d2c2-1108fb688b58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mPlease give top 5 Bengali food.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Cuisine Name:  Bengali \n",
            "\n",
            "List of Cuisines:\n",
            "1. **Machher Jhol (Fish Curry)**: A classic Bengali dish featuring tender fish cooked in a flavorful curry made with mustard seeds, turmeric, and other spices.\n",
            "2. **Bhapa Ilish (Steamed Hilsa)**: A delicate and aromatic dish where hilsa fish is steamed in a banana leaf with mustard oil, green chilies, and ginger.\n",
            "3. **Kosha Mangsho (Mutton Curry)**: A rich and flavorful mutton curry cooked slowly in a blend of spices, yogurt, and potatoes.\n",
            "4. **Luchi and Alu Dom (Fried Flatbread and Potato Curry)**: A popular Bengali breakfast or snack, consisting of crispy fried flatbread served with a spicy potato curry.\n",
            "5. **Mishti Doi (Sweet Yogurt)**: A creamy and velvety dessert made from sweetened yogurt, often flavored with jaggery or date palm sugar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Simple Sequential Chain:**\n",
        "\n",
        "**Definition:**<br>\n",
        "In LangChain, a Simple Sequential Chain is the most basic building block for building workflows.<br>\n",
        "\n",
        "Imagine a simple assembly line where one task follows another in a straight line. A Simple Sequential Chain works similarly. **It's a series of steps where the output from one step becomes the input for the next.**<br><br>\n",
        "\n",
        "**Each step typically involves:**<br>\n",
        "* **A single input:** This could be text, data, or instructions for the LLM.\n",
        "\n",
        "* **A single output:** This is the result of the step, which becomes the input for the next step in the chain.\n",
        "<br><br>\n",
        "\n",
        "**For example, a simple chain might involve:**<br>\n",
        "1. Taking a user query as input.\n",
        "2. Using that query to prompt an LLM for relevant information.\n",
        "3. Processing the LLM response and presenting it to the user.\n",
        "<br><br>\n",
        "\n",
        "**Pros of Simple Sequential Chains:**<br>\n",
        "* **Easy to understand and use:** Simple structure makes them ideal for beginners or for tasks with a linear flow.\n",
        "\n",
        "* **Efficient for linear workflows:** They're perfect for tasks where each step directly builds upon the previous one.\n",
        "\n",
        "* **Easy to debug:** Since the flow is straightforward, it's simpler to identify and fix any issues within the chain.\n",
        "<br><br>\n",
        "\n",
        "**Conos of Simple Sequential Chains:**<br>\n",
        "* **Limited complexity:** They can't handle complex workflows with branching logic or multiple inputs/outputs. **There is a issue with SimpleSequentialChain it only shows last input information.**\n",
        "\n",
        "* **Less flexibility:** They offer limited options for handling errors or unexpected situations within the chain.\n",
        "\n",
        "* **Potential redundancy:** For some tasks, there might be more efficient chain structures available."
      ],
      "metadata": {
        "id": "g_BYfhIsE2Hf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 01**"
      ],
      "metadata": {
        "id": "UUy8BH6-Izen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=Gemini_API_Key) # load the gemini-pro model\n",
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tjd7yOwZCYho",
        "outputId": "0c4568bc-71b3-4bbe-ace5-facca6bd0b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGoogleGenerativeAI(model='gemini-pro', google_api_key=SecretStr('**********'), client= genai.GenerativeModel(\n",
              "   model_name='models/gemini-pro',\n",
              "   generation_config={}.\n",
              "   safety_settings={}\n",
              "))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create prompt for getting unique restaurant name:\n",
        "get_restaurant_template = PromptTemplate(\n",
        "    input_variables = ['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest name for this.\"\n",
        ")\n",
        "restaurant_chain = LLMChain(llm=llm, prompt=get_restaurant_template)\n",
        "\n",
        "\n",
        "# Now create prompt for getting unique menu for those restaurant:\n",
        "get_menu_template = PromptTemplate(\n",
        "    input_variables = ['restaurant_name'],\n",
        "    template = \"Suggest some menu items for {restaurant_name}\"\n",
        ")\n",
        "menu_chain = LLMChain(llm=llm, prompt=get_menu_template)\n",
        "\n",
        "\n",
        "# now sequentialy access those chains:\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "chain = SimpleSequentialChain(chains=[restaurant_chain, menu_chain])\n",
        "\n",
        "response = chain.invoke(\"Indian\")"
      ],
      "metadata": {
        "id": "q-NTx0MlEeUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cuisine: \", response['input'])\n",
        "print(\"Restaurant Name:\")\n",
        "print(response['output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATmZ9zBtEeXX",
        "outputId": "dd329c9a-e433-471a-98da-4d10371e12b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuisine:  Indian\n",
            "Restaurant Name:\n",
            "**Traditional and Cultural:**\n",
            "\n",
            "* **Rasoi's Royal Thali:** A traditional Indian platter featuring various curries, rice, and breads.\n",
            "* **Dastarkhwan Delights:** A selection of kebabs, curries, and vegetarian dishes served on a traditional tablecloth.\n",
            "* **Tandoori Temptation:** An array of tandoori-cooked meats, vegetables, and breads.\n",
            "* **Chulha's Rustic Flavors:** Home-style curries and breads cooked on a traditional clay stove.\n",
            "* **Masala Magic:** A symphony of spices and flavors showcased in curries, chutneys, and pickles.\n",
            "\n",
            "**Regional and Specific:**\n",
            "\n",
            "* **Punjabi Dhaba Feast:** A hearty spread of Punjabi dishes like butter chicken, dal makhani, and tandoori roti.\n",
            "* **Hyderabadi Biryani Extravaganza:** A selection of fragrant and flavorful biryani dishes from the Nizami kitchen.\n",
            "* **Chettinad Curry House Classics:** A fiery and aromatic menu featuring Chettinad specialties like chicken 65 and mutton sukka.\n",
            "* **Kolkata Kathi Roll Carnival:** A street food extravaganza offering a variety of Kolkata's beloved kathi rolls.\n",
            "* **Kerala Coastal Kitchen Voyage:** A seafood-centric menu showcasing the vibrant flavors of Kerala's coastal cuisine.\n",
            "\n",
            "**Modern and Creative:**\n",
            "\n",
            "* **Indian Fusion Rhapsody:** A harmonious blend of traditional Indian flavors with contemporary culinary techniques.\n",
            "* **Spice Route Odyssey:** A culinary journey exploring the spices and flavors of the ancient spice trade.\n",
            "* **Curryology Symphony:** An exploration of different curry styles and their cultural influences.\n",
            "* **Tiffin Time Tradition with a Twist:** Lunch boxes filled with modern interpretations of classic Indian dishes.\n",
            "* **Naan Stop Indulgence:** A creative selection of naans featuring unique toppings and fillings.\n",
            "\n",
            "**Unique and Memorable:**\n",
            "\n",
            "* **The Saffron Way:** A menu infused with the aromatic and vibrant flavors of saffron.\n",
            "* **Turmeric Trail:** A journey through the medicinal and culinary properties of turmeric.\n",
            "* **Ginger & Jaggery:** A sweet and spicy combination that tantalizes the taste buds.\n",
            "* **Cumin & Coriander:** A classic duo that forms the backbone of many Indian dishes.\n",
            "* **The Spice Merchant:** A bazaar of exotic spices and their culinary applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sequential Chain:**\n",
        "\n",
        "\n",
        "**Definition:**<br>\n",
        "A Sequential Chain is a step up in complexity compared to a Simple Sequential Chain. It allows you to create workflows with more intricate data flow. Imagine a multi-lane highway where information can travel on different lanes but still progresses in a specific order.\n",
        "<br><br>\n",
        "\n",
        "**Here's what a Sequential Chain can handle:**<br>\n",
        "* **Multiple Inputs:** You can feed the chain with data from various sources, like user queries, retrieved information, or previous chain outputs.\n",
        "\n",
        "* **Multiple Outputs:** The chain can produce different outputs simultaneously, allowing for more informative results.\n",
        "\n",
        "* **Sub-Chains:** You can even integrate smaller, Simple Sequential Chains within a larger Sequential Chain for modularity.\n",
        "<br><br>\n",
        "\n",
        "**Pros of Sequential Chains:**<br>\n",
        "* **Increased Flexibility:** They enable you to design more complex workflows with branching logic or conditional actions based on intermediate outputs.\n",
        "\n",
        "* **Better Data Handling:** The ability to handle multiple inputs and outputs allows for richer processing and analysis of information.\n",
        "\n",
        "* **Modular Design:** By incorporating sub-chains, you can break down complex tasks into smaller, reusable components.\n",
        "<br><br>\n",
        "\n",
        "**Cons of Sequential Chains:**<br>\n",
        "* **Steeper Learning Curve:** Understanding and building these chains requires a bit more programming knowledge compared to Simple Sequential Chains.\n",
        "\n",
        "* **Increased Debugging Complexity:** With more moving parts, troubleshooting issues within the chain might take more effort.\n",
        "\n",
        "* **Potential for Over-engineering:** For simpler tasks, a Sequential Chain might be overkill, and a Simple Sequential Chain could be sufficient.\n",
        "\n"
      ],
      "metadata": {
        "id": "o0Y8gqykN_hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 01**"
      ],
      "metadata": {
        "id": "DZ43v3ddPoWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create prompt for getting unique restaurant name:\n",
        "get_restaurant_template = PromptTemplate(\n",
        "    input_variables = ['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest only 5 name for this.\"\n",
        ")\n",
        "restaurant_chain = LLMChain(llm=llm, prompt=get_restaurant_template, output_key='restaurant_name')\n",
        "\n",
        "\n",
        "# Now create prompt for getting unique menu for those restaurant:\n",
        "get_food_item_template = PromptTemplate(\n",
        "    input_variables = ['restaurant_name'],\n",
        "    template = \"Suggest some 5 food items for {restaurant_name}\"\n",
        ")\n",
        "food_menu_chain = LLMChain(llm=llm, prompt=get_food_item_template, output_key='menu_items')\n",
        "\n",
        "\n",
        "# now sequentialy access those chains:\n",
        "from langchain.chains import SequentialChain\n",
        "chain = SequentialChain(\n",
        "    chains=[restaurant_chain, food_menu_chain],\n",
        "    input_variables = ['cuisine'],\n",
        "    output_variables = ['restaurant_name', 'menu_items']\n",
        "\n",
        ")\n",
        "\n",
        "response = chain.invoke({\"cuisine\": \"indian\"})"
      ],
      "metadata": {
        "id": "bJjw6e8DOpJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cuisine Name: \", response['cuisine'], \"\\n\")\n",
        "\n",
        "# print(\"Restaurant Name:\")\n",
        "# print(response['restaurant_name'], \"\\n\")\n",
        "\n",
        "print(\"Restaurant Name with Food Menu Items: \")\n",
        "print(response['menu_items'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz1nJuNaEeb8",
        "outputId": "ec287544-3f68-4529-da21-dbd4dd1ce8d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuisine Name:  indian \n",
            "\n",
            "Restaurant Name with Food Menu Items: \n",
            "**1. Saffron & Spice**\n",
            "* Saffron-infused Chicken Biryani\n",
            "* Paneer Tikka Masala with Sautéed Spinach\n",
            "* Vegetable Korma with Cardamom and Cinnamon\n",
            "* Gulab Jamun with Saffron Syrup\n",
            "* Mango Lassi\n",
            "\n",
            "**2. Tandoori Temptation**\n",
            "* Tandoori Chicken with Mint Chutney\n",
            "* Chicken Tikka Masala with Smokey Flavors\n",
            "* Dal Makhani with Creamy Texture\n",
            "* Garlic Naan with Coriander\n",
            "* Kulfi with Pistachio and Saffron\n",
            "\n",
            "**3. Curry Corner**\n",
            "* Chicken Vindaloo with Tangy Spices\n",
            "* Lamb Rogan Josh with Aromatic Herbs\n",
            "* Vegetarian Thali with a Variety of Curries\n",
            "* Aloo Palak with Spinach and Potatoes\n",
            "* Raita with Cucumber and Mint\n",
            "\n",
            "**4. Namaste Flavors**\n",
            "* Butter Chicken with a Rich Tomato-Based Sauce\n",
            "* Palak Paneer with Silky Spinach and Cottage Cheese\n",
            "* Vegetable Samosas with Crisp Pastry and Savory Filling\n",
            "* Basmati Rice with Cardamom and Bay Leaves\n",
            "* Mango Chutney with Sweet and Tangy Notes\n",
            "\n",
            "**5. Masala Magic**\n",
            "* Paneer Butter Masala with Creamy Tomato Sauce\n",
            "* Chana Masala with Chickpeas and Aromatic Spices\n",
            "* Onion Bhaji with Crispy Batter and Tangy Sauce\n",
            "* Jeera Rice with Cumin and Coriander\n",
            "* Rava Laddu with Semolina and Jaggery\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Agent & Tools:**"
      ],
      "metadata": {
        "id": "iGDld48Hkv71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Agent:**\n",
        "\n",
        "**Definition:**<br>\n",
        "In LangChain, an Agent is a powerful component that acts as an intermediary between the user and the world. It's essentially a program designed to take actions and interact with the real world based on instructions and information processing. <br>\n",
        "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done.<br>\n",
        "\n",
        "When used correctly agents can be extremely powerful. **In order to load agents, you should understand the following concepts:**<br>\n",
        "* **Tool:** A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains.\n",
        "* **LLM:** The language model powering the agent.\n",
        "* **Agent:** The agent to use.\n",
        "<br><br>\n",
        "\n",
        "\n",
        "**Example:**<br>\n",
        "For example I have to travel from Dubai to Canada, I type this in ChatGPT. Like **Give me two flight options from Dubai to Canada on September 1, 2023.** <br>\n",
        "ChatGPT will not be able to answer because has knowledge till September 2021.\n",
        "ChatGPT plus has Expedia Plugin, if we enable this plugin it will go to Expedia Plugin and will try to pull information about Flights & it will show the information.<br>\n",
        "\n",
        "**What exactly happens when we try to enable this plugin:**<br>\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "When we think about LLM. Many people think that it is just a **knowledge engine**, it has knowledge and it will try to give answer based on that knowledge but the knowledge is only limited to September 2021. The think that most people missout is that Large Lanaguage Model has a reasoning engine, and using that **reasoning engine** it can figure out when someone types this type of Question. Like:<br>\n",
        "**Give me two flight options from Dubai to Canada on September 1, 2023.**<br>\n",
        "\n",
        "As a human we go to Expedia as we have a reasoning engine in our brain.<br>\n",
        "LLM has a reasoning engine as well, so it will figure out the Source, Destination, Date and it will call Expedia Plugin and it will return response back.<br>\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAUQAAAGTCAYAAABQ0WtRAAAUbklEQVR4nO3df0hd9/3H8df5YhlivU1tttXaJeHe24E4U5oNQoiybmD0j4KxW9pY9kcIXUEb/CNCJf+U4T9FV/uHNCn9p+SPMe3W1ARGWCK0+6JZ2D8tiYiw5V6SdtZ+O5N014qUCvf7h+/zyTn3hzGJerz6fIDUe+/xnM+9ic9+PufcqJfNZrMCAOh/oh4AAGwUBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBFbiud5UQ8BGxhBBABDEAHAEEQAMAQRAAxBBABDEAHAEESsunfffVee58nzPL377rtFtzt8+LDb7sqVK0W36+/vd9t5nqdkMql0Ol10+/Pnz6uqqsptX1VVtez2gI8gYtX997//Lfh5rq+//tp9Pjc3V3S7jz76KHQ7lUrpzTffLLp9V1eXbt++7W7fvn1bX3zxxbJjBiSCiBLz6KOPSpKGh4cLPn7lyhWlUilJ0s9+9rN1Gxc2B4KIknL48GFJS7O+8+fP5z3+pz/9SdJSOA8dOrSuY0PpI4goKc8995z7/C9/+Uve4++8846kO+EE7gVBREmJxWJ68cUXJS3F7+bNm+6x8+fPu3OHL730UiTjQ2kjiCg5Bw8edJ+Pjo66z/0ZYyKRUENDw7qPC6WPIKLkNDU1uc/Pnj0rSbp586a70OLPIIF7RRBRch577DEXvffff183b97U6OioWy6/8MILUQ4PJYwgoiQFl80ffPCBmykmEgk9/fTTUQ0LJa4s6gEA9yO4bP7973/v3nvIchkPghkiStJjjz3m3njtx1BiuYwHQxBRsl5++eXQbZbLeFAEEWuqp6cn9IMZPM9Tf39/3naNjY15242Pjy+77+CyWWK5jAdHELHqfvKTn6xou5/+9Kcr2m7btm2Slv453hNPPOHuj8fjam5udrdzl8uPPPKI+7yysnJFx8LW5mWz2WzUgwDWi+d54q88imGGCACGIAKA4X2IWFX8IniUMmaIAGCYIWJVbfQLFsxgsRxmiNhSNnqwES2CCACGIAKAIYgAYAgiABiCCACGIAKAIYgAYAgiABiCCACGIAKAIYgAYAgiABiCCACGIAKAIYgAYNb9B8TyAzoB3I/1+FmWzBABwBBEADAEEQBM5L9kit9xAaCQKK43MEMEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAFMW9QCi9nj981EPAdhwvpz4MOohRIIZIgCYLT9D9NUf6I56CEDkJi4ORD2ESDFDBABDEAHAEEQAMAQRAAxBBABDEAHAEEQAMAQRAAxBBABDEAHAEEQAMAQRAAxBxKZypnevTp/YE/UwUKL4aTdb1GDXbtXurMy7v6n7UgSjATYGgriFZeYX9avX/+Fun+ndq9GB/SUdxeDzAe4VS2Y4H3/6H0lSW2N1xCMBosEMEXd1pnevYhV3/qqcOpvWyNiMu10fj+mtV+tDX5M7ywwu0XNnppJ0+sQe1Wwvd7enbsypa/Cqu32sLa7WhmodPzkROta58Rm9PZIO7UeSjrzxiSSp92it6nbF9IfRz9R5MH5Pz0GSpmcX3L6w+TFDhNPaUK3M/GIoFKMD+zV5PaOm7ktq6r6kc+Mz6jwYd7NIPyT+403dlzQ9uxC6sNF7tFa1Oyvd4x9/+h/1Hq11j5/p3avK8odC+6jdWVnw4kjwWJcnb6m1oVr18diyzytWUabfNO1wXzd1Yy4UR3+/lydvuW0y84uaujFHDLcYgriFxSrKNDqw331cnrwVmrn1Hq1VZn5Rr7835e57eyStzPyifvHM9yVJE+lM3mzws/9bUGX5Q0WP+/ZI2u3zWFtcsYoy/e70VGibc+Mzqtlenhe74ycn3Od//tu0JOnnT2+/63MNPq/cUwP+f/39SdLk9UxoxoqtgSXzFhZcug527da+uirVx2OaSGckSdsefshFM9f07J3P2xqr82ZcQa+/N+Wim7sE/cGj35Mkd8w7+1+QJCVrKvIe8/n3+/tYqWvT85LkgjcytjTrPfRsjdtn3a6YGwO2DoIISVLX4FWNDuzXb5/bFTp3V+h8X5B/bi94Lq/3aK321VWFtmvqvuSW1/5sNDjz3Aj21VW5+E/PLoReB2wNBBHO5clboVni1998V/C9ikE/ePR7yswvhi5sFOMvrwe7dqtu19JS+Kvb30pSaGYq3Zm9+bO5tXSsLX7X8GNr4BwiHP8c2qFna0K3cy9unD6xx513++r2t4pVlIXOx+XODoPbS0uxm1v4TpJcSLtfTLrH6+MxtTZUa+rGXNHl8mqanl3IO586OrBfg1271/zY2FiYIcKZSGc0PbvgZm/+jM4PhO/c+Iy7Ev32SFo//tHD6jwYd+cRz43PqLXhTgAH3r+mt16td49n5hdD5xGbui+5N4X71nNJ7c9Cj5+cCAV4dGC/eo/WbrilPdaOl81ms+t6QM8L3V7nw+d5vP55Sfyi+q0s972Lwfsz84tb6lyi/4vqv5z4MOKRRNMKlszY8jLzi3lvsWlrrFbN9nL98/NvIhoVosCSGVte1+BVnT6xJ+/tRbn/mgWbH0EElL9cxtbEkhkADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcDwe5nNxMWBqIcAIGLMEAHAeNlsNruuB/S80O11PjyAEhFFK5ghAoAhiABgCCIAGIIIAIYgAoAhiABgCCIAGIIIAIYgAoAhiABgCCIAGIIIAIYgAoAhiABgCCIAGIIIAIYgAoAhiABgCCIAGIIIAIYgAoAhiABgCCIAGIIIAIYgAoAhiABgCCIAGIIIAIYgAoAhiABgCCIAGIIIAIYgAoAhiABgCCIAGIIIAIYgAoAhiABgCCIAGIIIAIYgAoAhiABgCCIAGIIIAIYgAoAhiABgCCIAGIIIAIYgAoAhiABgCCIAGIIIAIYgAoAhiABgCCIAGIKIPMlkUp2dnVEPA1h3BPE+tLS0yPM89zE+Ph71kDaFZDIZel37+/sLbhfcxvM8pdPp0OPDw8N52xQzPj4uz/PU0tKyqs8FpYkg3qOWlhZdu3ZN2WzWfTQ2Nq7Jsfxv1q0Q3M7OTr3yyivuNe3r61NPT08oiul0Wp7nqa+vz23X0dGhRCLhophOp9Xe3h7685FUMIotLS1r9meH0kQQ79GFCxf0yiuvhO7zv+lw/06dOqXXXnvN3fY//+ijj9x9b775phKJRGi7U6dOSZI++OADSVI8Hs/78+jr65Ok0P9Y+vv7deHCBaVSKTU3N6/ys0GpIoj3IfhNWsxyyzp/ppNOp0PL7+CyrbOz081eGhsb5Xle6Lxe7rIwd8nX39+vZDLpZpnBcfjHv9uSv7+//67LztxlbnBf/nGCYyi2DF6Jixcv6sCBA3n3Nzc3r+jPJOi1115TNptVPB6/7/FgE8quM0mhj1LT19fnxj42Npb3eCqVykrKDg0N5X1NKpUKbZO7naRsR0eHuz02NlbwOP7+ghKJRLa5uTlvm0Qi4e5rbm52x/XH0tHRUXBfuWNpbm4O7csfb19fn7s9NDQUGm/weRZ6rZbjP/fg/nPHtNzYch9f7u9ac3Nz6LXDxhBFKwjifQh+o+d+s3d0dBT85gp+cxeKZjabH7piQSz0tX6M/NAVimbuNsWOkUgk8sKTu11fX1/BCAW/1n+ewaitVKGIFdtXR0dH0SAWCmuhYxHEjSeKVrBkvg/+eaqsnatqbGx0S8V0Oq0LFy6s+Cpn0I4dO9w+ivEfa29vD+2/vb39QZ7SXT3xxBOSpH//+9+SpOvXryuVSuU9z1Qqlfe1/vNaKf/83tjYWN5j169fz7tvudersbFRzc3NofOOQDFlUQ+g1GWzWXmepz/+8Y9qaGiQtHRO669//euaHndoaEiHDx9e02PcTSKR0LVr11Z1n/39/erp6dHQ0JB7PYPHKyaZTObd53meEonEmv9ZYPNghrhKdu3aJWlp9ni/kfjss8/cPqQ7s7Ig/zF/2/XyxRdfSJKefPJJSUvPt9Bs8EEMDw+7GBaK/YEDB3Tx4sW8+y9cuKBf/vKXofuSyeSaBBubG0G8B8PDw3kzEf/K769//WtJ0ksvvaRUKpX3Lz2SyWTe0q69vV3Dw8OSlpZ9PT096ujoyDvu3//+99Dtjo4O9fT0hK7ojo+Pr+qbi995553Qczhy5IgSiYSbtfnPN/eYLS0t9/W+yeHhYbW3t6uvr6/ozNd/bYNXqv3jB5fEyWRSqVSKGOLercuZygCV+EWV4FXmYs8h96KLci5aBC+q+Fd0VeQKavB4wQsDuePIvajwoBdVhoaG3BXoQvv35T7P4MWeYhePCgm+Dsu9dv54i43Lf46FPgpdhS/0EXx9EJ0oWuHZgddN7gWGdT78hpBOp5VIJDbEeUBgo4qiFSyZAcAQRAAwLJkBbEgsmQEgQgQRAAxBBABDEAHAEEQAMAQRAAxBBABDEAHAEEQAMAQRAAxBBABDEAHAEEQAMAQRAAxBBABDEAHAEEQAMAQRAAxBBABDEAHAEEQAMAQRAAxBBABDEAHAEEQAMAQRAAxBBABDEAHAEEQAMGVRD2Cje7z++aiHAGw4X058GPUQ1gQzRAAwzBBXqP5Ad9RDACI3cXEg6iGsKWaIAGAIIgAYgggAhiACgCGIAGAIIgAYgggAhiACgCGIAGAIIgAYgggAhiCi5IwO7Ffv0doH3k9bY7VGB/arrbF6FUaFzYAgwjnWFtfowH7Vx2Puvvp4TKMD+3X6xJ4IRwasD4KIZf3uSK0y84s68sYnUQ8FWHP8+C8UdfrEHsUqytTUfSnqoQDrgiCioN6jtarZXq7jJydC97c1VqvzYFzHT07orVfr3f2XJ2/p9femQtsea4urtSF8fi4Y1zO9ezU9u6CuwavuvtMn9qiy/CH96vV/hMayr65q2TD72/imbsyF9itJg127Vbuz0t0+dTadtx//+eUK7i93m8z8Ymi8KF0smZHn0LM12ldXpVNn05pIZwpu89ar9Tp+ckJN3Zd0bnxG++qqQhcneo/WqrWhWqfOptXUfUlN3Zc0dWMudI5y8nomFChJqix/SJJC5zF3/LBcUzfmio53sGu36nbF3HGaui+pdmelBrt2h8ZTu7PSjfnU2XRe+OrjMXUejOvc+Izbj7QUez+Gx9ri7n8I/jZzC9/pTO/eu76u2PgIIvL4M62RsZmi2wRj+fbI0kzrmae2hfZxefJWaB9+VA49WyNJ+vRfX0uSC2lbY7WmZxc0Pbugnz+9XdJSpGq2l+ufn39TcBz18Zhqd1bqD6Ofhe6/PHkrFNu6XTFN3ZhzYx4Zm8mbIfrH9J+PtDQz3PHDcnf7F898X5cnb4X+R3FufEaxijKuVm8CLJmRx18On+ndu+KlYGZ+UdseDs/uvrr97bLbjYzN6DdNO/TMU9s0MjajZ57a5sL34x89LKlwpIKSNRWSpM6D8YJLXX88sYoyffxp4aj6/vfKrFobqnWsLe6OV7uzUpcnb7ltYhVl2ldXpdGB/cvuC6WJIKIgf0k52LU771zcapq8nlHdrqWA1u2K6c9/WzoP6Z97/PGPHl52uRwc73Iz2nvR2lDtjj91Yy7v3Gih86XYHAgiChoZm1HN9vK8GdNK+MvJHzz6vbzHYhVlmrz+nbv96b++ducfYxVl7msz84tqa6xWzfZyffzpf4oe69r0vCSpZnt50W2KjSf3aw49W6Pp2YVl32IUnOFi8+EcIop6eyStqRtzam2oDl3kWImpG3PaV1cV+jr/zd3B2dXI2Iwy84tqbVg6f+ibnl1Qa8NSJJeL8UQ647YNHutYWzx0USV3PG2N1XlXwL+6/a1qtpdrdGB/6CP4r2L8C0HH2u4sz+vjMS6qbBLMELGsrsGrOtO7V2+9Wn9P70fsGryq3qO1obfmZOYXC+5jenYh71zdPz//Rq0N1StaLh954xMNdu0OHSt3ptc1eFWnT+xx22TmF/PeOuQHOTjG+nhMb71a72bJr7835d5OFAwq79XcHLxsNptd1wN6Xuj2Oh/+nj1e/7wkfi/zVlDofZH+/ZPXM5w31J3fy/zlxIdrfqwoWsGSGTBzC9/lnVc81hZXrKLMvUUImxtLZsAceeMTnendm/eWmuMnJ4q+QR2bC0EEAvgneFsbS2YAMAQRAAxBBABDEAHAEEQAMAQRAAxBBABDEAHAEEQAMAQRAAxBBABDEAHAEEQAMAQRAAxBBABDEAHAEEQAMAQRAAxBBABDEAHAEEQAMAQRAAxBBABDEAHA8IvqV2ji4kDUQwCwxpghAoDxstlsdl0P6Hmh2+t8eAAlIopWMEMEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEAEMQAcAQRAAwBBEADEEEABP571TJ/am4ABAVZogAYAgiABiCCABm3c8h8lv2AGxUzBABwBBEADAEEQAMQQQAQxABwBBEADAEEQAMQQQAQxABwBBEADAEEQAMQQQAQxABwBBEADAEEQAMQQQAQxABwBBEADAEEQAMQQQAQxABwBBEADAEEQAMQQQAQxABwBBEADAEEQAMQQQAQxABwBBEADAEEQAMQQQAQxABwBBEADAEEQAMQQQAQxABwBBEADAEEQDM/wPoRwZ+/ibSfQAAAABJRU5ErkJggg==)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "48jWiNLCTtvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**An agent has access to a suite of tools, and determines which ones to use depending on the user input.**<br>\n",
        "\n",
        "Agent will conenct with external tools and it will use LLM reasoning capabilities.<br>\n",
        "All the tools like Google Search Tool and Math Tool are available as part of LangChain and you can configure agent, so agent is nothing but using all these tools and LLM reasoning capabilities to perform a given task.<br>\n",
        "To access Google Search Results in Real Time we use **serpapi**.\n",
        "<br><br>\n",
        "\n",
        "**Roles of a LangChain Agent:**\n",
        "* **Understanding User Input:** An agent can receive input from users in various forms, like text queries or voice commands. It processes this input to understand the user's intent.\n",
        "\n",
        "* **Decision Making:** Based on the user input and potentially other information sources, the agent decides on the course of action. This might involve using LangChain's tools or interacting with external systems.\n",
        "\n",
        "* **Interaction and Action Taking:** Agents can interact with various external systems through APIs or directly manipulate the real world (in controlled environments). This could involve things like:<br>\n",
        "  * Accessing and retrieving data from databases or web services,\n",
        "  * Controlling smart home devices,\n",
        "  * Generating different creative text formats based on user requests.\n",
        "\n",
        "* **Response Generation:** After taking action, the agent prepares a response for the user. This could be presenting retrieved information, summarizing results, or confirming actions taken.\n",
        "<br><br>\n",
        "\n",
        "**Responsibilities of a LangChain Agent:**<br>\n",
        "* **Flexibility and Adaptability:** Agents should be able to handle a variety of user inputs and situations, potentially adapting their actions based on context.\n",
        "\n",
        "* **Reasoning and Logic:** They need some level of reasoning ability to choose the most appropriate course of action based on the available information.\n",
        "\n",
        "* **Information Processing:** Agents can process information from various sources, including user input, LLM outputs, and external data sources, to make informed decisions.\n",
        "\n",
        "* **Communication:** Effective communication with the user is crucial. Agents should present clear and concise responses that reflect their actions and the results achieved.\n",
        "<br><br>\n",
        "\n",
        "**In Short:**<br>\n",
        "LangChain Agents are like smart assistants that leverage the power of large language models (LLMs) and other tools to understand user requests, take actions in the real world (or simulated environments), and communicate effectively. They play a vital role in building interactive and intelligent applications within the LangChain framework.\n"
      ],
      "metadata": {
        "id": "ZmFfNGh8efL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tools:**\n",
        "\n",
        "**Definition:**<br>\n",
        "In LangChain, Tools are essentially mini-programs that perform specific tasks and act as the bridge between Agents and the external world (or the world of data).<br>\n",
        "LangChain Tools act as the \"hands and feet\" of Agents, allowing them to interact with data sources, external systems, and LLMs to fulfill user requests and complete tasks within the defined workflow. They are essential components for building versatile and powerful LangChain applications.\n",
        "<br><br>\n",
        "\n",
        "**Roles of a LangChain Tool:**<br>\n",
        "* **Specialized Tasks:** Unlike Agents that handle a broader range of actions, Tools are designed for specific, well-defined tasks. These tasks can involve:<br>\n",
        "  * **Data retrieval:** Fetching information from databases, web APIs, or other sources based on instructions from the Agent.\n",
        "\n",
        "  * **Data Processing:** Transforming or manipulating data (like text, numbers, or images) to prepare it for further use.\n",
        "\n",
        "  * **Interaction with external systems:** Connecting to external APIs or services to perform actions or retrieve data that the Agent can't handle directly.\n",
        "\n",
        "  * **LLM interaction:** Facilitating communication with Large Language Models (LLMs) by providing prompts and processing their responses.\n",
        "\n",
        "* **Integration with Agents:**  Tools are building blocks for Agents. Agents can call upon different tools depending on the task at hand and combine their outputs to achieve a desired outcome.\n",
        "<br><br>\n",
        "\n",
        "**Responsibilities of a LangChain Tool:**<br>\n",
        "* **Clear and Defined Functionality:** Each tool should have a well-defined purpose and provide a consistent output format for seamless integration with Agents.\n",
        "\n",
        "* **Efficiency and Accuracy:** Tools should execute their tasks efficiently and deliver accurate results to maintain the overall effectiveness of the Agent.\n",
        "\n",
        "* **Ease of Use:** Ideally, tools should be designed in a way that makes them easy for developers to integrate into their LangChain workflows.\n",
        "<br><br>\n",
        "\n",
        "**Types of LangChain Tools:**<br>\n",
        "* **Built-in Tools:** The framework provides a collection of pre-built tools for common tasks like web scraping, database access, or interacting with specific APIs.\n",
        "\n",
        "  * **WikipediaQueryRun:** This tool retrieves information from Wikipedia based on a user query.\n",
        "\n",
        "  * **WebAPIRetriever:** This tool can fetch data from various web APIs based on a provided URL and parameters.\n",
        "\n",
        "  * **LLMChainRunner:** This tool helps execute workflows that involve interacting with Large Language Models (LLMs).\n",
        "\n",
        "  * **SQliteReader:** This tool allows you to read data from SQLite databases.\n",
        "<br>\n",
        "\n",
        "* **Custom Tools:** Developers can also create their own custom tools to handle unique functionalities or integrate with specialized external systems not covered by built-in options.\n",
        "\n",
        "  * **SocialMediaSentimentAnalysisTool:** This could be a custom tool designed to analyze sentiment from social media posts. It might involve retrieving posts using an external social media API, processing the text with an LLM, and calculating the overall sentiment.\n",
        "\n",
        "  * **ImageCaptionGeneratorTool:** A custom tool that generates captions for images. It could use an LLM trained on image-text datasets to create descriptive captions based on the image content.\n",
        "\n",
        "  * **SmartHomeControlTool:** This could be a custom tool designed to interact with a smart home system API. It might allow users to control lights, thermostats, or other devices through voice commands or text prompts.\n",
        "\n"
      ],
      "metadata": {
        "id": "aKWcqxyllPS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SerpAPI and LLM-Math tool:**\n",
        "\n",
        "\n",
        "**Definition:**<br>\n",
        "SerpAPI and LLM-Math are external tools that can be valuable additions to LangChain workflows. SerpAPI allows Agents to access and process information from the web, while LLM-Math empowers them to tackle mathematical problems using the power of LLMs. By integrating these tools, developers can create LangChain applications with a wider range of capabilities.\n",
        "<br><br>\n",
        "\n",
        "**SerpAPI:**<br>\n",
        "* **Description:** SerpAPI is a service that provides access to various search engine results through an API. It allows developers to programmatically retrieve search results for specific queries.\n",
        "\n",
        "* **Integration with LangChain:**  In LangChain, SerpAPI can be used as a custom tool within an Agent's workflow. Here's how it might work:\n",
        "  1. The Agent receives a user query.\n",
        "  2. The Agent utilizes a custom tool built around SerpAPI to search the web for relevant information based on the user query.\n",
        "  3. SerpAPI retrieves search results from a search engine (like Google).\n",
        "  4. The Agent processes the retrieved search results (potentially with the help of other tools) and presents them to the user.\n",
        "\n",
        "* **Benefits:**  Integrating SerpAPI with LangChain allows Agents to access and process information from the real world through search engines. This expands the capabilities of LangChain applications by enabling them to find and utilize relevant web content.\n",
        "<br><br>\n",
        "\n",
        "\n",
        "**LLM-Math:**<br>\n",
        "* **Description:** LLM-Math is a library or tool specifically designed to work with Large Language Models (LLMs) for mathematical problem solving. It helps bridge the gap between the capabilities of LLMs in understanding text and their ability to perform complex calculations.\n",
        "\n",
        "* **Integration with LangChain:**  LLM-Math can be used as a custom tool within a LangChain Agent's workflow for math-related tasks. Here's a possible scenario:\n",
        "  1. The Agent receives a user query asking to solve a math problem (e.g., \"What is the integral of x^2?\").\n",
        "  2. The Agent utilizes a custom tool built around LLM-Math to format the user query into a way suitable for the LLM.\n",
        "  3. LLM-Math interacts with an LLM, potentially providing additional context or steps to guide the LLM towards the solution.\n",
        "  4. The Agent receives the LLM's response (hopefully containing the solution) and presents it to the user in a clear and understandable way.\n",
        "\n",
        "* **Benefits:**  Integrating LLM-Math with LangChain empowers Agents to handle user queries involving mathematical problems. This can be particularly beneficial for applications like educational assistants or scientific data analysis tools.\n",
        "<br><br>\n",
        "\n",
        "\n",
        "**Note:**<br>\n",
        "* If you're using a text LLM, first try **zero-shot-react-description**, aka. the MRKL agent for LLMs.\n",
        "\n",
        "* If you're using a Chat Model, try **chat-zero-shot-react-description**, aka. the MRKL agent for Chat Models.\n",
        "\n",
        "* If you're using a Chat Model and want to use memory, try **chat-conversational-react-description**, the Conversational agent.\n",
        "\n",
        "* If you have a complex task that requires many steps and you're interested in experimenting with a new type of agent, try the **Plan-and-Execute agent**.\n",
        "<br><br>\n",
        "\n",
        "**Break Down:**<br>\n",
        "* **zero-shot:** This refers to a machine learning approach where a model can perform a task without any specific training data for that task.\n",
        "\n",
        "* **React:** This is a popular JavaScript library for building user interfaces.\n",
        "\n",
        "* **chat:** This suggests the components are related to building chat interfaces.\n",
        "\n",
        "* **conversational:** This implies the component focuses on building interfaces for natural, flowing conversations within a chat setting.\n",
        "\n",
        "* **Plan:** This agent first analyzes the situation and formulates a plan to achieve the desired outcome.\n",
        "\n",
        "* **Execute:** Once the plan is formulated, the agent takes actions to implement the plan.\n",
        "<br>\n",
        "\n",
        "Plan-and-Execute agents are often used in scenarios where tasks require careful planning and execution, like robot control systems or game-playing AI.<br>\n",
        "It's important to note that these are inferences based on the available information. Without more context or access to the actual descriptions, it's difficult to provide a definitive explanation.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MCZUjZ8ZoR3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example with Agent & Tools**"
      ],
      "metadata": {
        "id": "Nnb0Zv15sjsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 01**"
      ],
      "metadata": {
        "id": "wDbERQadsovK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results"
      ],
      "metadata": {
        "id": "0Onp7hjHNHJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "serp_api_key = userdata.get('Serp_api_key')\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['SERPAPI_API_KEY'] = serp_api_key"
      ],
      "metadata": {
        "id": "QBpB7__9NHMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=Gemini_API_Key) # load the gemini-pro model\n",
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnbyz4riNHPs",
        "outputId": "9def140c-2a04-4d79-cbd0-b20ab5de55bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGoogleGenerativeAI(model='gemini-pro', google_api_key=SecretStr('**********'), client= genai.GenerativeModel(\n",
              "   model_name='models/gemini-pro',\n",
              "   generation_config={}.\n",
              "   safety_settings={}\n",
              "))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "from langchain_community.utilities import SerpAPIWrapper\n",
        "\n",
        "\n",
        "#Google Search API\n",
        "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)\n",
        "\n",
        "# Let's test it out!\n",
        "agent.invoke(\"What was the GDP of India in 2023?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eImLxEnRzukq",
        "outputId": "24da548a-945d-4cd4-f189-bd4070685bc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction: Search\n",
            "Action Input: What was the GDP of India in 2023?\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m{'type': 'organic_result', 'title': 'Historical GDP and growth rate of India', 'source': 'Forbes India'}\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mAction: Search\n",
            "Action Input: GDP of India 2023\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPublished on Jan 02, 2024 The Indian economy witnessed a great year, closing 2023 with a GDP of US$ 3.73 trillion, GDP per capita at US$ 2,610 and a projected GDP growth rate of 6.3 percent against the global average of 2.9 percent.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 3.73 trillion USD\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What was the GDP of India in 2023?', 'output': '3.73 trillion USD'}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 02**\n",
        "\n",
        "wikipedia & llm-math tools"
      ],
      "metadata": {
        "id": "Y4iPFU4Y988l"
      }
    },
    {
      "source": [
        "!pip install wikipedia"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "u1N81y8g0TjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install this package: pip install wikipedia\n",
        "\n",
        "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
        "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
        "\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Let's test it out!\n",
        "agent.invoke(\"In what year was the film Departed with Leopnardo Dicaprio released? What is this year raised to the 0.43 power?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCVQhBz997E9",
        "outputId": "27a0f486-2af5-4bf2-de77-c01b9de7e957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAction: wikipedia\n",
            "Action Input: Departed (film)\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3mPage: The Departed\n",
            "Summary: The Departed is a 2006 American crime thriller film directed by Martin Scorsese and written by William Monahan. It is both a remake of the 2002 Hong Kong film Infernal Affairs and also loosely based on the real-life Boston Winter Hill Gang; the character Colin Sullivan is based on the corrupt FBI agent John Connolly, while the character Frank Costello is based on Irish-American gangster and crime boss Whitey Bulger. The film stars Leonardo DiCaprio, Matt Damon, Jack Nicholson, and Mark Wahlberg, with Martin Sheen, Ray Winstone, Vera Farmiga, Alec Baldwin, Anthony Anderson and James Badge Dale in supporting roles.\n",
            "The film takes place in Boston and the surrounding metro area, primarily in the South Boston neighborhood. Irish Mob boss Frank Costello (Nicholson) plants Colin Sullivan (Damon) as a spy within the Massachusetts State Police; simultaneously, the police assign undercover state trooper Billy Costigan (DiCaprio) to infiltrate Costello's mob crew. When both sides realize the situation, Sullivan and Costigan each attempt to discover the other's identity before they are found out.\n",
            "The Departed was a critical and commercial success, receiving acclaim for its direction, performances (particularly of DiCaprio, Nicholson, and Wahlberg), screenplay, and editing.\n",
            "It won several accolades, including four Oscars at the 79th Academy Awards: for Best Picture, Best Director, Best Adapted Screenplay, and Best Film Editing. It remains Scorsese's only personal Oscar win. The film also received six nominations each at the 64th Golden Globe Awards and the 60th British Academy Film Awards, and two nominations at the 13th Screen Actors Guild Awards.\n",
            "\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mAction: Calculator\n",
            "Action Input: 2006^(0.43)\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 26.30281917656938\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mFinal Answer: 26.30281917656938\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'In what year was the film Departed with Leopnardo Dicaprio released? What is this year raised to the 0.43 power?',\n",
              " 'output': '26.30281917656938'}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Memory:**\n",
        "\n",
        "\n",
        "**Definition:**<br>\n",
        "Memory plays a vital role in transforming LangChain applications from stateless interactions to contextual experiences. By storing and retrieving relevant information, Memory empowers developers to build applications that feel more natural, personalized, and intelligent. **It helps to remember past information.**\n",
        "<br><br>\n",
        "\n",
        "**Memory: The Backbone of Contextual Interactions:**<br>\n",
        "LangChain, by default, treats each interaction with a Large Language Model (LLM) as independent. This means the LLM doesn't have any inherent memory of previous conversations or user inputs. <br>\n",
        "**Memory in LangChain solves this by:**<br>\n",
        "* **Persisting Information:** It stores relevant information from past interactions, such as user queries, LLM responses, and other context-dependent data.\n",
        "\n",
        "* **Providing Context:** During subsequent interactions, the Memory module makes this stored information accessible to the current workflow. This allows the LLM to consider past interactions and user history, leading to more informed and relevant responses.\n",
        "<br><br>\n",
        "\n",
        "**Roles and Responsibilities of Memory:**<br>\n",
        "* **Data Storage:** Memory acts as a central repository for information relevant to user interactions within the LangChain application. This data can include:\n",
        "  * User queries and prompts,\n",
        "  * LLM responses and outputs,\n",
        "  * Intermediate processing results from other LangChain tools,\n",
        "  * User-specific preferences or settings (if applicable).\n",
        "\n",
        "* **Data Retrieval:** When needed, the Memory module provides relevant information based on the current context. This allows the LLM and other LangChain components to access past information and make decisions based on a user's history.\n",
        "\n",
        "* **Data Management:** Memory also handles tasks like:\n",
        "  * Defining what information to store and for how long.\n",
        "  * Managing storage capacity and potentially purging old data.\n",
        "  * Ensuring data security and access control (if sensitive data is involved).\n",
        "<br><br>\n",
        "\n",
        "**Benefits of Using Memory in LangChain:**<br>\n",
        "* **Improved Conversational Experience:** By remembering past interactions, LangChain applications can provide more natural and engaging conversations. Imagine a chatbot that remembers your preferences or a virtual assistant that keeps track of your ongoing tasks.\n",
        "\n",
        "* **Enhanced Personalization:** Applications can personalize experiences based on user history. This could involve tailoring content recommendations, suggesting follow-up actions based on previous interactions, or simply remembering a user's name.\n",
        "\n",
        "* **Efficient Information Processing:** By leveraging context from past interactions, LLMs and other tools within LangChain can potentially work more efficiently and require less repetitive information from the user.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HLIuZSAv-hA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ckech Memory is Empty or Not:**"
      ],
      "metadata": {
        "id": "Ik4CCyPjBhad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 01**"
      ],
      "metadata": {
        "id": "5nHj95bPAcYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=Gemini_API_Key) # load the gemini-pro model\n",
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb7Am2Xr97JC",
        "outputId": "ea40d0ef-221f-48bd-cd3a-a230bd7dc757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGoogleGenerativeAI(model='gemini-pro', google_api_key=SecretStr('**********'), client= genai.GenerativeModel(\n",
              "   model_name='models/gemini-pro',\n",
              "   generation_config={}.\n",
              "   safety_settings={}\n",
              "))"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest me 5 name for this.\"\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
        "name = chain.invoke(\"Bengali\")\n",
        "print(name['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh0mCMnQ97MZ",
        "outputId": "1b14a6cf-3459-46d5-ad55-b25e9c0fba44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Mishti Mando\n",
            "2. Ananda Aahar\n",
            "3. Rasgulla Rasoi\n",
            "4. Mishti Maach\n",
            "5. Kosha Mangsho Mahal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check is data present on memeory or not:\n",
        "\n",
        "type(chain.memory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM9nx37yz8GC",
        "outputId": "f7b47ad4-8fd2-4619-f88e-a49975ac3bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NoneType"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ConversationBufferMemory:**\n",
        "\n",
        "**Definition:**<br>\n",
        "ConversationBufferMemory is a specific type of Memory component designed for LangChain. It focuses on storing and managing the raw conversation history between a user and the LLM. Here's a deeper dive into its roles, responsibilities, advantages, and drawbacks.<br>\n",
        "ConversationBufferMemory offers a basic way to build context awareness in LangChain applications by storing the entire conversation history. While it's easy to use, it can become inefficient for long interactions.\n",
        "<br><br>\n",
        "\n",
        "**Roles and Responsibilities:**<br>\n",
        "* **Conversation History Storage:** ConversationBufferMemory acts as a temporary buffer, storing the complete conversation history, including:\n",
        "  * User queries and prompts.\n",
        "  * LLM responses and outputs.\n",
        "\n",
        "* **Data Retrieval:** When prompted, it provides the entire conversation history, typically as a formatted string or a list of messages. This allows the LLM or other LangChain tools to access the context of the current interaction.\n",
        "<br><br>\n",
        "\n",
        "**Pros of ConversationBufferMemory:**<br>\n",
        "* **Simplicity:** It's a straightforward and easy-to-use memory solution for basic conversational applications.\n",
        "\n",
        "* **Preserves Completeness:** It stores the entire conversation, which can be helpful for tasks that require access to all historical details.\n",
        "\n",
        "* **Faster Implementation:** For simple use cases, ConversationBufferMemory can be a quick way to incorporate context awareness into your LangChain application.\n",
        "<br><br>\n",
        "\n",
        "**Cons of ConversationBufferMemory:**<br>\n",
        "* **Limited Context Management:** It doesn't perform any intelligent processing on the stored data. All information is simply saved as raw text.\n",
        "\n",
        "* **Potential for Redundancy:** Storing the entire conversation history can be inefficient, especially for lengthy interactions. This can lead to increased memory usage and potentially slower processing as the conversation grows.\n",
        "\n",
        "* **Limited Summarization:** If the conversation is long, the LLM might struggle to process the entire history effectively. It might be beneficial to summarize or extract key points before feeding the history to the LLM.\n",
        "<br><br>\n",
        "\n",
        "**Alternatives to Consider:** For more complex scenarios, LangChain offers other memory options like **ConversationSummaryMemory**. This type of memory analyzes the conversation history and provides a concise summary for the LLM, improving efficiency and potentially leading to better results.\n"
      ],
      "metadata": {
        "id": "4B8xlFKeBVBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 01**"
      ],
      "metadata": {
        "id": "a60ACvzjDDMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory()"
      ],
      "metadata": {
        "id": "SshRFRc3AzRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the PromptTemplate:\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "get_cuisine_template = PromptTemplate(\n",
        "    input_variables=['cuisine'],\n",
        "    template=\"Please give top 5 {cuisine} food.\"\n",
        ")\n",
        "\n",
        "# create a chain:\n",
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=get_cuisine_template, memory=memory)\n",
        "\n",
        "# get Indian Cuisine\n",
        "indian_cuisine_response = chain.invoke('Indian')\n",
        "print(indian_cuisine_response['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DQuABdjAzVW",
        "outputId": "6691ea9d-8867-459f-df38-aa7b7ff9e6d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. **Butter Chicken:** A creamy, tomato-based dish with tender chicken cooked in a blend of spices and herbs.\n",
            "2. **Biryani:** A layered dish made with rice, vegetables, meat, and aromatic spices, cooked in a sealed pot.\n",
            "3. **Palak Paneer:** A vegetarian dish consisting of spinach cooked with paneer (Indian cottage cheese) and seasoned with cumin and coriander.\n",
            "4. **Dal Makhani:** A rich and flavorful lentil dish simmered in a buttery tomato sauce with cream and spices.\n",
            "5. **Tandoori Chicken:** Chicken marinated in yogurt, spices, and herbs and cooked in a clay oven (tandoor).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get Bengali Cuisine\n",
        "\n",
        "bengali_cuisine_response = chain.invoke('Bengali')\n",
        "print(bengali_cuisine_response['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-Wawe1cAzZ5",
        "outputId": "a590759d-67a6-4cb8-ebef-b5555db782ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. **Machher Jhol:** A classic Bengali fish curry made with mustard, poppy seeds, and turmeric.\n",
            "2. **Kosha Mangsho:** Slow-cooked goat meat in a rich and flavorful gravy.\n",
            "3. **Shukto:** A mixed vegetable dish made with bitter gourd, eggplant, potato, and drumsticks.\n",
            "4. **Bhapa Ilish:** Steamed hilsa fish wrapped in banana leaves and flavored with mustard and turmeric.\n",
            "5. **Cholar Dal:** A lentil dish made with black-eyed peas, potatoes, and spices.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get Panjabi Cuisine\n",
        "\n",
        "panjabi_cuisine_response = chain.invoke('Panjabi')\n",
        "print(panjabi_cuisine_response['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MUHi4VOD8_a",
        "outputId": "c9c159f0-7351-4003-adaf-735c50997640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. **Makki di Roti and Sarson da Saag:** A classic combination of unleavened flatbread made from cornmeal and a creamy, spiced spinach dish.\n",
            "2. **Chole Bhature:** Spicy chickpea curry served with fluffy fried bread.\n",
            "3. **Butter Chicken:** Chicken cooked in a creamy, tomato-based sauce.\n",
            "4. **Lassi:** A refreshing yogurt-based drink, often flavored with fruits or spices.\n",
            "5. **Tandoori Chicken:** Chicken marinated in yogurt and spices, then roasted in a clay oven.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now get the Memory Information:\n",
        "\n",
        "print(chain.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xGdtqzKD9DE",
        "outputId": "3ce3ab07-43a7-470e-a753-0e8d63d88e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Indian\n",
            "AI: 1. **Butter Chicken:** A creamy, tomato-based dish with tender chicken cooked in a blend of spices and herbs.\n",
            "2. **Biryani:** A layered dish made with rice, vegetables, meat, and aromatic spices, cooked in a sealed pot.\n",
            "3. **Palak Paneer:** A vegetarian dish consisting of spinach cooked with paneer (Indian cottage cheese) and seasoned with cumin and coriander.\n",
            "4. **Dal Makhani:** A rich and flavorful lentil dish simmered in a buttery tomato sauce with cream and spices.\n",
            "5. **Tandoori Chicken:** Chicken marinated in yogurt, spices, and herbs and cooked in a clay oven (tandoor).\n",
            "Human: Bengali\n",
            "AI: 1. **Machher Jhol:** A classic Bengali fish curry made with mustard, poppy seeds, and turmeric.\n",
            "2. **Kosha Mangsho:** Slow-cooked goat meat in a rich and flavorful gravy.\n",
            "3. **Shukto:** A mixed vegetable dish made with bitter gourd, eggplant, potato, and drumsticks.\n",
            "4. **Bhapa Ilish:** Steamed hilsa fish wrapped in banana leaves and flavored with mustard and turmeric.\n",
            "5. **Cholar Dal:** A lentil dish made with black-eyed peas, potatoes, and spices.\n",
            "Human: Panjabi\n",
            "AI: 1. **Makki di Roti and Sarson da Saag:** A classic combination of unleavened flatbread made from cornmeal and a creamy, spiced spinach dish.\n",
            "2. **Chole Bhature:** Spicy chickpea curry served with fluffy fried bread.\n",
            "3. **Butter Chicken:** Chicken cooked in a creamy, tomato-based sauce.\n",
            "4. **Lassi:** A refreshing yogurt-based drink, often flavored with fruits or spices.\n",
            "5. **Tandoori Chicken:** Chicken marinated in yogurt and spices, then roasted in a clay oven.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 02**\n",
        "\n",
        "\n",
        "**ConversationChain**<br>\n",
        "* Conversation buffer memory goes growing endlessly.\n",
        "* Just remember last 5 Conversation Chain.\n",
        "* Just remember last 10-20 Conversation Chain."
      ],
      "metadata": {
        "id": "fC41UlAXExlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "convo = ConversationChain(llm=llm)\n",
        "print(convo.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWZ9rEovD9HA",
        "outputId": "ba071080-a019-491a-b0b2-497e88dfaeb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = convo.invoke(\"Who won the first cricket world cup? And get some information about it\")\n",
        "print(result['response'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF5HnPUZD9LI",
        "outputId": "d8b6e49e-8140-457a-8bae-0878f8544935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**West Indies**\n",
            "\n",
            "* The first Cricket World Cup was held in England from May 30 to June 23, 1975.\n",
            "* Eight teams participated in the tournament: Australia, England, India, New Zealand, Pakistan, Sri Lanka, the West Indies, and East Africa.\n",
            "* The tournament was played in a round-robin format, with each team playing each other once.\n",
            "* The top two teams from the round-robin stage advanced to the final.\n",
            "* The West Indies defeated Australia by 17 runs in the final to win the first Cricket World Cup.\n",
            "* Clive Lloyd was the captain of the West Indies team.\n",
            "* The West Indies team was considered to be one of the strongest teams in the world at the time, and they were the favorites to win the tournament.\n",
            "* The West Indies team played very well throughout the tournament, and they were never really in danger of losing.\n",
            "* The victory was a major triumph for the West Indies, and it helped to establish them as one of the leading cricket teams in the world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = convo.invoke(\"How much is 5+5?\")\n",
        "print(result['response'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3V-CNvIFIwI",
        "outputId": "ce8d8033-c3e4-44db-e59c-e5b818761bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = convo.invoke(\"Who was the captain ofthe winning team?\")\n",
        "print(result['response'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDYNDYMSFI1a",
        "outputId": "c3683b28-330d-4fbb-adb9-b7341d0c7241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frank Worrell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the Memory information:\n",
        "\n",
        "print(convo.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlguhteFFI6N",
        "outputId": "3a669faf-0473-48d9-c8fd-3212b60a859d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Who won the first cricket world cup?\n",
            "AI: West Indies\n",
            "Human: How much is 5+5?\n",
            "AI: 10\n",
            "Human: Who was the captain ofthe winning team?\n",
            "AI: Frank Worrell\n",
            "Human: Who won the first cricket world cup? And get some information about it\n",
            "AI: **West Indies**\n",
            "\n",
            "* The first Cricket World Cup was held in England from May 30 to June 23, 1975.\n",
            "* Eight teams participated in the tournament: Australia, England, India, New Zealand, Pakistan, Sri Lanka, the West Indies, and East Africa.\n",
            "* The tournament was played in a round-robin format, with each team playing each other once.\n",
            "* The top two teams from the round-robin stage advanced to the final.\n",
            "* The West Indies defeated Australia by 17 runs in the final to win the first Cricket World Cup.\n",
            "* Clive Lloyd was the captain of the West Indies team.\n",
            "* The West Indies team was considered to be one of the strongest teams in the world at the time, and they were the favorites to win the tournament.\n",
            "* The West Indies team played very well throughout the tournament, and they were never really in danger of losing.\n",
            "* The victory was a major triumph for the West Indies, and it helped to establish them as one of the leading cricket teams in the world.\n",
            "Human: Who won the first cricket world cup? And get some information about it\n",
            "AI: **West Indies**\n",
            "\n",
            "* The first Cricket World Cup was held in England from May 30 to June 23, 1975.\n",
            "* Eight teams participated in the tournament: Australia, England, India, New Zealand, Pakistan, Sri Lanka, the West Indies, and East Africa.\n",
            "* The tournament was played in a round-robin format, with each team playing each other once.\n",
            "* The top two teams from the round-robin stage advanced to the final.\n",
            "* The West Indies defeated Australia by 17 runs in the final to win the first Cricket World Cup.\n",
            "* Clive Lloyd was the captain of the West Indies team.\n",
            "* The West Indies team was considered to be one of the strongest teams in the world at the time, and they were the favorites to win the tournament.\n",
            "* The West Indies team played very well throughout the tournament, and they were never really in danger of losing.\n",
            "* The victory was a major triumph for the West Indies, and it helped to establish them as one of the leading cricket teams in the world.\n",
            "Human: How much is 5+5?\n",
            "AI: 10\n",
            "Human: How much is 5+5?\n",
            "AI: 10\n",
            "Human: Who was the captain ofthe winning team?\n",
            "AI: Frank Worrell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ConversationBufferWindowMemory:**\n",
        "\n",
        "\n",
        "**Definition:**<br>\n",
        "ConversationBufferWindowMemory is another type of Memory component in LangChain specifically designed for managing conversation history. It offers a balance between preserving context and memory efficiency compared to ConversationBufferMemory. Here's a breakdown of its functionalities, advantages, and disadvantages.<br>\n",
        "ConversationBufferWindowMemory provides a more memory-efficient way to manage conversation history in LangChain. By focusing on the most recent interactions, it offers a good balance between context awareness and resource management. However, it's important to choose the appropriate window size based on your specific application's needs.\n",
        "<br><br>\n",
        "\n",
        "**Roles and Responsibilities:**<br>\n",
        "* **Limited Window Storage:** Unlike ConversationBufferMemory, which stores the entire conversation, ConversationBufferWindowMemory keeps track of only the most recent interactions. This window size is configurable, allowing you to define how many past interactions to remember.\n",
        "\n",
        "* **Data Retrieval:** It provides access to the conversation history within the defined window. This retrieved history can be in the form of a formatted string or a list of recent messages.\n",
        "<br><br>\n",
        "\n",
        "**Pros of ConversationBufferWindowMemory:**<br>\n",
        "* **Manages Memory Usage:** By limiting the stored conversation history, it avoids the inefficiency of saving everything. This is particularly beneficial for longer interactions or applications where memory usage is a concern.\n",
        "\n",
        "* **Focuses on Recent Context:** It prioritizes the most relevant parts of the conversation, potentially leading to more focused and relevant responses from the LLM compared to using the entire history.\n",
        "\n",
        "* **Suitable for Most Use Cases:** This window-based approach offers a good balance for many conversational applications where recent context is crucial but full history might not be necessary.\n",
        "<br><br>\n",
        "\n",
        "**Cons of ConversationBufferWindowMemory:**<br>\n",
        "* **Potential Loss of Context:** If the window size is too small, it might exclude important information from earlier parts of the conversation, leading to potentially less informed LLM responses for certain situations.\n",
        "\n",
        "* **Tuning the Window Size:** Finding the optimal window size can be a balancing act. A very small window might lose crucial context, while a large window might approach the inefficiency of storing the entire history.\n",
        "<br><br>\n",
        "\n",
        "\n",
        "**ConversationBufferWindowMemory(k=1), \"k\" means:**<br>\n",
        "In the context of ConversationBufferWindowMemory(k=1) within LangChain, **k represents the number of most recent interactions (user queries and LLM responses) to be stored in memory**.<br><br>\n",
        "\n",
        "**Here's a breakdown of how k functions:**<br>\n",
        "* **Limited Window:** ConversationBufferWindowMemory doesn't store the entire conversation history. Instead, it maintains a focus on the most recent interactions.\n",
        "\n",
        "* **Configurable Window Size:** The **'k'** parameter allows you to define the size of this window. In this case, k=1 specifies that only the most recent interaction (one user query and the corresponding LLM response) will be kept in memory.\n",
        "<br>\n",
        "\n",
        "* **Impact of k=1:**\n",
        "  * **Focus on Immediate Context:** With **k=1**, the LLM will primarily base its responses on the user's most recent query and its own immediate response. This can be beneficial for tasks where the conversation flows quickly, and prior context is less important.\n",
        "\n",
        "  * **Limited Memory Usage:** By storing only the single most recent interaction, **k=1** minimizes memory consumption. This can be crucial for applications with resource limitations.\n",
        "<br>\n",
        "\n",
        "* **Choosing the right k value:** The ideal value for 'k' depends on your specific application's needs. Here are some factors to consider:\n",
        "  * **Task Complexity:** If your task requires the LLM to consider information from previous interactions (like a multi-step task or a conversation following a specific storyline), you might need a larger window size (higher k value).\n",
        "\n",
        "  * **Memory Constraints:** If memory usage is a concern, a smaller window size (lower k value) might be necessary.\n",
        "<br><br>\n",
        "\n",
        "\n",
        "\n",
        "**Alternative Memory Options:** For scenarios where even a limited window might be too much, LangChain offers other memory components like Key-Value Memory. This allows you to store specific data points (key-value pairs) extracted from the conversation, providing a highly focused and efficient approach to context management.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_Aw9dQQDGT_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 01**"
      ],
      "metadata": {
        "id": "oAl6IpDzImAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "\n",
        "convo = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "id": "TBYKPaZSFI-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = convo.invoke(\"Who won the first cricket world cup? And get some information about it within 3 lines\")\n",
        "print(result['response'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNWJi8vSFJCn",
        "outputId": "d48b893a-66dd-4752-fa88-4d5f103b78b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The West Indies won the inaugural Cricket World Cup in 1975, defeating Australia by 17 runs in the final at Lord's, London. The tournament was played in England from June 7 to June 21, 1975, and featured eight teams. The West Indies, led by Clive Lloyd, went undefeated throughout the tournament, winning all five of their matches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = convo.invoke(\"How much is 5+5?\")\n",
        "print(result['response'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9DdOsLPI23h",
        "outputId": "68fa8eaa-c5c9-4ad9-b91b-fd9e1c24660f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = convo.invoke(\"Who was the captain ofthe winning team?\")\n",
        "print(result['response'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itq5pIwgI27Y",
        "outputId": "518f3146-3e00-41f0-80df-f757281fe899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This context does not mention anything about a winning team, so I cannot answer this question from the provided context.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the Memory Infomration:\n",
        "\n",
        "print(convo.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGMYTl7RFJHM",
        "outputId": "27e6a578-c852-4377-d29a-23e7b140b04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Who was the captain ofthe winning team?\n",
            "AI: This context does not mention anything about a winning team, so I cannot answer this question from the provided context.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 02**"
      ],
      "metadata": {
        "id": "cU94XKznJNKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "memory = ConversationBufferWindowMemory(k=5)\n",
        "\n",
        "convo = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "id": "dv5v1iDcJIq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = convo.invoke(\"Who won the first cricket world cup? And get some information about it within 3 lines\")\n",
        "print(result['response'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jtnx3DFvJTLJ",
        "outputId": "87efa1bc-8f9d-4605-871e-240cf133cb64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "West Indies won the first Cricket World Cup in 1975. The tournament was held in England, and the West Indies defeated Australia by 17 runs in the final. The tournament was a 60-over format, and 8 teams participated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = convo.invoke(\"How much is 5+5?\")\n",
        "print(result['response'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3eS2yVuJTPf",
        "outputId": "d0b05d2c-b2f2-4a0b-e416-d413110e136d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 + 5 is 10.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = convo.invoke(\"Who was the captain ofthe winning team?\")\n",
        "print(result['response'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLShm7xWJIvJ",
        "outputId": "e1601cd3-cfbd-4cad-a8c4-f5b7ad3c31c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clive Lloyd was the captain of the West Indies team that won the first Cricket World Cup in 1975.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the Memory Infomration:\n",
        "\n",
        "print(convo.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25g9dx0gJYLK",
        "outputId": "f8551da6-db7e-42bf-f358-88be24e98418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Who won the first cricket world cup? And get some information about it within 3 lines\n",
            "AI: West Indies won the first Cricket World Cup in 1975. The tournament was held in England, and the West Indies defeated Australia by 17 runs in the final. The tournament was a 60-over format, and 8 teams participated.\n",
            "Human: How much is 5+5?\n",
            "AI: 5 + 5 is 10.\n",
            "Human: Who was the captain ofthe winning team?\n",
            "AI: Clive Lloyd was the captain of the West Indies team that won the first Cricket World Cup in 1975.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example 03**"
      ],
      "metadata": {
        "id": "iSybqhuWJh_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "memory = ConversationBufferWindowMemory(k=3)"
      ],
      "metadata": {
        "id": "4DjVrVH7KE8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the PromptTemplate:\n",
        "from langchain.prompts import PromptTemplate\n",
        "get_cuisine_template = PromptTemplate(\n",
        "    input_variables=['cuisine'],\n",
        "    template=\"Please give top 5 {cuisine} food.\"\n",
        ")\n",
        "\n",
        "\n",
        "# create a chain:\n",
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=get_cuisine_template, memory=memory)"
      ],
      "metadata": {
        "id": "Hpi4cVroJYmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get Indian Cuisine\n",
        "indian_cuisine_response = chain.invoke('Indian')\n",
        "print(indian_cuisine_response['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c4GAbZwJYrR",
        "outputId": "c83fde20-598d-4820-b4f6-67f6383ad56c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Butter Chicken\n",
            "2. Chicken Tikka Masala\n",
            "3. Palak Paneer\n",
            "4. Biryani\n",
            "5. Samosas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get Begali Cuisine\n",
        "indian_cuisine_response = chain.invoke('Begali')\n",
        "print(indian_cuisine_response['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YaRWO64J0_c",
        "outputId": "d70f1dd8-5a01-4703-ea47-c8016be60e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Macher Jhol (Fish Curry)\n",
            "2. Biryani\n",
            "3. Rasgulla\n",
            "4. Mishti Doi (Sweet Yogurt)\n",
            "5. Sandesh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the Memory Infomration:\n",
        "\n",
        "print(convo.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJELuSzjJ1Di",
        "outputId": "9204380d-9f1f-4979-9757-e22d74f37948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Who won the first cricket world cup? And get some information about it within 3 lines\n",
            "AI: West Indies won the first Cricket World Cup in 1975. The tournament was held in England, and the West Indies defeated Australia by 17 runs in the final. The tournament was a 60-over format, and 8 teams participated.\n",
            "Human: How much is 5+5?\n",
            "AI: 5 + 5 is 10.\n",
            "Human: Who was the captain ofthe winning team?\n",
            "AI: Clive Lloyd was the captain of the West Indies team that won the first Cricket World Cup in 1975.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Document Loaders:**\n",
        "\n",
        "\n",
        "\n",
        "**Definition:**<br>\n",
        "In LangChain, Document Loaders are essentially data connectors. They act as the bridge between your LangChain application and various external data sources, allowing you to bring information into the system for processing.<br>\n",
        "\n",
        "Document Loaders are essential tools for expanding the reach of your LangChain applications. They allow you to leverage information from diverse external sources, enriching your workflows and enabling more comprehensive data-driven tasks.\n",
        "<br><br>\n",
        "\n",
        "**What Documents can they Load ?**<br>\n",
        "Document Loaders are versatile and can handle a wide range of document formats, including:<br>\n",
        "* Text Files (like .txt or .csv),\n",
        "* Spreadsheets (like .csv or .xlsx),\n",
        "* Web Pages (through scraping techniques),\n",
        "* Database entries,\n",
        "* Files from specific directories,\n",
        "* Data from APIs (through custom Loaders).\n",
        "<br><br>\n",
        "\n",
        "**How do they work ?**<br>\n",
        "The core functionality of a Document Loader involves:<br>\n",
        "* **Configuration:** You specify the document source (file path, URL, API endpoint) and any relevant settings for the loader.\n",
        "\n",
        "* **Data Retrieval:** The loader interacts with the specified source and retrieves the data according to its format.\n",
        "\n",
        "* **Preprocessing (Optional):** Some loaders might perform basic cleaning or transformation on the retrieved data before presenting it for further processing.\n",
        "\n",
        "* **Output:** The loader provides the retrieved and potentially preprocessed data to your LangChain workflow.\n",
        "<br><br>\n",
        "\n",
        "**Benefits of Document Loaders:**<br>\n",
        "* **Data Integration:** They enable LangChain applications to access and process information from various sources, enriching the capabilities of your application.\n",
        "\n",
        "* **Flexibility:** The support for diverse document formats allows you to work with a wide range of data.\n",
        "\n",
        "* **treamlined Workflow:** Loaders simplify the process of bringing external data into your LangChain environment.\n",
        "<br><br>\n",
        "\n",
        "**Example Usage:**<br>\n",
        "Imagine you're building a LangChain application that analyzes customer reviews. * You could use a Document Loader to:<br>\n",
        "* Load customer reviews from a text file.\n",
        "* Load survey responses from a spreadsheet.\n",
        "* Scrape review data from a specific website (if publicly available and following ethical scraping practices).\n",
        "<br>\n",
        "\n",
        "By utilizing Document Loaders, you can combine this data from various sources and feed it into your LangChain workflow for tasks like sentiment analysis or topic modeling."
      ],
      "metadata": {
        "id": "uJfizbScKpuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAnl-BHfJ1Hc",
        "outputId": "17a1f532-88e3-498c-c7d5-bb7f74540637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/Dibyendu-Biswas-Resume.pdf\")\n",
        "pages = loader.load()"
      ],
      "metadata": {
        "id": "0L7MekMMKih6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWkQ9vEKKil7",
        "outputId": "0b8f1041-2301-4420-9dea-0afa36fcdd3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Profile\\nA results-driven BCA graduate with a passion for leveraging advanced data science techniques to extract valuable insights\\nfrom complex datasets, my expertise lies in the strategic application of statistical methods, machine learning, deep learning,\\nNLP, computer vision, and state-of-the-art tools. I possess the ability to perform in-depth data analysis and manipulation\\nand can develop full-stack data science projects. Currently, I am expanding my knowledge in MLOps and Generative\\nAI.\\nSkills\\nPython|Excel|Power BI|SQL|Rest-API (Flask)|Operating System (Windows | Ubuntu)|Statistics\\nMachine Learning (Scikit-learn)|Deep Learning (PyTorch | TensorFlow)\\nComputer Vision (Convolutional Neural Network | Image processing | Object Classification)\\nNatural Language Processing (Classification | Summarization | Generation | Name Entity Recognition | RNN | LSTM |\\nGRU | Transformers | GPT)\\nGit|GitHub|ML-Ops (DVC | MLFlow | Docker | Kubernetes | CI-CD (GitHub Actions))|Cloud (AWS)\\nProfessional Experience\\nData Scientist Intern, ineuron.ai Sep\\xa02023 – Dec\\xa02023 | Bangalore, India\\nTech Stack: Python, PyTorch, S3, EC2, ECR, Git, CI/CD (GitHub Actions), DVC, MLflow, Docker.\\n•Engineered a highly efficient Document-Tagging system utilizing state-of-the-art models from Hugging Face; achieved a \\n50% increase in accuracy and reduced manual categorization time by 70%, enhancing data organization and decision-\\nmaking processes.\\n•Spearheaded the implementation of transformer-based models, resulting in a 35% decrease in document categorization \\ntime and a 20% improvement in accuracy after fine-tuning for increased efficiency.\\n•Employed S3 buckets to enhance data management efficiency.\\n•Containerized the project for seamless deployment and reproducibility, hosted on Docker Hub for accessibility.\\n•Established a resilient CI/CD pipeline via GitHub Actions, automating end-to-end processes including Data \\nCollection, Preprocessing, Model Training, Evaluation, and Deployment.\\n•Implemented AWS hosting for the system, ensuring scalability, and optimizing storage and retrieval efficiency.\\nProjects\\nNews Short, Text Summarization Web App Jun\\xa02023 – Aug\\xa02023\\nTech Stack: Python, PyTorch, S3, EC2, ECR, Git, CI/CD (GitHub Actions), DVC, MLflow, Docker.\\n•Developed a Text Summarization Web App with CI/CD integration, automating Data Collection, Preprocessing, \\nModel Training, and Prediction, resulting in a 40% reduction in development time and increased efficiency.\\n•Employed a Fine-Tuning approach in the development process, contributing to the creation of an intuitive web interface \\nfor the Text Summarization App, which led to a 25% improvement in user engagement and overall satisfaction.\\n•Implemented adjustable parameters, empowering users to customize summarization lengths.\\n•Provided real-time updates on local and global events during the summarization process, enhancing user engagement by \\n20% and delivering more dynamic content summaries.\\n•Dockerized the entire project for streamlined deployment and reproducibility, resulting in a 50% reduction in \\ndeployment time and increased collaboration efficiency.\\n•Deployed the Text Summarization App on AWS, achieving 99.9% uptime, ensuring scalability, and delivering reliable \\nperformance for users, contributing to a 15% improvement in overall user experience.Dibyendu Biswas\\nData Scientist\\ndibyendubiswas1998@gmail.com 9907278562 Siliguri, West Bengal\\nhttps://github.com/dibyendubiswas1998 https://www.linkedin.com/in/dibyendubiswas1998\\ndibyendubiswas1998@gmail.com 1 / 2', metadata={'source': '/content/Dibyendu-Biswas-Resume.pdf', 'page': 0}),\n",
              " Document(page_content=\"Insurance Premium Prediction, ML App Apr\\xa02023 – Jun\\xa02023\\nTech Stack: Python, Scikit-learn, S3, EC2, ECR, Git, CI/CD (GitHub Actions), DVC, MLflow, Docker.\\n•Engineered precise Insurance Premium Predictions using diverse machine learning models in Python with Scikit-\\nlearn, achieving a significant 95% improvement in accuracy.\\n•Empowered customers in plan selection by providing accurate estimates, and emphasizing health aspects over financial \\nconsiderations.\\n•Conducted a comparative analysis of machine learning models, resulting in a trustworthy 95% improvement in \\npremium prediction accuracy.\\n•Implemented enhancements for a reliable 95% improvement in premium prediction accuracy, ensuring trustworthy \\nestimates for users.\\n•Utilized S3 bucket for efficient data management, optimizing the storage and retrieval processes.\\n•Containerized the application for seamless deployment and reproducibility. Established a robust CI/CD pipeline with \\nGit, GitHub Actions, and DVC for automated expense estimate generation. Tracked experiments in DagsHub using \\nMLflow for comprehensive documentation. Deployed on AWS for scalability and dependable performance.\\nSupply Chain Insights in FMCG Domain Mar\\xa02023 – Apr\\xa02023\\nTech Stack: Excel, Power BI, Microsoft PowerPoint.\\n•Constructed a comprehensive Power BI dashboard focused on key performance indicators (KPIs) such as on-time \\ndelivery (OT), in-full delivery (IF), and on-time in-full (OTIF) percentages, effectively tracking service levels for \\nAtliQ Mart, an FMCG manufacturer.\\n•The Power BI dashboard facilitated proactive service management at AtliQ Mart, enabling the identification and \\nresolution of key customer issues before business expansion.\\n•The analysis conducted had the potential to save 20% of AtliQ Mart's investment in expansion by addressing critical \\nissues highlighted in the dashboard.\\n•Developed a final report summarizing supply chain insights and their potential impact, shared with stakeholders.\\n•Utilized Excel for data analysis and Microsoft PowerPoint for the creation and presentation of the final report.\\n•Contributed to enhanced decision-making processes by providing actionable insights through the Power BI dashboard.\\nRevenue Insights in Hospitality Domain Jan\\xa02023 – Feb\\xa02023\\nTech Stack: Excel, Power BI, Microsoft PowerPoint.\\n•Conducted a detailed analysis for Atliq Grands to identify the root causes of declining market share and revenue.\\n•Developed a Power BI dashboard using three months of data, offering a comprehensive view of revenue trends for \\nAtliq Grands.\\n•Utilized the Power BI dashboard to provide valuable insights to the Revenue team, enabling strategic actions for \\nregaining lost revenue and market share.\\n•Insights gathered from the dashboard aimed at contributing to a significant recovery, projecting a goal of increasing \\nrevenue and market share by 20% in the next month.\\n•Created a final report summarizing revenue insights and proposed strategies, sharing it with stakeholders.\\n•Employed Excel for data analysis and Microsoft PowerPoint for the creation and presentation of the final report.\\nEducation\\nBCA, IGNOU 2023 | Siliguri, India\\nPersonal Information\\n•Date of Birth: 12-Mar-1999.\\n•Father’s Name: Tapan Kumar Biswas.\\n•Nationality: Indian.\\n•Marital Status: Single.\\n•Languages: English, Bengali, and Hindi.\\n•Hobbies: listening to music, and Travelling.\\ndibyendubiswas1998@gmail.com 2 / 2\", metadata={'source': '/content/Dibyendu-Biswas-Resume.pdf', 'page': 1})]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "attoeLCJKiqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VHGAckHVKiuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e0tHrP8RKiyb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}