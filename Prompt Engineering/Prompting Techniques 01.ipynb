{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPmsejwydLcB3gV/gkd5xLp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Basic Prompting Techniques:**\n","\n","Here we learn how to write prompt in best possible way:\n","\n","* String Prompt Template,\n","* Chat Prompt Template,\n","* System Prompt,\n","* Message Place Holder,\n","* Memory\n"],"metadata":{"id":"lJ11KOuR9-Uq"}},{"cell_type":"code","source":["# install necessary libaries:\n","\n","%pip install --upgrade --quiet sentence_transformers\n","%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-google-genai langchain-chroma bs4 boto3\n","%pip install --upgrade --quiet langchain-aws"],"metadata":{"id":"nu7n1sjJ9RiY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Load the LLM:**"],"metadata":{"id":"b-j3fufN-UQi"}},{"cell_type":"code","source":["# Load the Tokens:\n","\n","from google.colab import userdata\n","import os\n","\n","os.environ['GOOGLE_API_KEY'] = userdata.get('GEMINI_API_KEY')\n","os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"],"metadata":{"id":"N7kY93bt9Rk9","executionInfo":{"status":"ok","timestamp":1733991982877,"user_tz":-330,"elapsed":8308,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","model1 = \"gemini-pro\"\n","model2 = \"gemini-1.5-pro\"\n","\n","llm = ChatGoogleGenerativeAI(\n","    model=model1,\n","    temperature=0.4,\n","    max_tokens=512,\n","    timeout=None,\n","    max_retries=2,\n","    # other params...\n",")\n","\n","print(llm.invoke(\"hi\").content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qdcQIg7H9RnV","executionInfo":{"status":"ok","timestamp":1733992803245,"user_tz":-330,"elapsed":922,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"9fd241e3-701a-452e-aeac-d597df78e8f3"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello there! How can I assist you today?\n"]}]},{"cell_type":"code","source":["llm_2 = ChatGoogleGenerativeAI(\n","    model=model2,\n","    temperature=0.4,\n","    max_tokens=512,\n","    timeout=None,\n","    max_retries=2,\n","    # other params...\n",")"],"metadata":{"id":"hJVHy54vP52m","executionInfo":{"status":"ok","timestamp":1733996560029,"user_tz":-330,"elapsed":2,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":["## **Load Embeddings from HF:**"],"metadata":{"id":"u4BxDVqw_TJG"}},{"cell_type":"code","source":["# Get the Embeddings:\n","\n","from langchain.embeddings import HuggingFaceEmbeddings\n","\n","model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n","embeddings = HuggingFaceEmbeddings(model_name=model_name)"],"metadata":{"id":"oNP-UkLw9Rpv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Vector Store:**"],"metadata":{"id":"QoT5xBCM_WU6"}},{"cell_type":"code","source":["import bs4\n","from langchain import hub\n","from langchain_chroma import Chroma\n","from langchain_community.document_loaders import WebBaseLoader\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","\n","# Load, chunk and index the contents of the blog.\n","loader = WebBaseLoader(\n","    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n","    bs_kwargs=dict(\n","        parse_only=bs4.SoupStrainer(\n","            class_=(\"post-content\", \"post-title\", \"post-header\")\n","        )\n","    ),\n",")\n","docs = loader.load()\n","\n","# Splitting:\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","splits = text_splitter.split_documents(docs)\n","vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)"],"metadata":{"id":"zXhTCkGR9RsP","executionInfo":{"status":"ok","timestamp":1733992286462,"user_tz":-330,"elapsed":2,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# retriever:\n","retriever = vectorstore.as_retriever(search_kwargs=dict(k=10))"],"metadata":{"id":"AUlwz7PpDzhz","executionInfo":{"status":"ok","timestamp":1733993386217,"user_tz":-330,"elapsed":428,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["## **PromptTemplates:**\n","\n","<u>Prompt templates help to translate user input and parameters into instructions for a language model.</u> This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.<br>\n","\n","Prompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.<br>\n","\n","<u>Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages.</u> The reason this PromptValue exists is to make it easy to switch between strings and messages.<br>\n","\n","There are a few different types of prompt templates:\n","\n","* **String PromptTemplates**\n","* **ChatPromptTemplates**"],"metadata":{"id":"L6i9kx4C_5qS"}},{"cell_type":"markdown","source":["### **String PromptTemplates:**\n","\n","**These prompt templates are used to format a single string, and generally are used for simpler inputs.** For example, a common way to construct and use a PromptTemplate is as follows:"],"metadata":{"id":"-bvCO8mKA6bx"}},{"cell_type":"code","source":["from langchain_core.prompts import PromptTemplate\n","\n","\n","prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n","prompt_template.invoke({\"topic\": \"cats\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iSfPUdQp9Ruf","executionInfo":{"status":"ok","timestamp":1733992665238,"user_tz":-330,"elapsed":458,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"30f8b253-d28a-47e5-9789-893c98048932"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["StringPromptValue(text='Tell me a joke about cats')"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["#### **Example 00:**"],"metadata":{"id":"rYjPyo8tGmLD"}},{"cell_type":"markdown","source":["**Prompt Template with variables:**\n","\n","This is the much more common use case for prompt templates. We have a template string, which we want to dynamically be able to replace certain parts of that string only.<br>\n","\n","The variable parts in the template are surround by curly brackets **{ }**, and to fill these parts we pass in a list of key-value pairs (kwargs in python) with the variable name and text they should be filled with to the **format()** method on the Prompt Template."],"metadata":{"id":"2-0NGxATGjp-"}},{"cell_type":"code","source":["prompt_template = PromptTemplate.from_template(\n","    'Tell me a {adjective} joke about {content}'\n",")\n","print(prompt_template.format(adjective='funny', content='chickens'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i8xgYPOqGhyE","executionInfo":{"status":"ok","timestamp":1733994188476,"user_tz":-330,"elapsed":458,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"5339a447-ebed-41eb-9e39-9da3b21c627b"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Tell me a funny joke about chickens\n"]}]},{"cell_type":"markdown","source":["**Creating Prompt Templates via the Constructor:**<br>\n","\n","We can also create a PromptTemplate via the constructor, instead of via the **from_template** method."],"metadata":{"id":"m3fvSNZxHIN3"}},{"cell_type":"code","source":["prompt_template = PromptTemplate(\n","    template='Tell me a {adjective} joke about {content}',\n","    input_variables=['adjective', 'cotent']\n",")\n","print(prompt_template.format(adjective='funny', content='chickens'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"do8X58llGh0h","executionInfo":{"status":"ok","timestamp":1733994281493,"user_tz":-330,"elapsed":614,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"50e95faa-50d0-46ee-a30a-8824cde67163"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Tell me a funny joke about chickens\n"]}]},{"cell_type":"markdown","source":["**Prompt Templates with Multiline Strings and Variables:** <br>\n","\n","This is actually just the same as a normal string. You just need to include triple quotes at the start and end and voila. Single or double quotes work the same.\n"],"metadata":{"id":"Xx034rJfHtco"}},{"cell_type":"code","source":["template = '''You are a joke generating chatbot.\n","Provide funny jokes based on the themes requested.\n","\n","Question: Tell me a {adjective} joke about {content}\n","\n","Answer: '''\n","\n","prompt_template = PromptTemplate.from_template(template)\n","print(prompt_template.format(adjective='funny', content='chickens'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TnOqNAgaGh21","executionInfo":{"status":"ok","timestamp":1733994427544,"user_tz":-330,"elapsed":442,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"ba9f1cfe-52e8-4c96-8c7c-167bf8053fff"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["You are a joke generating chatbot.\n","Provide funny jokes based on the themes requested.\n","\n","Question: Tell me a funny joke about chickens\n","\n","Answer: \n"]}]},{"cell_type":"markdown","source":["**Prompt Templates with f-strings and Variables:**<br>\n","\n","These come along all the time, and are probably the foundation for my confusion with template strings, which probably points at a lack of python knowledge, v.s anything actually do to with LangChain."],"metadata":{"id":"iZigXGkKH3mu"}},{"cell_type":"code","source":["from datetime import date\n","\n","today = date.today()\n","\n","prompt_template = PromptTemplate.from_template(\n","    f'Todays Date: {today}: Tell me a {{adjective}} joke about {{content}}',\n","    template_format='f-string'\n",")\n","print(prompt_template.format(adjective=\"funny\", content=\"chickens\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"69UM4xrPHTEy","executionInfo":{"status":"ok","timestamp":1733994468844,"user_tz":-330,"elapsed":540,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"81130642-1dd5-4c8e-e688-535fc79985e3"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Todays Date: 2024-12-12: Tell me a funny joke about chickens\n"]}]},{"cell_type":"markdown","source":["**Prompt Templates with Multiline f-strings and Variables:**<br>\n","\n","This is just the same as the above, except again, we use triple quotes for the start and end of multiline strings, even if they are **f-strings**."],"metadata":{"id":"5rt3665TIAZL"}},{"cell_type":"code","source":["template = f'''You are a joke generating chatbot.\n","Provide funny jokes based on the themes requested.\n","\n","Question: Tell me a {{adjective}} joke about {{content}}'\n","\n","Answer: '''\n","\n","prompt_template = PromptTemplate.from_template(template)\n","print(prompt_template.format(adjective='funny', content='chickens'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cpk74003IAEq","executionInfo":{"status":"ok","timestamp":1733994508238,"user_tz":-330,"elapsed":444,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"10bf71a4-fa5e-45a8-d421-e9ecc0a11817"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["You are a joke generating chatbot.\n","Provide funny jokes based on the themes requested.\n","\n","Question: Tell me a funny joke about chickens'\n","\n","Answer: \n"]}]},{"cell_type":"markdown","source":["**Using a PromptTemplate in LLM Calls:**<br>\n","\n"],"metadata":{"id":"HVFaOMpzIJRL"}},{"cell_type":"code","source":["prompt_template = PromptTemplate.from_template(\n","    'Tell me a {adjective} joke about {content}'\n",")\n","\n","chain = prompt_template | llm\n","chain.invoke({'adjective': 'funny', 'content': 'chickens'}).content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"DEMbg_GJHTHC","executionInfo":{"status":"ok","timestamp":1733994626579,"user_tz":-330,"elapsed":1639,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"8dc4508d-3e8f-4b78-8103-3806047730bc"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Why did the chicken go to the s√©ance?\\n\\nTo get to the bottom of its clucking problem!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":[],"metadata":{"id":"ND9iQgUIGDsy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Example 01:**"],"metadata":{"id":"I3gJlR9QBtzx"}},{"cell_type":"code","source":["template = \"\"\"Your are a helpful AI assistant, your name is Lili, designed by Dibyendu Biswas, an AI/ML Engineer.\n","Answer the question based only on the following context: {context}\n","\n","Question: {question}\n","\"\"\"\n","\n","\n","prompt = PromptTemplate.from_template(template)"],"metadata":{"id":"u1uiZmGb9Rw_","executionInfo":{"status":"ok","timestamp":1733993364271,"user_tz":-330,"elapsed":9,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["from langchain.schema.output_parser import StrOutputParser\n","from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n","from langchain_core.runnables import RunnableParallel, RunnablePassthrough , RunnableLambda\n","from IPython.display import display, Markdown\n","\n","retrieveal_chain = (\n","    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n","    )"],"metadata":{"id":"2vCktNxn9Rzf","executionInfo":{"status":"ok","timestamp":1733993391774,"user_tz":-330,"elapsed":738,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["response = retrieveal_chain.invoke(\"What are the approaches to Task Decomposition?\")\n","display(Markdown(response))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":64},"id":"fVJ93kmB9R2P","executionInfo":{"status":"ok","timestamp":1733993395016,"user_tz":-330,"elapsed":1857,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"6b7a912a-2407-46ad-b1c3-23b3fd407a9d"},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs."},"metadata":{}}]},{"cell_type":"markdown","source":["#### **Example 02:**"],"metadata":{"id":"cKZBt-WpFNgi"}},{"cell_type":"code","source":["template = \"\"\"\n","You are an AI-powered virtual assistant, your name is 'Lily', designed by Dibyendu Biswas, an AI Engineer.\n","Your task is to answer based on user's query in detailed way.\n","Use the following pieces of context to answer the within 164 words.\n","If you don't know the answer, just say that 'I don't have enough information to answer this question'.\n","Provide only the helpful answer. Do not include any other information.\n","\n","Whenever people ask the generaal question you must answer it as well, like:\n","Question: Hi\n","Answer: Hello! How can I assist you with your studies today?\n","\n","Question: What is your name?\n","Answer: I am Lily, your virtual assistant designed by Dibyendu Biswas, an AI Engineer.\n","\n","Context: `{context}`\n","Question: `{question}`\n","\"\"\"\n","\n","prompt = PromptTemplate(\n","    template=template,\n","    input_variables=['context', 'question']\n",")\n","\n","chain = (\n","    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n","    )"],"metadata":{"id":"k-vnwisnFMwQ","executionInfo":{"status":"ok","timestamp":1733993930445,"user_tz":-330,"elapsed":501,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["response = chain.invoke(\"What are the approaches to Task Decomposition?\")\n","display(Markdown(response))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120},"id":"G2m_UYtgFMyk","executionInfo":{"status":"ok","timestamp":1733993889836,"user_tz":-330,"elapsed":2473,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"538a0e12-da56-4dd5-d93f-dfb3ee984f06"},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Task decomposition can be done in three ways:\n1. By using LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\"\n2. By using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel\n3. With human inputs"},"metadata":{}}]},{"cell_type":"code","source":["response = chain.invoke(\"Tell me something about yourself.\")\n","display(Markdown(response))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":64},"id":"wrM8ukpgFM03","executionInfo":{"status":"ok","timestamp":1733993935192,"user_tz":-330,"elapsed":2122,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"98f4799d-1742-4e71-96e2-d4a623064701"},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"I am Lily, your virtual assistant designed by Dibyendu Biswas, an AI Engineer. I am powered by GPT-3.5 and have access to a vast knowledge base. I am here to help you with your studies and any other tasks you may have."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"nw4tHzgSFM3Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **ChatPromptTemplate:**"],"metadata":{"id":"_YnD8yKsKOFB"}},{"cell_type":"markdown","source":["**ChatPromptTemplate:**<br>\n","The **ChatPromptTemplate** is a LangChain utility for creating structured prompts. It helps organize different components of a conversation, like system messages, user inputs, and placeholders for dynamic variables.<br>\n","\n","**Key Features:**\n","* Allows mixing **system prompts**, **human messages**, and **AI messages**.\n","* Dynamically fills placeholders during runtime with specific values.\n","* Ideal for applications requiring structured conversations, like chatbots."],"metadata":{"id":"pKZsQzo8NgoD"}},{"cell_type":"markdown","source":["**MessagesPlaceholder:**<br>\n","<u>The **MessagesPlaceholder** is a placeholder for inserting a list of messages dynamically during the execution.</u> It allows you **to maintain conversational memory** by incorporating previous exchanges between the user and the AI model.<br>\n","\n","**Why It's Important:**\n","* Helps maintain context across **multi-turn conversations**.\n","* Allows seamless integration of stored chat history."],"metadata":{"id":"vv0TV_hhN9xz"}},{"cell_type":"markdown","source":["**System Prompt:**<br>\n","The **System Prompt** sets the behavior, tone, or role of the LLM in a conversation. It provides high-level guidance to the model before processing user inputs.<br>\n","\n","**Purpose of a System Prompt:**\n","* Define the role of the LLM (e.g., **tutor**, **assistant**, **interviewer**).\n","* Outline constraints or rules for the model's responses.\n","* Set the tone (e.g., **formal**, **friendly**, **concise**)."],"metadata":{"id":"hwziyj1iOqnD"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","prompt_template = ChatPromptTemplate([\n","    (\"system\", \"You are a helpful assistant\"),\n","    (\"human\", \"Tell me a joke about {topic}\")\n","])\n","\n","prompt_template.invoke({\"topic\": \"cats\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I-Ve6uRRFM5k","executionInfo":{"status":"ok","timestamp":1733996391563,"user_tz":-330,"elapsed":507,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"c47706fd-378a-464d-8003-5f27133b0bd3"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["#### **Example 00:**"],"metadata":{"id":"XkCRHrQ-QbQp"}},{"cell_type":"code","source":["prompt_template = ChatPromptTemplate.from_messages([\n","   (\"system\",\"You are a helpful AI Assistant with a sense of humor\"),\n","   (\"human\",\"Hi how are you?\"),\n","   (\"ai\",\"I am good. How can I help you?\"),\n","   (\"human\",\"{input}\")\n","])\n","\n","prompt_template.invoke({\"input\": \"Tell me a joke about chickens\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YxMbNI1mFM8e","executionInfo":{"status":"ok","timestamp":1733996454366,"user_tz":-330,"elapsed":445,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"56debf2a-711b-4f72-ccd5-2ace4d9d5ea4"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI Assistant with a sense of humor', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hi how are you?', additional_kwargs={}, response_metadata={}), AIMessage(content='I am good. How can I help you?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about chickens', additional_kwargs={}, response_metadata={})])"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["prompt_template = ChatPromptTemplate.from_messages([\n","   (\"system\",\"You are a helpful AI Assistant with a sense of humor\"),\n","   (\"human\",\"Hi how are you?\"),\n","   (\"ai\",\"I am good. How can I help you?\"),\n","   (\"human\",\"{input}\")\n","])\n","\n","chain = prompt_template | llm_2\n","chain.invoke({\"input\": \"Tell me a joke about chickens\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iTqjWwApFM-7","executionInfo":{"status":"ok","timestamp":1733996574569,"user_tz":-330,"elapsed":2085,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"c88bd8ed-c9c4-44be-feb3-d1efcc8680e5"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content=\"Why did the chicken cross the playground?\\n\\nTo get to the other slide! \\n\\n...I apologize if you were expecting sophisticated humor.  I'm still working on my comedic timing.  Do you want to hear another one?\\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-50a8393d-2caa-481f-b1f0-c0ba13df91d3-0', usage_metadata={'input_tokens': 35, 'output_tokens': 49, 'total_tokens': 84, 'input_token_details': {'cache_read': 0}})"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["# Example 03:\n","\n","prompt_template = ChatPromptTemplate([\n","    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n","    (\"human\", \"Hello, how are you doing?\"),\n","    (\"ai\", \"I'm doing well, thanks!\"),\n","    (\"human\", \"{user_input}\"),\n","])\n","\n","prompt_value = prompt_template.invoke(\n","    {\n","        \"name\": \"Bob\",\n","        \"user_input\": \"What is your name?\"\n","    }\n",")\n","\n","prompt_value"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MjgNr5P99R56","executionInfo":{"status":"ok","timestamp":1733996764925,"user_tz":-330,"elapsed":572,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"76be7d57-c08a-43d1-f600-1e7a33e3175b"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})])"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","source":["#### **Example 02:**\n","\n","Use\n","* **System Prompt**"],"metadata":{"id":"wblAW7C2RUtY"}},{"cell_type":"code","source":["from langchain.prompts import ChatPromptTemplate\n","from langchain.schema import SystemMessage, HumanMessage"],"metadata":{"id":"6CnCNs_XQtn7","executionInfo":{"status":"ok","timestamp":1733997421724,"user_tz":-330,"elapsed":503,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["# Example 01:\n","\n","system_prompt = SystemMessage(\n","    content=(\"You are an AI tutor specializing in math and science for K-12 students. \"\n","             \"You provide clear, step-by-step explanations and encourage students to learn. \"\n","             \"If a question is unclear, ask for clarification.\")\n",")\n","\n","\n","prompt = ChatPromptTemplate.from_messages(\n","    [\n","        system_prompt,\n","        (\"human\", \"{question}\"),\n","        (\"ai\", \"{ai_message}\"),\n","    ]\n",")\n","\n","prompt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gzeXr4C5ST8M","executionInfo":{"status":"ok","timestamp":1733997398674,"user_tz":-330,"elapsed":750,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"4d201359-9a1a-4542-db1d-d96a15274731"},"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptTemplate(input_variables=['ai_message', 'question'], input_types={}, partial_variables={}, messages=[SystemMessage(content='You are an AI tutor specializing in math and science for K-12 students. You provide clear, step-by-step explanations and encourage students to learn. If a question is unclear, ask for clarification.', additional_kwargs={}, response_metadata={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['ai_message'], input_types={}, partial_variables={}, template='{ai_message}'), additional_kwargs={})])"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["response = prompt.invoke({\n","    \"question\": \"How are you?\",\n","    \"ai_message\": \"I'm good, thank you. How can I help you?\"\n","})\n","\n","response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MNsEIILWSUeC","executionInfo":{"status":"ok","timestamp":1733997483268,"user_tz":-330,"elapsed":422,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"ea4cf575-06e6-47b5-d7bc-1aeeac09a78c"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptValue(messages=[SystemMessage(content='You are an AI tutor specializing in math and science for K-12 students. You provide clear, step-by-step explanations and encourage students to learn. If a question is unclear, ask for clarification.', additional_kwargs={}, response_metadata={}), HumanMessage(content='How are you?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm good, thank you. How can I help you?\", additional_kwargs={}, response_metadata={})])"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["#### **Example 02:**\n","\n","Use,\n","* **MessagesPlaceholder**,\n","* **System Prompt**\n","\n","<br>\n","\n","The **MessagesPlaceholder** is a placeholder for inserting a **list of messages dynamically during the execution**. <u>It allows you to maintain **conversational memory** by incorporating **previous exchanges** between the **user** and the **AI** model.</u>"],"metadata":{"id":"SuuO9f5hTyGw"}},{"cell_type":"code","source":["# Example 02:\n","\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.messages import HumanMessage\n","\n","\n","\n","system_prompt = SystemMessage(\n","    content=(\"You are an AI tutor specializing in math and science for K-12 students. \"\n","             \"You provide clear, step-by-step explanations and encourage students to learn. \"\n","             \"If a question is unclear, ask for clarification.\")\n",")\n","\n","\n","prompt = ChatPromptTemplate.from_messages(\n","    [\n","        system_prompt, # define the behaviour of llm\n","        MessagesPlaceholder(variable_name=\"chat_history\") # chat_history will be pass dynamically\n","    ]\n",")\n","\n","prompt_value = prompt.invoke(\n","    {\n","        \"chat_history\": [\n","            (\"human\", \"Hi!\"),\n","            (\"ai\", \"How can I assist you today?\"),\n","            (\"human\", \"What is 2+2?\"),\n","            (\"ai\", \"2+2 is 4.\")\n","        ]\n","    }\n",")\n","\n","prompt_value"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2l2Wn_AVSUgW","executionInfo":{"status":"ok","timestamp":1733998062558,"user_tz":-330,"elapsed":683,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"ee31c433-a33a-493c-e35b-1bc25f342406"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptValue(messages=[SystemMessage(content='You are an AI tutor specializing in math and science for K-12 students. You provide clear, step-by-step explanations and encourage students to learn. If a question is unclear, ask for clarification.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}), AIMessage(content='How can I assist you today?', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is 2+2?', additional_kwargs={}, response_metadata={}), AIMessage(content='2+2 is 4.', additional_kwargs={}, response_metadata={})])"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["# Example 02:\n","\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","\n","prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", \"You are a helpful assistant.\"),\n","        MessagesPlaceholder(\"history\"),\n","        (\"human\", \"{question}\")\n","    ]\n",")\n","\n","\n","prompt.invoke(\n","   {\n","       \"history\": [\n","           (\"human\", \"what's 5 + 2\"),\n","           (\"ai\", \"5 + 2 is 7\")\n","        ],\n","       \"question\": \"now multiply that by 4\"\n","   }\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LIptlI1NSUiv","executionInfo":{"status":"ok","timestamp":1733998180951,"user_tz":-330,"elapsed":429,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"167b869b-f1d7-4df9-8172-64e50362a6bd"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"what's 5 + 2\", additional_kwargs={}, response_metadata={}), AIMessage(content='5 + 2 is 7', additional_kwargs={}, response_metadata={}), HumanMessage(content='now multiply that by 4', additional_kwargs={}, response_metadata={})])"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["# Example 03:\n","# In MessagesPlaceholder, limiting the number of messages:\n","\n","from langchain_core.prompts import MessagesPlaceholder\n","\n","\n","chat_history = [\n","    ('human', 'Hi!'),\n","    ('ai', 'Hello! How can I assist you today?'),\n","    ('human', 'What is 2+2?'),\n","    ('ai', '2+2 is 4.'),\n","    ('human', \"what's 5 + 2\"),\n","    (\"ai\", \"5 + 2 is 7\"),\n","    ('human', 'Tell me something about yourself.'),\n","    ('ai', 'I am an AI tutor specializing in math and science for K-12 students')\n","]\n","\n","\n","\n","prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", \"You are a helpful assistant.\"),\n","        MessagesPlaceholder(variable_name=\"chat_history\", n_messages=3),\n","        (\"human\", \"{question}\")\n","    ]\n",")\n","\n","\n","prompt.invoke(\n","   {\n","       \"chat_history\": chat_history,\n","       \"question\": \"now multiply that by 4\"\n","   }\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HDlJ36TXSUlF","executionInfo":{"status":"ok","timestamp":1733998560618,"user_tz":-330,"elapsed":476,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"22562b28-45b6-48d0-e775-9cd8f61f8c36"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}), AIMessage(content='5 + 2 is 7', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me something about yourself.', additional_kwargs={}, response_metadata={}), AIMessage(content='I am an AI tutor specializing in math and science for K-12 students', additional_kwargs={}, response_metadata={}), HumanMessage(content='now multiply that by 4', additional_kwargs={}, response_metadata={})])"]},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","source":["#### **Example 03:**\n","\n","Implement\n","* **Retriever Prompt**,\n","* **SystemPrompt**,\n","* **MessagesPlaceholder**\n","* **Memory**\n","  * BaseChatMessageHistory,\n","  * InMemoryChatMessageHistory,\n","  * RunnableWithMessageHistory,\n","  * trim_messages\n","* **Session**\n","* **Trim the chat history**"],"metadata":{"id":"WAxhGoyUbF3M"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain.chains import create_history_aware_retriever\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","from langchain.chains import create_retrieval_chain\n","from langchain_core.messages import HumanMessage, AIMessage"],"metadata":{"id":"sqkWRz14Z_vC","executionInfo":{"status":"ok","timestamp":1734001100268,"user_tz":-330,"elapsed":415,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["# Step 1: Create History-Aware-Retriever:\n","\n","retriever_prompt = (\n","    \"Given a chat history and the latest user question which might reference context in the chat history,\"\n","    \"formulate a standalone question which can be understood without the chat history.\"\n","    \"Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",")\n","\n","\n","contextualize_q_prompt  = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", retriever_prompt),\n","        MessagesPlaceholder(variable_name=\"chat_history\"),\n","        (\"human\", \"{input}\"),\n","\n","\n","     ]\n",")\n","\n","history_aware_retriever = create_history_aware_retriever(llm_2, retriever, contextualize_q_prompt)"],"metadata":{"id":"sFNuswSHZ_xV","executionInfo":{"status":"ok","timestamp":1734001135282,"user_tz":-330,"elapsed":694,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["# Step 2: Define the Custom System Prompts and Create Question-Aware-Chain:\n","\n","\n","system_prompt = (\n","    \"You are an AI assistant, named 'Lily', designed by Dibyendu Biswas (a AI Engineer) for question-answering tasks. \"\n","    \"Use the following pieces of retrieved context to answer the question \"\n","    \"If you don't know the answer, say that you don't know.\"\n","    \"Use three sentences maximum and keep the answer concise.\"\n","    \"\\n\\n\"\n","    \"{context}\"\n",")\n","\n","qa_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", system_prompt),\n","        MessagesPlaceholder(variable_name=\"chat_history\"),\n","        (\"human\", \"{input}\"),\n","    ]\n",")\n","\n","\n","question_answer_chain = create_stuff_documents_chain(llm_2, qa_prompt)"],"metadata":{"id":"7JMxug16Z_zx","executionInfo":{"status":"ok","timestamp":1734001232067,"user_tz":-330,"elapsed":424,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["# Step 3: Create Rag-Chain using history_aware_retriever and question_answer_chain:\n","\n","rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"],"metadata":{"id":"uzcPsRa3Z_18","executionInfo":{"status":"ok","timestamp":1734001249052,"user_tz":-330,"elapsed":504,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["# Step 4: Use the Memory and Session to store the current conversation:\n","\n","from langchain_community.chat_message_histories import ChatMessageHistory\n","from langchain_core.chat_history import BaseChatMessageHistory\n","from langchain_core.runnables.history import RunnableWithMessageHistory"],"metadata":{"id":"E-Riv27LZ_6J","executionInfo":{"status":"ok","timestamp":1734001265394,"user_tz":-330,"elapsed":411,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["# Create a Session to Store Conversation:\n","\n","store = {}\n","\n","def get_session_history(session_id: str) -> BaseChatMessageHistory:\n","    if session_id not in store:\n","        store[session_id] = ChatMessageHistory()\n","    return store[session_id]"],"metadata":{"id":"V692eANCZ_8F","executionInfo":{"status":"ok","timestamp":1734001316264,"user_tz":-330,"elapsed":518,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["# Create Conversational Rag Chain using memory and rag_chain:\n","\n","# conversational_rag_chain:\n","conversational_rag_chain = RunnableWithMessageHistory(\n","    rag_chain,\n","    get_session_history,\n","    input_messages_key=\"input\",\n","    history_messages_key=\"chat_history\",\n","    output_messages_key=\"answer\",\n",")"],"metadata":{"id":"0zpeFkWNZ_-o","executionInfo":{"status":"ok","timestamp":1734001329431,"user_tz":-330,"elapsed":410,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["# Generate Response and store the current user session:\n","\n","conversational_rag_chain.invoke(\n","    {\"input\": \"What is Chain of Thought Prompt?\"},\n","    config={\n","        \"configurable\": {\"session_id\": \"abc123\"}\n","    },  # constructs a key \"abc123\" in `store`.\n",")[\"answer\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"YZYttzH8SUnW","executionInfo":{"status":"ok","timestamp":1734001361603,"user_tz":-330,"elapsed":2614,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"e14e723b-55ef-4100-c77d-f6eb2ea2f387"},"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Chain of Thought (CoT) prompting is a technique that enhances large language model performance on complex tasks by instructing the model to \"think step by step\".  This decomposes the task into smaller, simpler steps, making it more manageable.  CoT also provides insights into the model\\'s reasoning process.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":71}]},{"cell_type":"code","source":["conversational_rag_chain.invoke(\n","    {\"input\": \"What are common ways of doing it?\"},\n","    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",")[\"answer\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"-vwRClcPiOSq","executionInfo":{"status":"ok","timestamp":1734001366887,"user_tz":-330,"elapsed":3145,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"7aec0670-e5d5-450b-9828-3bcd448535df"},"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'CoT can be elicited by prompting the LLM with instructions like \"think step by step\" or by providing a few-shot examples demonstrating the desired step-by-step reasoning process.  More advanced methods like Tree of Thoughts extend CoT by exploring multiple reasoning possibilities at each step to form a tree-like search structure.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["store"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"It3u8m5AiOUt","executionInfo":{"status":"ok","timestamp":1734001375052,"user_tz":-330,"elapsed":422,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"6d17d5d8-98ba-4b64-9ca6-a64c03e72cdd"},"execution_count":73,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'abc123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is Chain of Thought Prompt?', additional_kwargs={}, response_metadata={}), AIMessage(content='Chain of Thought (CoT) prompting is a technique that enhances large language model performance on complex tasks by instructing the model to \"think step by step\".  This decomposes the task into smaller, simpler steps, making it more manageable.  CoT also provides insights into the model\\'s reasoning process.\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content='What are common ways of doing it?', additional_kwargs={}, response_metadata={}), AIMessage(content='CoT can be elicited by prompting the LLM with instructions like \"think step by step\" or by providing a few-shot examples demonstrating the desired step-by-step reasoning process.  More advanced methods like Tree of Thoughts extend CoT by exploring multiple reasoning possibilities at each step to form a tree-like search structure.\\n', additional_kwargs={}, response_metadata={})])}"]},"metadata":{},"execution_count":73}]},{"cell_type":"code","source":["for message in store[\"abc123\"].messages:\n","    if isinstance(message, AIMessage):\n","        prefix = \"AI\"\n","    else:\n","        prefix = \"User\"\n","\n","    print(f\"{prefix}: {message.content} \\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"teVU0Z1fiTJm","executionInfo":{"status":"ok","timestamp":1734001463069,"user_tz":-330,"elapsed":718,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"9ef2c999-ceb5-4b58-b197-2cfa5b86f063"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["User: What is Chain of Thought Prompt? \n","\n","AI: Chain of Thought (CoT) prompting is a technique that enhances large language model performance on complex tasks by instructing the model to \"think step by step\".  This decomposes the task into smaller, simpler steps, making it more manageable.  CoT also provides insights into the model's reasoning process.\n"," \n","\n","User: What are common ways of doing it? \n","\n","AI: CoT can be elicited by prompting the LLM with instructions like \"think step by step\" or by providing a few-shot examples demonstrating the desired step-by-step reasoning process.  More advanced methods like Tree of Thoughts extend CoT by exploring multiple reasoning possibilities at each step to form a tree-like search structure.\n"," \n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"okHgkr7wicr-"},"execution_count":null,"outputs":[]}]}