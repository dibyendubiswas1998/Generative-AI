{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN6As/PTOP2N+oUr90Mvnjw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Parent Document Retriever:**"],"metadata":{"id":"5eO8CaBJilgo"}},{"cell_type":"code","source":["# Install the Dependencies:\n","\n","\n","!pip install langchain -qU\n","!pip install langchain_community -qU\n","!pip install langchain_google_genai -qU\n","!pip install boto3 -qU\n","!pip install \"pinecone[grpc]\" -qU\n","!pip install langchain_chroma -qU"],"metadata":{"id":"6GH17G7vikf-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get Embeddings:\n","\n","import boto3\n","from langchain.llms.bedrock import Bedrock\n","from langchain.embeddings import BedrockEmbeddings\n","\n","AWS_REGION = 'us-east-1'\n","AWS_ACCESS_KEY = 'AKIA6ODU6VC2NYPVGUUG'\n","AWS_SECRET_KEY = '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'\n","\n","def get_embeddings():\n","  try:\n","    bedrock_client = boto3.client(\n","          service_name = \"bedrock-runtime\",\n","          region_name = AWS_REGION,\n","          aws_access_key_id = AWS_ACCESS_KEY,\n","          aws_secret_access_key = AWS_SECRET_KEY,\n","      )\n","    # The model_id was incorrect. Changing it to amazon.titan-text-embed-v1\n","    bedrock_embedding = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrock_client)\n","    return bedrock_embedding\n","\n","  except Exception as ex:\n","    return ex\n","\n","embedding_model = get_embeddings()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CHKr5Fyvikil","executionInfo":{"status":"ok","timestamp":1730195358602,"user_tz":-330,"elapsed":922,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"8adf79c0-8cd5-409a-9250-fdf7704cc7ef"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-29-41d85c9041f4>:20: LangChainDeprecationWarning: The class `BedrockEmbeddings` was deprecated in LangChain 0.2.11 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-aws package and should be used instead. To use it run `pip install -U :class:`~langchain-aws` and import as `from :class:`~langchain_aws import BedrockEmbeddings``.\n","  bedrock_embedding = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrock_client)\n"]}]},{"cell_type":"code","source":["from google.colab import userdata\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","\n","GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n","\n","llm = ChatGoogleGenerativeAI(\n","    model=\"gemini-pro\",\n","    google_api_key=GEMINI_API_KEY,\n","    temperature=0.1,\n","    max_tokens=512,\n","    max_length=512,\n",")"],"metadata":{"id":"icQ3to_mrMzZ","executionInfo":{"status":"ok","timestamp":1730195300541,"user_tz":-330,"elapsed":10001,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["## **Load The Documents:**"],"metadata":{"id":"gIUph8J9kJBE"}},{"cell_type":"code","source":["!pip install \"langchain-unstructured[local]\" -qU\n","\n","!pip install nltk -qU\n","\n","\n","import nltk\n","\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O8ly0boom0OT","executionInfo":{"status":"ok","timestamp":1730194358769,"user_tz":-330,"elapsed":13094,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"39c17d0c-afd1-4a37-8044-997c05c4ea11"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["from langchain_community.document_loaders import TextLoader\n","from langchain_unstructured import UnstructuredLoader\n","\n","\n","ai_ml_loaders = UnstructuredLoader(\"/content/ai_ml_docs.docx\")\n","gen_ai_loaders = UnstructuredLoader(\"/content/gen_ai_docs.docx\")"],"metadata":{"id":"GpeHwSUOikle","executionInfo":{"status":"ok","timestamp":1730194635542,"user_tz":-330,"elapsed":852,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from langchain_core.documents import Document\n","\n","docs = ai_ml_loaders.load()\n","ai_ml = []\n","for doc in docs:\n","  ai_ml.append(doc.page_content)  # Add each document to the ai_ml list\n","\n","\n","ai_ml_document = Document(\n","    page_content=\"\\n\\n\".join(ai_ml),\n","    metadata={\n","        \"source\": \"/content/ai_ml_docs.docx\"\n","    }\n",")"],"metadata":{"id":"JhYNCtaCik10","executionInfo":{"status":"ok","timestamp":1730195005322,"user_tz":-330,"elapsed":1524,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["docs = gen_ai_loaders.load()\n","gen_ai = []\n","for doc in docs:\n","  gen_ai.append(doc.page_content)\n","\n","gen_ai_document = Document(\n","    page_content=\"\\n\\n\".join(gen_ai),\n","    metadata={\n","        \"source\": \"/content/gen_ai_docs.docx\"\n","    }\n",")"],"metadata":{"id":"01ixVarUoxCu","executionInfo":{"status":"ok","timestamp":1730195009256,"user_tz":-330,"elapsed":560,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# main docs:\n","\n","parent_docs = [ai_ml_document, gen_ai_document]"],"metadata":{"id":"jOLxNnaPpZZ2","executionInfo":{"status":"ok","timestamp":1730195051024,"user_tz":-330,"elapsed":585,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["len(parent_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b-NGh5P7qLoL","executionInfo":{"status":"ok","timestamp":1730195079799,"user_tz":-330,"elapsed":574,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"e96a8d3d-5aa4-4d32-8ca8-600575ab5a45"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["## **Vector Store:**"],"metadata":{"id":"OFA1DOccrEar"}},{"cell_type":"code","source":["# This text splitter is used to create the child documents:\n","\n","\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","child_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=120)\n"],"metadata":{"id":"a0l4By8uqbcW","executionInfo":{"status":"ok","timestamp":1730195146038,"user_tz":-330,"elapsed":831,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["from langchain.storage import InMemoryStore\n","from langchain_chroma import Chroma"],"metadata":{"id":"f8sN5nV3rHfg","executionInfo":{"status":"ok","timestamp":1730195327603,"user_tz":-330,"elapsed":864,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["\n","vectorstore = Chroma(\n","    collection_name=\"full_documents\", embedding_function=embedding_model\n",")\n","\n"],"metadata":{"id":"8O4ue797qb0v","executionInfo":{"status":"ok","timestamp":1730195367202,"user_tz":-330,"elapsed":1151,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["### **ParentDocumentRetriever:**"],"metadata":{"id":"lVnwz4zarsS4"}},{"cell_type":"markdown","source":["#### **Retriever 1:**"],"metadata":{"id":"rbc1NSums51W"}},{"cell_type":"code","source":["# ParentDocumentRetriever\n","from langchain.retrievers import ParentDocumentRetriever\n","\n","store = InMemoryStore()\n","\n","retriever = ParentDocumentRetriever(\n","    vectorstore=vectorstore,\n","    docstore=store,\n","    child_splitter=child_splitter,\n",")"],"metadata":{"id":"cs2kZ8Vgqb3I","executionInfo":{"status":"ok","timestamp":1730195434686,"user_tz":-330,"elapsed":680,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# Add Documents:\n","\n","retriever.add_documents(parent_docs, ids=None)"],"metadata":{"id":"Ikzo8I1prwih","executionInfo":{"status":"ok","timestamp":1730195532353,"user_tz":-330,"elapsed":59255,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["list(store.yield_keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XumuOwRdr7-j","executionInfo":{"status":"ok","timestamp":1730195532354,"user_tz":-330,"elapsed":8,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"eabe2d5c-1a76-4d6c-8221-d37d293df09e"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['ba31ccd0-b26b-404a-ac11-e8fef7c0e699',\n"," 'f94315fc-7b2a-40e2-94d7-2ece3e44619e']"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["retriever_docs = retriever.invoke(\"Tell me about AI/ML\")\n","retriever_docs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tOKIp5thr8hN","executionInfo":{"status":"ok","timestamp":1730195632413,"user_tz":-330,"elapsed":846,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"10a50f37-3b4d-42f8-ac85-ec5d2a63c021"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'source': '/content/ai_ml_docs.docx'}, page_content='AI/ML Documentation\\t\\tdibyendubiswas1998@gmail.com\\n\\nResearch Paper: Sequence to Sequence Learning with Neural Networks\\n\\nThe core idea of Seq2Seq learning involves mapping variable-length input sequences to variable-length output sequences using two Long Short-Term Memory (LSTM) networks: “one for encoding the input sequence” and “another for decoding the target sequence”. This method achieves state-of-the-art results for machine translation and other sequential tasks.\\n\\nThe paper introduces a novel approach to handling sequence-to-sequence (Seq2Seq) problems using Recurrent Neural Networks (RNNs), specifically Long Short-Term Memory (LSTM) networks. This method was primarily developed to address tasks like machine translation, but it has broader implications for other tasks involving sequences.\\n\\nKey Contributes:\\n\\nGeneralized Sequence Learning: The Seq2Seq model can handle tasks with variable-length sequences, which previously posed challenges for traditional deep neural networks (DNNs).\\n\\nLSTM Networks: The authors use LSTM networks to handle long-term dependencies in sequences, which addresses the vanishing gradient problem associated with Recurrent Neural Networks (RNNs).\\n\\nEnglish-to-French Translation: On the WMT\\'14 English-to-French translation task, the Seq2Seq model achieves a BLEU score of 34.8, outperforming traditional phrase-based Statistical Machine Translation (SMT) systems, which score 33.3.\\n\\nWord Reversal Trick: They found that reversing the order of words in source sentences significantly improved model performance by introducing short-term dependencies that simplified optimization.\\n\\nKey Components of the Paper:\\n\\nThe Problem of Sequence Learning:\\n\\nThe paper starts by identifying a gap in traditional neural networks\\' ability to handle sequences. Earlier Deep Neural Networks (DNNs) could only work on tasks where inputs and outputs had fixed dimensionalities, such as image classification. However, many real-world problems (e.g., machine translation, speech recognition) involve variable-length sequences, making fixed-dimensional models unsuitable.\\n\\nFor example, in Machine Translation:\\n\\nThe input might be an English sentence: “I am a student.”\\n\\nThe output could be a French sentence: “Je suis un étudiant.”\\n\\nSince the lengths of the sentences can differ, and each sequence has a varying structure, typical DNNs can\\'t directly map such inputs to outputs.\\n\\nThe authors propose a Seq2Seq model that uses two RNNs:\\n\\nOne to encode the input sequence into a vector.\\n\\nAnother to decode this vector into the target sequence.\\n\\nLong Short-Term Memory (LSTM) Architecture:\\n\\nThe paper highlights the use of LSTM networks to handle long-range dependencies in sequences. Traditional RNNs suffer from the vanishing gradient problem, where the network struggles to learn long-term dependencies due to diminishing gradients during backpropagation.\\n\\nLSTMs overcome this by introducing memory cells that can retain information over long periods, allowing the model to learn relationships between distant elements in a sequence (e.g., words at the beginning and end of a sentence). This is crucial for tasks like translation, where words at the start of a sentence influence the translation of words toward the end.\\n\\n\\n\\nThe authors explain that LSTMs are particularly well-suited for sequence-to-sequence tasks because they can learn from temporal dependencies, which are critical in sequences where order matters.\\n\\nSeq2Seq Model (Encoder-Decoder Architecture):\\n\\nThe core of the model is the Encoder-Decoder architecture, consisting of two LSTMs:\\n\\nEncoder: Reads the input sequence (e.g., an English sentence) one token at a time and transforms it into a context vector (a fixed-length vector representation of the entire input sequence).\\n\\nExample: For the sentence \"I am a student\", the encoder converts the sequence of words into a single, fixed-size context vector that encodes the meaning of the entire sentence.\\n\\nDecoder: Takes the context vector produced by the encoder and generates the output sequence (e.g., the French translation) one word at a time.\\n\\nExample: The decoder takes the context vector and generates the sentence \"Je suis un étudiant\" by predicting each word in sequence based on the context vector and the previously predicted words.\\n\\nThis architecture is flexible and can handle sequences of different lengths.\\n\\nTraining the Sequence Model:\\n\\nThe Seq2Seq model is trained to maximize the probability of the correct output sequence given the input sequence. The objective function for training is the conditional probability of the output sequence, p(y1, ….., yT | x1, ….., xT ), where:\\n\\nx1, ….., xT  is the input sequence (e.g., an English sentence).\\n\\ny1, ….., yT is the output sequence (e.g., the translated French sentence).\\n\\nThe model uses Stochastic Gradient Descent (SGD) to update its parameters and improve translation quality over time.\\n\\nWord Reversal Trick:\\n\\nOne of the notable innovations in this paper is the word reversal trick. The authors found that reversing the order of words in the source sentence (but not in the target sentence) improves the model’s performance. This technique introduces short-term dependencies between the input and output, making the optimization problem easier for the LSTM to solve.\\n\\nExample: Instead of feeding the sentence \"I am a student\" into the encoder, they reverse it to \"student a am I\", and then use the same process to generate the output. This allows the first word of the target sentence to be influenced by the last word of the input sentence, making training more effective.\\n\\nThe result of reversing the input sequence is a reduction in test perplexity (a measure of how well a probabilistic model predicts a sample) and an increase in translation quality, as measured by the BLEU score.\\n\\nEvaluation on Machine Translation Task:\\n\\nThe model was tested on the WMT’14 English to French machine translation task, using an ensemble of deep LSTM networks. The authors used a beam search decoder to generate the translations.\\n\\nThe model achieved a BLEU score of 34.8, outperforming traditional phrase-based Statistical Machine Translation (SMT) systems, which achieved a score of 33.3.\\n\\nThey also experimented with resorting hypotheses generated by SMT systems using the LSTM model, which further improved the BLEU score to 36.5, close to the state-of-the-art results of 37.0 at that time.\\n\\nThis performance demonstrates the model’s ability to capture the meaning of sentences and generate accurate translations, even for long and complex sentences.\\n\\nStrengths and Limitation:\\n\\nStrengths: \\n\\nGeneralization: The Seq2Seq model can be applied to any sequence-based problem, not just machine translation. It has since been applied to tasks like speech recognition, summarization, question answering, and more.\\n\\nHandling Long Sequences: By using LSTMs, the model can successfully learn long-term dependencies, a common challenge in tasks like translation.\\n\\nLimitations: \\n\\nContext Vector Bottleneck: The entire input sequence is compressed into a single fixed-length vector. For very long sequences, this can become a bottleneck, as all the information must fit into a single vector. Later work introduced attention mechanisms to overcome this by allowing the model to focus on different parts of the input sequence at different times.\\n\\nEncoder-Decoder Architecture:\\n\\nThe Seq2Seq model consists of two main components: an Encoder and a Decoder both implemented using LSTMs. Let\\'s explore these components with the help of an example.\\n\\nEncoder:\\n\\nThe encoder reads the input sequence (e.g., a sentence in English), one element (word) at a time, and produces a fixed-size context vector.\\n\\nThis context vector is a compressed representation of the entire input sequence and is generated from the hidden states of the LSTM at each timestep.\\n\\nEncoder Process:\\n\\nThe encoder reads each word of the input sentence (\"I\", \"am\", \"a\", \"student\") sequentially.\\n\\nAt each step, the encoder\\'s LSTM updates its hidden state and processes the input, ultimately compressing the entire sentence into a context vector.\\n\\nThe final hidden state of the encoder (context vector) is a summary of the input sentence.\\n\\nDecoder:\\n\\nThe decoder takes the context vector from the encoder as input and generates the output sequence (e.g., the translated sentence in French), one word at a time.\\n\\nAt each step, the decoder predicts the next word in the output sequence based on the previous words and the context vector.\\n\\nDecoder Process:\\n\\nThe decoder takes the context vector and generates the output sentence (\"Je suis un étudiant\").\\n\\nAt each decoding step, the decoder predicts the next word based on the context vector and previously generated words.\\n\\nFor instance, it first predicts \"Je\", then \"suis\", and so on until it produces the complete sentence \"Je suis un étudiant\".\\n\\n\\n\\nThe Seq2Seq model with LSTMs introduced in this paper represents a major breakthrough in handling sequence-to-sequence problems like machine translation. The use of Encoder-Decoder architecture allows the model to map sequences of arbitrary length to one another, overcoming the limitations of traditional DNNs.\\n\\nThe model achieved state-of-the-art results in machine translation and laid the foundation for future advancements in neural machine translation, speech recognition, and other sequential tasks. The introduction of techniques like word reversal and the innovative use of LSTMs for handling long-term dependencies made it possible for this model to handle complex tasks with impressive accuracy.\\n\\nResearch Paper: Neural Machine Translation by Jointly Learning to Align and Translate\\n\\nThe research paper \"Neural Machine Translation by Jointly Learning to Align and Translate\" by Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio introduces a significant improvement to neural machine translation (NMT) through the integration of an attention mechanism. This paper presents a solution to the limitations of earlier encoder-decoder models, especially for handling long sentences.\\n\\nKey Contributions of the Paper:\\n\\nAddressing the Fixed-Length Context Bottleneck: The authors identified that traditional encoder-decoder models, such as those used in earlier NMT systems, compress the entire input sentence into a fixed-length vector. This approach works well for short sentences but struggles with long sentences, leading to performance degradation.\\n\\nIntroduction of Attention Mechanism: To overcome the fixed-length vector limitation, the paper introduces an attention mechanism that allows the model to focus on different parts of the input sentence as it generates each word of the output. This is key to improving translation quality, particularly for longer sentences.\\n\\nLearning to Align and Translate Simultaneously: The attention mechanism effectively aligns parts of the source sentence with the corresponding parts of the target sentence during translation. This alignment is soft rather than hard, meaning that it assigns varying degrees of attention to different words in the source sentence, rather than selecting just one word.\\n\\nNeural Machine Translation Overview (NMT):\\n\\nThe goal of machine translation is to find a target sentence ‘y’ that maximizes the conditional probability ‘p(x|y)’, where ‘x’ is the source sentence. NMT models are designed to learn this probability distribution from large parallel corpora.\\n\\nIn traditional encoder-decoder models:\\n\\nThe encoder reads the source sentence and compresses it into a fixed-length vector.\\n\\nThe decoder uses this vector to generate the target sentence.\\n\\nThis method works, but when sentences become long, the fixed-length vector struggles to encapsulate all the necessary information, leading to a decline in translation accuracy. The paper proposes a solution to this problem using the attention mechanism.\\n\\nKey Components of the Paper:\\n\\nMotivation and Problem Statement:\\n\\nIn earlier neural machine translation systems (such as the Seq2Seq model), a fixed-length vector is used to encode an entire input sentence, regardless of its length. This becomes problematic when translating long sentences because the model is forced to compress all the information into a single vector. The fixed-length vector is a bottleneck, limiting the model\\'s ability to translate longer and more complex sentences.\\n\\nThe authors propose a solution: instead of using a single fixed-length vector, they introduce an attention mechanism that allows the model to focus on different parts of the input sentence at different steps of the translation. This method enables the model to dynamically select relevant parts of the input for each word it generates, making the translation of longer sentences more accurate.\\n\\nBackground: Encoder-Decoder Models in NMT\\n\\nThe traditional encoder-decoder model for NMT consists of:\\n\\n\\n\\nAn encoder, typically a Recurrent Neural Network (RNN), which processes the input sentence (source language) and encodes it into a single fixed-length vector (context vector).\\n\\nA decoder, another RNN, which takes this context vector and generates the output sentence (target language) one word at a time.\\n\\nWhile this model works well for short sentences, the performance drops for longer sentences because of the information bottleneck—the encoder must compress the entire input sentence into a single vector.\\n\\nKey Innovation: Attention Mechanism\\n\\nThe attention mechanism works by computing a context vector for each target word based on the entire input sentence, but it assigns different weights (or attention scores) to different parts of the input sentence. The higher the weight for a word, the more attention the model pays to that word when generating the next target word.\\n\\nSteps of the Attention Mechanism:\\n\\nEncoder: The encoder reads the input sentence and produces a sequence of hidden states, where each hidden state represents the encoding of a word and its surrounding context.\\n\\nAlignment Scores: For each word the decoder generates, the model computes alignment scores between the current hidden state of the decoder and each hidden state of the encoder. These alignment scores indicate how important each word in the input sequence is for predicting the current word in the output sequence.\\n\\nAttention Weights: The alignment scores are normalized using the SoftMax function to produce attention weights, which sum to 1. These weights determine how much attention the model should pay to each word in the input sentence.\\n\\nContext Vector: A context vector is computed as a weighted sum of the encoder’s hidden states. The decoder uses this context vector, along with its current hidden state, to generate the next word in the target sequence.\\n\\nExample of the Attention Mechanism in Translation:\\n\\nConsider the translation of the English sentence \"I am a student.\" into the French sentence \"Je suis étudiant.\"\\n\\nThe encoder processes the entire English sentence, generating a sequence of hidden states.\\n\\nWhen the decoder generates the first word ‘Je’, it looks back at the hidden states of the encoder. The attention mechanism assigns higher weights to the hidden state corresponding to ‘I’, while still giving some attention to other words like ‘am’.\\n\\nWhen the decoder generates the next word ‘suis’, it focuses more on the hidden state of ‘am’.\\n\\nFinally, for the word ‘étudiant’, the model focuses mainly on the hidden state corresponding to ‘student’.\\n\\nThe attention mechanism ensures that the model can adaptively focus on different parts of the input sentence, making it easier to generate accurate translations for each word in the output.\\n\\nImportance of Attention Mechanism:\\n\\nBetter Handling of Long Sentences: Traditional encoder-decoder models struggle with long sentences because they compress all information into a single vector. Attention eliminates this bottleneck by allowing the decoder to access the full input sequence at each step, ensuring that even long sentences are translated accurately.\\n\\n\\n\\nImproved Translation Alignment: The attention mechanism helps the model to align words in the source sentence with corresponding words in the target sentence, even when the word orders are different. This improves translation quality, especially for languages with different syntactic structures.\\n\\nFlexibility: The attention mechanism allows the model to dynamically adjust its focus, providing greater flexibility in translating complex or ambiguous sentences.\\n\\nVisualization of Alignment: One of the most useful features of attention is that it provides interpretable alignment matrices. These matrices show how each word in the source sentence aligns with each word in the target sentence, making it possible to visualize the translation process.\\n\\nModel Architecture:\\n\\nThe attention-based model extends the basic encoder-decoder framework by introducing a few key components:\\n\\nEncoder: Bidirectional RNN\\n\\nThe encoder is a Bidirectional RNN (BiRNN), meaning it processes the input sentence in two directions:\\n\\nFrom the first word to the last (forward).\\n\\nFrom the last word to the first (backward).\\n\\nEach word in the input sentence has two hidden states: one from the forward RNN and one from the backward RNN. These hidden states are concatenated to form the annotations for each word, capturing both the preceding and following context for each word in the sentence.\\n\\nDecoder: Attention-Based Decoder\\n\\nThe decoder is also an RNN. However, instead of relying on a single context vector from the encoder, the decoder now uses the attention mechanism to compute a context vector for each target word it generates.\\n\\nFor each target word ‘Yi’, the decoder:\\n\\nComputes attention scores for each annotation from the encoder.\\n\\nUses these scores to compute a context vector as a weighted sum of the annotations.\\n\\nCombines the context vector with the previous decoder hidden state to generate the next word.\\n\\nTraining and Experiments:\\n\\nThe authors trained and evaluated their attention-based NMT model on the English-to-French translation task from the WMT\\'14 dataset. The training dataset consists of millions of English-French sentence pairs, and the model was trained using stochastic gradient descent (SGD) with the Adadelta optimizer.\\n\\nKey Benefits:\\n\\nThe attention-based model (RNNsearch) significantly outperformed the traditional encoder-decoder model (RNNencdec), especially on long sentences.\\n\\nThe BLEU score (a metric for evaluating the quality of machine-translated text) was significantly higher for the attention-based model, especially for longer sentences (more than 30 words).\\n\\nFor example, the BLEU score for sentences of length up to 50 words using the attention-based model was 26.75, compared to 17.82 for the traditional model.\\n\\nQualitative Analysis: Alignment and Translation\\n\\n\\n\\nThe paper also provides a qualitative analysis of the model’s ability to align source and target words during translation. By visualizing the attention weights, the authors showed that the model learns to soft-align the source and target words in a linguistically plausible way.\\n\\nSoft alignment refers to a concept used in the attention mechanism of neural networks, especially in neural machine translation (NMT), where each word (or token) in the output sequence is aligned with all the words in the input sequence, but with different degrees of focus or attention.\\n\\nUnlike hard alignment, where each word in the target sentence is strictly mapped to one word in the source sentence (e.g., one-to-one alignment), soft alignment allows the model to assign weights to all the source words based on their relevance to the current target word. These weights reflect how much attention or importance each source word contributes to the generation of the current target word. Soft alignment is often represented as a distribution over all the input words, meaning the model attends to multiple words in the source sentence simultaneously, with varying degrees of focus.\\n\\nHow Soft Alignment Works:\\n\\nSoft alignment is part of the attention mechanism in neural networks. The process works as follows:\\n\\nEncoder: For each word in the source sentence, the encoder produces a hidden state or annotation. These annotations capture the context surrounding each word, and the entire sentence is represented as a sequence of such annotations.\\n\\nAlignment Scores: For each word the decoder generates, it computes an alignment score between the current decoder hidden state and each annotation from the encoder. This score indicates how relevant each source word is for generating the current target word.\\n\\nAttention Weights (Soft Alignment): The alignment scores are normalized using the SoftMax function, which transforms them into probabilities (weights). These weights, called attention weights, represent how much attention should be paid to each word in the source sentence. The sum of all the weights equals 1, and each weight indicates the degree of attention on a specific source word.\\n\\nContext Vector: The model uses these attention weights to compute a context vector, which is a weighted sum of the encoder\\'s annotations. The context vector focuses on the relevant parts of the input sentence, depending on the current target word being generated.\\n\\nIn soft alignment, instead of focusing on a single source word, the model softly attends to multiple words in the source sentence, using the context vector to represent a blend of the most relevant words.\\n\\nExample of Soft Attention:\\n\\nLet’s consider translating the English sentence “I am a student” into French “Je suis étudiant”. Here\\'s how soft alignment works in this scenario:\\n\\nThe encoder processes the English sentence and produces hidden states (annotations) for each word: ‘I’, ‘am’, ‘a’, and ‘student’.\\n\\nWhen generating the first word “Je”, the decoder computes alignment scores between its current hidden state and each of the annotations. The alignment score for ‘I’ might be higher than for ‘am’, ‘a’, or ‘student’, because ‘Je’ corresponds more closely to ‘I’ in the translation. However, the model will still consider all the words in the input sentence.\\n\\nThe scores are transformed into attention weights, which are applied to the annotations to produce a context vector. In this case, the context vector is weighted more toward ‘I’, but it still includes information from the other words in the sentence.\\n\\nDifference between Soft and Hard Alignment:\\n\\nSoft Attention: The model attends to multiple input words simultaneously, with different weights representing varying levels of importance. This method allows the model to capture richer relationships between words and is more flexible.\\n\\nHard Attention: A strict one-to-one mapping where each output word is aligned with only one input word. This approach is more rigid and doesn\\'t work well when the word order or grammatical structures differ significantly between languages.\\n\\nWhy Use Soft Alignment:\\n\\nHandling Reordering: In translation tasks, especially between languages with different word orders (e.g., English and French), soft alignment helps the model attend to relevant parts of the sentence even when words are reordered. For example, adjectives often appear before nouns in English but after nouns in French. Soft alignment allows the model to handle these differences gracefully.\\n\\nImproving Translation Accuracy: By considering multiple words in the source sentence when generating each word in the target sentence, soft alignment helps the model produce more accurate translations, especially when long-distance dependencies exist.\\n\\nHandling Variable-Length Sentences: Soft alignment helps overcome the limitations of encoding an entire sentence into a fixed-length vector. The model can focus on different parts of the input dynamically as it generates each word, allowing it to handle longer and more complex sentences effectively.\\n\\nImportant and Impact of Attention Mechanism:\\n\\nThe introduction of the attention mechanism in NMT is one of the most important innovations in neural translation systems, and it has a wide-reaching impact across many sequence-to-sequence tasks.\\n\\nKey Benefits:\\n\\nImproved Handling of Long Sentences: By allowing the model to dynamically focus on relevant parts of the input sentence, the attention mechanism overcomes the limitations of fixed-length vectors. This is especially important for long sentences that contain more complex information.\\n\\nBetter Alignment: The attention mechanism enables the model to learn soft alignments between source and target words, which is crucial for translation tasks, especially when translating between languages with different syntactic structures (e.g., English and French).\\n\\nIncreased Interpretability: The attention weights provide insight into which parts of the input sentence the model is focusing on when generating each word in the output. This makes the model more interpretable, as we can visualize and analyze the alignment between source and target words.\\n\\nGeneral Impact:\\n\\nThe attention mechanism introduced in this paper has since become a foundational component in various deep learning tasks beyond machine translation. It has been widely used in tasks like text summarization, image captioning, speech recognition, and more. Moreover, it laid the groundwork for the development of the Transformer architecture, which entirely replaces the recurrent structure with self-attention mechanisms and has become the basis of models like BERT and GPT.\\n\\nThe paper \"Neural Machine Translation by Jointly Learning to Align and Translate\" introduced a ground-breaking attention mechanism that significantly improved the performance of NMT systems, particularly for long sentences. By allowing the model to dynamically focus on different parts of the input sentence, the \\n\\n\\n\\nattention mechanism solves the bottleneck caused by compressing all information into a single fixed-length vector. This innovation has had a profound impact on the field of machine translation and beyond, influencing many modern AI architectures.\\n\\nResearch Paper: Attention All You Need (Transformer)\\n\\nThe research paper \"Attention Is All You Need\" by Vaswani et al. (2017) introduced the Transformer model, which replaced the traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) with a structure based entirely on self-attention mechanisms. This ground-breaking model improved efficiency and performance for various sequence-to-sequence tasks, particularly in Neural Machine Translation (NMT). Let’s dive into the details of this paper and its key contributions.\\n\\nBefore the introduction of the Transformer, the dominant models for sequence-to-sequence tasks (like Neural Machine Translation) were based on Recurrent Neural Networks (RNNs), particularly Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRUs). These models worked by processing inputs and outputs sequentially, one token at a time, creating significant bottlenecks due to their inability to parallelize operations effectively.\\n\\nThe Transformer proposed an entirely new approach:\\n\\nIt replaced recurrence and convolutions with self-attention mechanisms, which allow the model to process all tokens in parallel and capture dependencies between distant tokens efficiently.\\n\\nThis approach significantly sped up training times, improved performance, and allowed the model to scale to much larger datasets.\\n\\nKey Contributions of the Paper:\\n\\nMotivation:\\n\\nPrior to the Transformer, most sequence transduction models (e.g., for machine translation) relied on RNNs or CNNs with an encoder-decoder architecture. While these models were effective, they had several limitations:\\n\\nRNNs: Suffered from sequential processing, which made it difficult to parallelize and inefficient for long sequences. They struggled with learning long-range dependencies due to the vanishing gradient problem.\\n\\nCNNs: While more parallelizable than RNNs, CNNs require a large number of layers or complex structures to capture relationships between distant positions in sequences, making them computationally expensive.\\n\\nThe authors proposed a new model, Transformer that uses self-attention to process sequences. This model eliminates the need for recurrence or convolution entirely, resulting in a more efficient and parallelizable architecture that achieves state-of-the-art results.\\n\\nKey Innovation: Transformer Model\\n\\nThe Transformer model is built around the concept of self-attention and consists of encoder and decoder stacks, each composed of several identical layers. The key components of the Transformer include multi-head attention, positional encodings, and feed-forward networks. Let’s break these down:\\n\\nEncoder and Decoder Architecture:\\n\\nEncoder:\\n\\nThe encoder consists of 6 identical layers. Each layer has two primary sub-layers:\\n\\nMulti-Head Self-Attention Mechanism: Each token in the input attends to every other token in the sequence. This allows the model to capture relationships between words regardless of their position in the sequence.\\n\\n\\n\\nFeed-Forward Neural Network (FFN): A simple fully connected network applied to each position in the sequence.\\n\\nA residual connection and layer normalization are applied after each sub-layer to prevent the vanishing gradient problem and speed up convergence.\\n\\nDecoder:\\n\\nThe decoder is also composed of 6 identical layers, similar to the encoder. However, in addition to the two sub-layers found in the encoder, the decoder has a third sub-layer that performs attention over the encoder’s output. The layers are.\\n\\nMasked Multi-Head Self-Attention: Prevents each position from attending to future positions, preserving the autoregressive property.\\n\\nMulti-Head Attention over Encoder Output: Each position in the decoder attends to all positions in the encoded sequence.\\n\\nFeed-Forward Neural Network (FFN): The same as in the encoder.\\n\\nAs with the encoder, residual connections and layer normalization are applied after each sub-layer.\\n\\nAttention Mechanism\\n\\nThe Transformer’s core innovation is its attention mechanism, which replaces recurrence and convolutions. Specifically, the paper introduces scaled dot-product attention and multi-head attention.\\n\\nScaled Dot-Product Attention:\\n\\nGiven queries ‘Q’, keys ‘K’, and values ‘V’, attention is computed as:\\n\\nQuery: The vector representing the current word or token.\\n\\nKey: A representation of the tokens in the sequence that are being attended to.\\n\\nValue: The actual data associated with the tokens being attended to.\\n\\nThe scaling factor ‘root(dk)’ prevents large dot-products from pushing the SoftMax into regions with small gradients, improving training stability.\\n\\nMulti-Headed Attention:\\n\\nInstead of performing a single attention function, the Transformer uses multi-head attention. The input is projected into multiple smaller subspaces, and attention is applied in parallel. The results are concatenated and projected to the final output. This allows the model to capture information from different subspaces and attend to different positions simultaneously.\\n\\nPosition-Wise Feed-Forward Networks:\\n\\nEach layer of the encoder and decoder contains a feed-forward network (FFN) that applies two linear transformations with ReLU activation in between. This network operates independently on each position in the sequence.\\n\\nDespite being simple, this layer helps the model learn complex transformations of the input.\\n\\nPositional Encoding:\\n\\nSince the Transformer does not have recurrence or convolutions, it lacks a way to incorporate the order of tokens in a sequence. To address this, the model adds positional \\n\\n\\n\\nencodings to the input embeddings. These encodings use sine and cosine functions of different frequencies to inject information about token positions into the model:\\n\\nThis allows the model to capture positional relationships between tokens without relying on recurrence.\\n\\nWhy Self-Attention:\\n\\nThe authors compared self-attention to traditional RNNs and CNNs in terms of computational complexity, parallelization, and path length between dependencies:\\n\\nComputational Complexity: Self-attention operates with complexity ‘O(n^2d)’ where ‘n’ is the sequence length and ‘d’ is the dimension of the embeddings. This is more efficient than recurrent models, which operate with complexity ‘O(n^2d)’.\\n\\nParallelization: Self-attention allows for full parallelization because it processes the entire sequence simultaneously. RNNs, by contrast, process tokens sequentially, limiting the ability to parallelize.\\n\\nLong-Range Dependencies: In self-attention, the maximum path length between any two tokens is 1, since all tokens attend to each other directly. In RNNs, the path length is proportional to the sequence length, making it harder to capture long-range dependencies. CNNs require many layers to achieve this, but self-attention achieves it in a single step.\\n\\nResults and Performance:\\n\\nThe Transformer model was evaluated on two machine translation tasks: English-to-German and English-to-French. It achieved state-of-the-art performance, surpassing previous models (including those with RNNs and CNNs). Key results include.\\n\\nEnglish-to-German Translation: The Transformer achieved a BLEU score of 28.4, outperforming all previous models, including ensembles.\\n\\nEnglish-to-French Translation: The model set a new state-of-the-art BLEU score of 41.8.\\n\\nThe Transformer required significantly less training time compared to previous architectures, achieving these results in just 12 hours on 8 GPUs for the base model.\\n\\nImportance and Impact:\\n\\nThe Transformer represents a shift away from traditional sequence models like RNNs and CNNs, introducing a new paradigm based entirely on attention. Its benefits include:\\n\\nParallelization: By removing recurrence, the Transformer can process sequences much faster, making it highly efficient for training large models.\\n\\nHandling Long Sequences: Self-attention allows the model to capture long-range dependencies in a single step, making it better at handling long sentences and complex relationships.\\n\\nScalability: The Transformer is highly scalable, allowing for the training of larger models that achieve superior performance on a variety of tasks.\\n\\nThis model laid the groundwork for subsequent advancements in NLP, most notably BERT, GPT, and other Transformer-based models that now dominate the field of natural language processing.\\n\\n\\n\\nKey Components of the Transformer Model:\\n\\nSelf-Attention Mechanism:\\n\\nThe core innovation of the Transformer is the self-attention mechanism, which allows each token in a sequence to attend to all other tokens, making it possible to capture long-range dependencies.\\n\\nSelf-attention: computes a representation of a sequence by focusing on different parts of the sequence. For each word in the input, the model calculates how much \"attention\" to give to each other word in the sequence.\\n\\nScaled Dot-Product Attention: Given queries ‘Q’, keys ‘K’, and values ‘V’, self-attention is computed as:\\n\\nWhere,\\n\\nQ (query) represents the word being processed.\\n\\nK (key) represents the words being attended to.\\n\\nV (value) is the data associated with each key.\\n\\ndk is the dimensionality of the keys, and the scaling factor ‘root(dk)’ prevents extremely large values from pushing the SoftMax into regions with small gradients.\\n\\nMulti-Headed Attention:\\n\\nTo further improve the model’s ability to focus on different parts of the input sequence, the Transformer employs multi-head attention. Instead of performing a single attention function, the input is projected into multiple smaller spaces, and several attention mechanisms (called heads) are applied in parallel. This allows the model to attend to different aspects of the input at the same time.\\n\\nThe outputs from these heads are then concatenated and projected back into the original space. The advantage is that the model can jointly attend to information from different parts of the sequence more effectively.\\n\\nEach head computes attention independently:\\n\\nPositional Encoding:\\n\\nSince the Transformer lacks the inherent sequential structure of RNNs, it uses positional encodings to inject information about the position of tokens in the sequence. Without positional information, the model would treat all tokens as if they were unordered, which is problematic for tasks like translation, where word order is critical.\\n\\nThe paper introduces sinusoidal positional encodings, where the position of each token is represented by a combination of sine and cosine functions:\\n\\nThese encodings are added to the token embeddings to retain positional information. The choice of sine and cosine ensures that the model can generalize to sequences longer than those seen during training.\\n\\nFeed-Forward Networks (FFN):\\n\\n\\n\\nEach layer of the Transformer’s encoder and decoder also contains a feed-forward neural network (FFN). This is a simple fully-connected network that is applied to each position in the sequence independently.\\n\\nThe FFN is composed of two linear transformations with a ReLU activation between them:\\n\\nThe purpose of the FFN is to introduce non-linearities and learn complex transformations for each token in the sequence.\\n\\nResidual Connections and Layer Normalization:\\n\\nTo facilitate gradient flow and improve training stability, residual connections are added around each sub-layer in both the encoder and decoder. This means the input to a sub-layer is added to its output:\\n\\nWhere Sublayer(x) represents either the multi-head attention or feed-forward sub-layer. After applying the residual connection, the result is normalized using layer normalization.\\n\\nWhy Transformer is Important:\\n\\nThe Transformer model revolutionized NLP and sequence-based tasks due to its key advantages:\\n\\nParallelization: By removing recurrence, the Transformer can process sequences in parallel, drastically reducing training times and allowing it to scale too much larger datasets.\\n\\nHandling Long-Range Dependencies: Self-attention allows the model to capture dependencies between distant tokens in a single step, making it far more effective at handling long sequences than RNNs or CNNs, which require multiple layers or recurrent steps to do so.\\n\\nScalability: The Transformer’s efficiency and ability to parallelize operations made it highly scalable, allowing researchers to train massive models like BERT and GPT on enormous datasets, leading to dramatic improvements in tasks like text generation, question answering, and language understanding.\\n\\nSimpler Architecture: The Transformer’s architecture is simpler than RNNs and CNNs, relying only on self-attention and feed-forward networks. This simplicity, combined with its effectiveness, made it a popular choice for a wide range of sequence transduction tasks.\\n\\nImpact and Legacy:\\n\\nThe Transformer has had a profound impact on machine learning, particularly in the field of natural language processing. It became the foundation for subsequent models such as:\\n\\nBERT (Bidirectional Encoder Representations from Transformers): A Transformer-based model for language understanding tasks like text classification and question answering.\\n\\nGPT (Generative Pretrained Transformer): Used for text generation, GPT-3 is one of the most well-known Transformer models and is capable of generating human-like text.\\n\\nT5, RoBERTa, XLNet, and many other models are also built upon the Transformer architecture.\\n\\nThe paper \"Attention Is All You Need\" introduced a new era of NLP, pushing the boundaries of what machine learning models can achieve in text-based tasks.\\n\\nSelf-Attention:\\n\\nSelf-attention is a mechanism that allows a model to relate different parts of a single input sequence to each other when processing that sequence. This mechanism helps capture dependencies between \\n\\n\\n\\ntokens in a sequence, regardless of their distance from one another, and is a core component of the Transformer model.\\n\\nWhy is it called ‘Self’?\\n\\nThe term \"self-attention\" is used because, instead of attending to a different input sequence (as in the traditional encoder-decoder architecture), the model is attending to itself. Each token in the sequence computes its attention score with every other token in the sequence, including itself.\\n\\nHow Self-Attention Works:\\n\\nLet’s break it down with an example:\\n\\nExample: Processing a Sentence\\n\\nSuppose we have a sentence for translation: ‘The cat is on the mat.’\\n\\nThe self-attention mechanism would allow each word in the sentence to ‘attend’ to all the other words, computing a weighted relevance score for each pair of words in the sequence. This means that when processing the word ‘cat’, the model can look at the entire sentence and determine which words are important for understanding ‘cat’ in context.\\n\\n\\t\\tSteps of Self-Attention:\\n\\nCreate Queries, Keys, and Values: For each word, the model creates three vectors:\\n\\nQuery (Q): A vector that represents the word you\\'re focusing on.\\n\\nKeys (K): A vector for each word in the sequence (used to compare with the query).\\n\\nValues (V): Another vector for each word, which holds the actual information you want to extract based on the attention score.\\n\\nCalculate Attention Scores: The attention score between each word pair is computed using the dot product between the query of one word and the key of another word, followed by a scaling factor (to prevent overly large values) and a SoftMax to normalize the scores into probabilities:\\n\\nThe output is a weighted sum of the values, where the weights come from the attention scores.\\n\\nContext Representation: The final result is a ‘context’ vector for each word, which is a combination of the other words in the sequence, weighted by their importance to the current word. This allows the model to better understand the meaning of each word in the context of the entire sentence.\\n\\nWhy Self-Attention is Important:\\n\\nIt helps capture long-range dependencies between words.\\n\\nIt’s parallelizable: Unlike RNNs, which process inputs sequentially, self-attention can process all tokens in parallel, speeding up training.\\n\\nMulti-Headed Attention:\\n\\nMulti-headed attention is an extension of self-attention that allows the model to attend to different parts of the sequence simultaneously in multiple ways. Instead of performing a single attention calculation, the model splits the input into multiple smaller subspaces, applies attention independently to each one, and then concatenates the results.\\n\\nWhy Multi-Headed?\\n\\n\\n\\nIt allows the model to capture different relationships between words. For example, one attention head might focus on the syntactic structure, while another focuses on the semantic meaning.\\n\\nExample:\\n\\nConsider the sentence “The cat sat on the mat.”\\n\\nOne head might focus on “The cat” as a subject-predicate relationship.\\n\\nAnother head might focus on “sat on” as a prepositional phrase.\\n\\nA third head might focus on “on the mat” to understand the location.\\n\\nEach head computes attention independently, and the final result is the concatenation of all the head outputs.\\n\\nMulti-Head Attention Formula:\\n\\nMasked Self-Attention:\\n\\nMasked self-attention is used in tasks where the model generates sequences, such as in language generation. The mask prevents the model from looking ahead at future tokens in the sequence while making predictions, ensuring the model generates one token at a time in an autoregressive manner.\\n\\nWhy Masked?\\n\\nWhen generating text, you want to ensure that each token in the sequence only has access to the tokens that have already been generated, not the future ones. This prevents the model from ‘cheating’ by looking at future words.\\n\\nExample:\\n\\nIf the input sentence is “The cat sat on the mat.” and the model is predicting the next word:\\n\\nWhile predicting the word “sat”, the model should not have access to “on” or “the mat”.\\n\\nThe mask prevents the model from seeing future tokens during training, ensuring that predictions are made one step at a time.\\n\\nCross-Attention:\\n\\nCross-attention (or encoder-decoder attention) is used when the model processes information from two different sequences, typically in a sequence-to-sequence model like machine translation. In cross-attention, the decoder attends to the output of the encoder, allowing the decoder to focus on relevant parts of the input sequence while generating the output.\\n\\nExample:\\n\\nIn machine translation, suppose we’re translating the sentence “The cat is on the mat.” from English to French:\\n\\nThe encoder processes the English sentence and generates a sequence of hidden states.\\n\\nThe decoder generates the French translation. At each step, cross-attention allows the decoder to focus on specific parts of the encoded English sentence (such as focusing on the word “cat” when generating “chat” in French).\\n\\nHow Cross-Attention Works:\\n\\nThe queries come from the decoder’s hidden states, and the keys and values come from the encoder’s output.\\n\\nThis enables the decoder to focus on the most relevant parts of the input sequence at each generation step.\\n\\nThe “Attention Is All You Need” paper introduced the Transformer, a highly efficient and scalable architecture for sequence-to-sequence tasks, which has become the standard model in NLP. The key innovation, self-attention, replaced recurrence and convolution with parallelizable operations that can handle long-range dependencies efficiently. The Transformer has since become the backbone of many state-of-the-art models, revolutionizing the way we approach tasks such as machine translation, text generation, and language understanding.\\n\\nDifference between Attention, Self-Attention, Multi-Headed Attention:\\n\\nThese three concepts are related to how models focus on different parts of input data when processing sequences. Let’s explore each of them with definitions and examples.\\n\\nAttention:\\n\\nAttention is a mechanism that allows models to focus on different parts of an input sequence when producing an output. It assigns weights to different input elements, indicating their importance for generating each output element. Attention mechanisms are often used in sequence-to-sequence tasks, such as machine translation, where it’s necessary to map input sequences to output sequences of different lengths.\\n\\nExample:\\n\\nMachine Translation: Suppose you are translating the English sentence “The cat is on the mat.” into French “Le chat est sur le tapis.”\\n\\nWhen generating the French word “chat”, the model might assign a higher weight to the English word “cat” because it is more relevant at that step.\\n\\nSimilarly, while translating “mat” to “tapis”, the model will focus more on the word “mat” in the input sentence.\\n\\nIn this case, the attention mechanism allows the model to dynamically focus on different words in the input sequence when producing each word in the output.\\n\\nSelf-Attention:\\n\\nSelf-Attention is a type of attention mechanism where a sequence attends to itself. In self-attention, each word (or token) in the input sequence computes its relationship with all other words in the sequence (syntactic or semantic), allowing the model to capture dependencies between words, regardless of their positions. Self-attention is widely used in Transformer models.\\n\\nExample:\\n\\nSentence Understanding: For the sentence “The cat sat on the mat.”, self-attention helps the model determine how each word relates to the others.\\n\\nWhen processing “cat”, the model attends to related words like “sat” and “mat”, understanding that “cat” is the subject that “sat” on something.\\n\\nEach word is assigned a weight, indicating its relevance to other words in the sentence. This helps capture the context more effectively than relying only on word order.\\n\\nSelf-attention enables the model to consider the entire sequence simultaneously, making it suitable for tasks where long-range dependencies are important.\\n\\n\\n\\nMulti-Headed Attention:\\n\\nMulti-Headed Attention extends self-attention by using multiple attention heads to capture different aspects of relationships between (syntactic, semantic, positional, overall context) words. Each attention head works independently to learn different subspaces of the data, and the results are combined to form a more comprehensive representation.\\n\\nExample:\\n\\nUnderstanding Different Relationships: In the sentence “The cat sat on the mat.”, different attention heads can focus on different types of relationships:\\n\\nHead 1 might focus on the subject-verb relationship (cat → sat).\\n\\nHead 2 might capture prepositional phrases (on → mat).\\n\\nHead 3 might consider the overall context, understanding that the entire phrase describes the location of the cat.\\n\\nEach head captures different patterns, and their outputs are concatenated and linearly transformed to get the final output. This enables the model to attend to multiple aspects of the input sequence simultaneously, enhancing its ability to understand complex relationships.\\n\\nKey Components of Transformer Architecture:\\n\\nThe Transformer architecture is built on several core components that allow it to process sequences of data efficiently. Let\\'s dive into each of the following concepts in detail:\\n\\nFeed-Forward Neural Network (FFN):\\n\\nIn the Transformer architecture, the Feed-Forward Neural Network (FFN) is used after the multi-head attention mechanism in both the encoder and decoder layers. It introduces non-linearity into the model and transforms the input features for better representation.\\n\\nHow It Works:\\n\\nThe FFN is applied independently to each position in the sequence. It consists of two linear transformations with a ReLU activation function in between:\\n\\nThe same set of weights is used for each position, making it a position-wise FFN.\\n\\nPurpose:\\n\\nTo introduce non-linear transformations, which help the model learn complex patterns in the data.\\n\\nHelps capture relationships and dependencies between different features in the transformed space.\\n\\nPositional Encoding:\\n\\nSince the Transformer architecture does not use recurrence or convolution, it lacks a mechanism to capture the order of words in a sequence. Positional Encoding is introduced to inject information about the positions of tokens in a sequence.\\n\\nHow It Works:\\n\\n\\n\\nPositional encodings are added to the input embeddings before they are fed into the encoder. The positional encoding vector has the same dimension as the input embeddings.\\n\\nThe positional encoding uses a combination of sine and cosine functions to represent different positions in the sequence:\\n\\nPurpose:\\n\\nAllows the model to distinguish between different positions in the sequence.\\n\\nHelps the Transformer understand the order of words, which is crucial for tasks like language modelling and translation.\\n\\nLayer Normalization in Transformer:\\n\\nLayer Normalization is a normalization technique used to stabilize the training process and improve convergence by normalizing the input across the features for each data point.\\n\\nHow It Works:\\n\\nIn a Transformer layer, normalization is applied before the feed-forward neural network and multi-head attention sub-layers.\\n\\nFor a given input vector x-bar layer normalization is computed as:\\n\\nPurpose:\\n\\nHelps in stabilizing the training process by reducing the internal covariate shift.\\n\\nImproves gradient flow through the network and enables deeper networks to be trained.\\n\\nDecoder Masked Attention:\\n\\nIn the decoder of the Transformer, there is a mechanism called Masked Self-Attention, which prevents the model from attending to future tokens in the sequence during training.\\n\\nHow It Works:\\n\\nIn masked self-attention, an upper triangular mask is applied to the attention matrix, setting the weights of future positions to negative infinity.\\n\\nThis ensures that, when predicting the next token in the sequence, the model can only attend to the current and previous tokens.\\n\\n\\n\\nPurpose:\\n\\nEnsures the model generates the output auto regressively, meaning one token at a time.\\n\\nPrevents the model from “cheating” by looking at future tokens, thereby maintaining causality.\\n\\nEncoder Decoder Multi-Headed Attention:\\n\\nThe Multi-Head Attention mechanism is used in both the encoder and decoder to allow the model to focus on different parts of the input sequence or previously generated tokens simultaneously.\\n\\nHow It Works:\\n\\nMulti-Head Attention splits the input into multiple heads, applies self-attention independently to each head, and then concatenates the results:\\n\\nPurpose in Encoder:\\n\\nSelf-Attention in Encoder: Each word in the input attends to every other word, helping the model understand relationships within the input sequence.\\n\\nPurpose in Decoder:\\n\\nMasked Self-Attention in Decoder: Allows the model to attend/focus only to previous words when generating the next token.\\n\\nEncoder-Decoder Attention: The decoder uses multi-head attention to attend to the encoder’s output, helping it incorporate information from the input sequence when generating output tokens.\\n\\nLiner and Softmax layer:\\n\\nAfter the multi-head attention and feed-forward neural network, the output goes through a linear transformation followed by a Softmax function to generate probabilities for the target vocabulary.\\n\\nHow It Works:\\n\\nLinear Layer: Projects the final output vector to a vector of size equal to the vocabulary size. This step converts the hidden states into logits for each possible token in the vocabulary.\\n\\nSoftmax Layer: Applies the Softmax function to convert the logits into a probability distribution over the vocabulary.\\n\\nPurpose:\\n\\nConverts the hidden representations into a probability distribution, allowing the model to predict the next word in the sequence.\\n\\n\\n\\nContextual Embeddings:\\n\\nContextual Embeddings: An Overview\\n\\nContextual embeddings are representations of words or phrases that take into account the context in which they appear. Unlike traditional word embeddings (such as Word2Vec or GloVe), which generate a single static vector for each word regardless of its usage, contextual embeddings produce different vectors for the same word depending on the surrounding words, capturing the nuances of meaning based on context.\\n\\nExample of Contextual Embeddings:\\n\\nConsider the word “bank” in two different sentences:\\n\\n“He sat on the river bank.”\\n\\n“She went to the bank to deposit money.”\\n\\nIn these examples:\\n\\nIn the first sentence, “bank” refers to the side of a river.\\n\\nIn the second sentence, “bank” refers to a financial institution.\\n\\nA contextual embedding model, like BERT or GPT, would generate different vector representations for “bank” in each sentence because the surrounding context indicates different meanings. This is in contrast to traditional word embeddings, where “bank” would have the same vector representation in both cases.\\n\\nHow Contextual Embeddings Works:\\n\\nContextual embedding models use Transformer-based architectures that rely on self-attention mechanisms to understand the relationship between each word and its surrounding context within a sentence. These models process entire sentences at once, capturing the meaning of each word as influenced by other words in the sequence.\\n\\nArchitecture of Embedding Models: Explanation using an Example (e.g., text-embedding-3-large):\\n\\nAn embedding model like OpenAI\\'s text-embedding-3-large is designed to generate high-quality embeddings for various natural language processing (NLP) tasks. \\n\\nLet\\'s explain the architecture typically found in such models:\\n\\nInput Layer:\\n\\nThe input to the model is a sequence of tokens, which are typically words or subwords (e.g., \"bank\", \"river\", etc.). The input text is tokenized using a tokenizer.\\n\\nEach token is converted into a corresponding input embedding, which includes word embeddings, positional encodings, and token type embeddings (if used).\\n\\nTransformer Layers (Contextual Embedding Generation):\\n\\nThe core of the embedding model consists of multiple layers of Transformers. Each layer includes multi-head self-attention and feed-forward neural networks:\\n\\nMulti-Head Self-Attention: Allows the model to focus on different parts of the input sequence simultaneously, capturing dependencies between words regardless of their positions.\\n\\nFeed-Forward Neural Network (FFN): Add non-linearity to the model, helping it learn complex patterns in the data.\\n\\nResidual Connections and Layer Normalization: These help stabilize training and improve gradient flow.\\n\\nThe model generates contextual embeddings at each layer, where each word\\'s representation is influenced by its surrounding context. The deeper the layer, the more refined the contextual information becomes.\\n\\nOutput Layer (Embedding Extraction):\\n\\nThe final embeddings are extracted from the last layer of the Transformer. Depending on the task, the output could be:\\n\\nToken-level embeddings: Representing each word or subwords in the sequence, useful for tasks like named entity recognition.\\n\\nToken-level embeddings: Representing each word or subwords in the sequence, useful for tasks like named entity recognition.\\n\\nTraining the Embedding Model:\\n\\nEmbedding models like text-embedding-3-large are trained on large corpora using tasks such as masked language modelling (e.g., predicting masked words) or next sentence prediction. This helps the model learn meaningful relationships and generate high-quality embeddings.\\n\\nDifference between Language Model and Large Language Model:\\n\\nLanguage Model (LM) and Large Language Model (LLM) are terms often used interchangeably, but there are some key differences between them, particularly in terms of scale, capabilities, and architecture. Let\\'s explore these differences in detail.\\n\\nLanguage Model (LM):\\n\\nA Language Model (LM) is a model that is trained to understand and generate human language. The primary goal of a language model is to predict the next word in a sequence or to estimate the probability of a given word sequence. Language models are used in various natural language processing (NLP) tasks such as speech recognition, text generation, machine translation, and sentiment analysis.\\n\\nCommon Characteristics of Language Model:\\n\\nTrained on Limited Datasets: Traditional language models are trained on smaller or more specific datasets.\\n\\nSmaller Parameter Size: They usually have a smaller number of parameters, which limits their ability to understand complex language patterns.\\n\\nArchitecture: Traditional language models can use different architectures like n-grams, Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, or Transformers.\\n\\nNot necessarily. Early language models used n-grams, RNNs, or LSTMs, which do not follow the Transformer architecture. However, many recent language models do use the Transformer architecture due to its superior performance.\\n\\nLarge Language Model (LLM):\\n\\nA Large Language Model (LLM) is an advanced type of language model that is trained on massive datasets with billions or even trillions of parameters. LLMs leverage the Transformer architecture and are capable of understanding and generating human-like text in more sophisticated ways.\\n\\nCommon Characteristics of Large Language Model:\\n\\nTrained on Massive Datasets: LLMs are trained on large corpora of text, including books, articles, websites, and other textual sources.\\n\\nLarge Number of Parameters: These models have hundreds of millions to trillions of parameters, enabling them to understand complex language patterns, perform reasoning, and generate coherent text.\\n\\n\\n\\nUse of Transformer Architecture: LLMs are typically based on the Transformer architecture, which allows for efficient parallel processing and capturing of long-range dependencies in text.\\n\\nPre-Training and Fine-Tuning: LLMs are often pre-trained on large datasets in an unsupervised manner and then fine-tuned on specific tasks for better performance.\\n\\nScalability: LLMs consist of many layers (12, 24, or even 96+ layers) in the Transformer architecture, enabling them to capture deep semantic relationships. Use large embedding sizes and multiple attention heads, increasing their capacity to understand language nuances.\\n\\nYes, they are almost exclusively based on the Transformer architecture. The ability to scale with large datasets, handle long sequences, and perform parallel processing has made Transformers the standard architecture for LLMs.\\n\\nThe main difference between Language Models (LMs) and Large Language Models (LLMs) lies in the scale and capabilities of the models. While LLMs are always built on the Transformer architecture, traditional LMs may use a variety of architectures such as n-grams, RNNs, or LSTMs. The advent of Transformers has significantly advanced the development of LLMs, allowing them to achieve state-of-the-art performance in many natural language processing tasks.'),\n"," Document(metadata={'source': '/content/gen_ai_docs.docx'}, page_content='Generative AI Part 2\\t\\tdibyendubiswas1998@gmail.com\\n\\nPros and Cons of NoSQL Database for Vector Search:\\n\\nNoSQL databases, such as MongoDB and Cassandra, are becoming increasingly popular for use in vector search applications, especially in fields like machine learning and AI. Vector search is critical for tasks such as recommendation systems, natural language processing, and image retrieval, where data is represented as high-dimensional vectors. Let’s explore the pros and cons of using NoSQL databases for vector search with a real-world example.\\n\\nPros of NoSQL for Vector Search:\\n\\nScalability: NoSQL databases are inherently designed to scale horizontally, which means they can handle large datasets and distribute them across many servers. This is crucial for vector search applications that require quick retrieval from massive datasets.\\n\\nExample: Suppose you\\'re building an e-commerce recommendation engine that stores millions of product embeddings (vector representations). A NoSQL database like MongoDB can scale horizontally to support fast retrieval across large clusters of data.\\n\\nFlexibility in Data Models: NoSQL databases support unstructured and semi-structured data, which is often the case in machine learning applications. They can store not only vector representations but also associated metadata in flexible document structures.\\n\\nExample: In a recommendation system, you may store user behaviour as metadata alongside the vector embeddings, allowing more personalized and context-aware vector search.\\n\\nLow Latency Retrieval: With efficient indexing and sharding, NoSQL databases can deliver low-latency retrieval for vector search, making them suitable for real-time applications.\\n\\nExample: A chatbot application might use vector search to match user queries to relevant responses in real-time, requiring near-instantaneous retrieval.\\n\\nDistributed and Fault-Tolerant: Most NoSQL databases are distributed and come with built-in fault tolerance. This ensures that even in the event of hardware failures, the vector search can continue without significant downtime.\\n\\nExample: In a large-scale ML platform using vector search for recommendation, NoSQL can ensure high availability even if a node in the cluster goes down.\\n\\nCons of NoSQL for Vector Search:\\n\\nLimited Nativ Support for Vector Operations: NoSQL databases were not originally designed for vector search. While they can store vectors, they often lack specialized algorithms (like approximate nearest neighbour search, ANN) that are critical for efficient vector search in high-dimensional space.\\n\\nExample: If you\\'re using MongoDB to store embeddings of product reviews, running a k-nearest neighbours (k-NN) query might require additional layers of application logic or external libraries, which increases complexity and slows down performance.\\n\\nQuery Performance at Scale: NoSQL databases may struggle with performance at a large scale if the database doesn\\'t have built-in support for high-dimensional search. Running vector similarity queries can become inefficient as datasets grow larger, especially when specialized indexing techniques like HNSW or FAISS are absent.\\n\\nExample: An image search engine that uses millions of image embeddings might face slowdowns when using NoSQL for vector search, as performing cosine similarity calculations on such large datasets requires heavy computation.\\n\\n\\n\\nLack of Specialized Indexing: Unlike vector search engines like Pinecone, Milvus, or FAISS, most NoSQL databases do not support highly optimized vector search indexing techniques. Instead, they rely on traditional indexing methods, which can limit performance for high-dimensional vectors.\\n\\nExample: If you’re building a real-time facial recognition system, using NoSQL for vector search would lead to slower retrieval compared to a dedicated vector search solution, because NoSQL databases often lack optimized tree or graph-based indexes for vector.\\n\\nComplexity in Integration: NoSQL databases don’t natively support vector search, so integrating an efficient vector search mechanism often requires additional development, such as using external libraries or a separate vector search service. This adds architectural complexity and maintenance overhead.\\n\\nExample: If you\\'re building a personalized search engine, you may need to integrate MongoDB with a separate ANN engine (like FAISS), complicating the data pipeline and increasing system complexity.\\n\\nWhy Use NoSQL and Vector Database in Generative AI Projects?\\n\\nHandling Unstructured and Semi-Structured Data:\\n\\nGenerative AI models often deal with various types of data, such as text, images, audio, and even unstructured data. NoSQL databases, like MongoDB or Cassandra, are ideal for storing this data as they support flexible schema structures.\\n\\nFor instance, language models trained on textual data may need to access both structured information (like user profiles) and unstructured content (like user-generated text). NoSQL databases can seamlessly handle this mix.\\n\\nEfficient Similarity Search:\\n\\nMany generative AI applications (e.g., text generation, image generation) require efficient similarity searches. Vector databases are purpose-built to handle high-dimensional vector embeddings, which are often the result of deep learning models.\\n\\nFor instance, in a generative AI-based recommendation system, embeddings for users and items are stored in a vector database, allowing the system to quickly find similar vectors and generate personalized recommendations.\\n\\nSupport for Real-Time and Low-Latency Application:\\n\\nNoSQL databases offer low-latency access to large datasets, which is crucial for real-time generative AI applications like chatbots, recommendation engines, and image generation systems.\\n\\nVector databases, with optimized nearest-neighbor search algorithms (such as Approximate Nearest Neighbors), provide fast query responses for applications that require quick similarity matching.\\n\\nHow People Use NoSQL and Vector Databases in Generative AI Projects:\\n\\nNoSQL Database Usages:\\n\\nStorage of Training and Metadata: NoSQL databases, like MongoDB, store metadata about the training data, such as user profiles, documents, and interaction histories. These databases also serve as repositories for storing fine-tuned model parameters, user data, and other non-relational datasets.\\n\\n\\n\\nExample: In a chatbot application, MongoDB stores user conversations, interactions, and preferences in flexible JSON-like documents. The generative AI model can then use this data to generate responses based on past conversations.\\n\\nData processing: In AI workflows, NoSQL databases are often used for pre-processing data before it\\'s fed into the model. Unstructured or semi-structured data, like social media posts or news articles, can be stored and then transformed into structured formats for model training.\\n\\nExample: In a text generation model, you may scrape web articles or social media data and store them in MongoDB, where the data can be cleaned and pre-processed before being fed into the model for fine-tuning.\\n\\nVector Database Usages:\\n\\nEfficient Retrieval of Embeddings: Vector databases are used to store and retrieve vector embeddings generated by AI models. These embeddings are high-dimensional representations of data (e.g., text, images) and are used for similarity searches.\\n\\nExample: In an AI-powered image generator, vector databases like Milvus store image embeddings. When a user uploads a sketch or partial image, the system retrieves similar images from the database based on their vector representation, helping the generative model create a new image.\\n\\nSimilarity Search for Generative AI: Vector databases enable fast similarity searches by efficiently finding vectors that are close to each other in high-dimensional space. This is crucial in generative AI applications where matching embeddings is key for generating content (e.g., finding similar texts, images, or user preferences).\\n\\nExample: In a recommendation engine powered by a generative AI model, vector embeddings of user preferences and product attributes are stored in a vector database. When a user requests a recommendation, the system retrieves the nearest product vectors, and the AI model generates a list of personalized recommendations.\\n\\nBenefits of Using NoSQL and Vector Databases in Generative AI Projects:\\n\\nScalable Data Storage:\\n\\nNoSQL databases handle massive, complex datasets without strict schema requirements, making them ideal for AI applications where data formats evolve frequently.\\n\\nVector databases are optimized to store embeddings at scale, providing efficient querying and retrieval capabilities.\\n\\nReal-Time Data Processing:\\n\\nNoSQL databases offer fast access to diverse datasets, supporting real-time generative AI applications like chatbots or image generation.\\n\\nVector databases support low-latency similarity searches, which is essential for applications like recommendation systems and conversational AI.\\n\\nEfficient Vector Search:\\n\\nVector databases handle the complexity of high-dimensional vector search (e.g., nearest neighbor search), enabling faster and more accurate retrieval of data points for generative tasks like text generation, image synthesis, and voice cloning.\\n\\nFlexibility:\\n\\n\\n\\nThe flexibility of NoSQL databases allows for easy storage and manipulation of heterogeneous data types (e.g., text, images, audio), which is critical for multimodal AI applications.\\n\\nVector databases are highly specialized for vector retrieval, allowing for seamless integration with deep learning models that generate embeddings.\\n\\nUsing NoSQL databases for storing diverse and large datasets, along with Vector databases for efficient similarity searches, enhances the performance, scalability, and flexibility of generative AI applications. These technologies enable real-time data handling, seamless integration of multimodal data, and efficient retrieval of relevant information for tasks like content generation, personalization, and recommendation.\\n\\nPinecone Vector Database:\\n\\nPinecone is a specialized vector database designed to store, manage, and query vector embeddings. It is built specifically for high-dimensional vector search and enables applications like semantic search, recommendation systems, question-answering, and image retrieval, where traditional databases struggle to perform efficiently.\\n\\nVectors are numerical representations of data (e.g., text, images, audio) that encode the semantics of the information in high-dimensional space. Pinecone provides an infrastructure for managing these vectors, allowing users to quickly perform similarity searches, nearest-neighbor lookups, and other vector-based queries.\\n\\nKey features of Pinecone:\\n\\nEfficiency Vector Storage and Retrieval: Pinecone is optimized for storing large volumes of high-dimensional vectors and performing efficient nearest-neighbor searches on them. It uses advanced indexing algorithms like Approximate Nearest Neighbor (ANN) search techniques (e.g., HNSW).\\n\\nScalability: It can handle billions of vectors, enabling large-scale applications. Pinecone\\'s infrastructure allows for easy scaling without the complexities of managing distributed databases.\\n\\nReal-Time Search Capabilities: Pinecone supports low-latency vector search, allowing for real-time applications such as interactive chatbots or dynamic content recommendations.\\n\\nIndexing and Filtering: Pinecone offers customizable vector indexing and filtering capabilities, enabling users to perform conditional searches, such as finding vectors that are similar while meeting certain conditions (e.g., vectors associated with a particular category).\\n\\nIntegration with ML Workflows: It integrates seamlessly with machine learning workflows by allowing easy ingestion of embeddings generated by models like BERT, GPT, ResNet, etc. These embeddings can be indexed, searched, and managed using Pinecone.\\n\\nHow Pinecone Works: Step-by-Step Process\\n\\nVector Generation: Machine learning models (e.g., language models like BERT or image models like ResNet) generate embeddings (vectors) from input data such as text, images, or audio.\\n\\nIndexing in Pinecone: These vectors are indexed in Pinecone, which organizes them using an internal data structure optimized for efficient similarity search. The database uses ANN algorithms to index the vectors in a way that supports fast retrieval.\\n\\nQuerying: When a query vector is provided, Pinecone performs a similarity search to find the nearest neighbors (vectors that are most similar to the query). This is typically done using distance metrics such as cosine similarity or Euclidean distance.\\n\\n\\n\\nResult Filtering: Pinecone allows for filtering based on metadata associated with vectors. For example, you can search for vectors with specific tags or within certain ranges.\\n\\nReal-Time Updates and management: Vectors can be added, updated, or deleted in real-time, ensuring that the search results remain relevant.\\n\\nExample use Cases: Semantic Search\\n\\nLet’s look at how Pinecone can be used for a semantic search application.\\n\\nScenario: Building a Document Search Engine.\\n\\nSuppose you have a collection of articles, and you want to create a search engine where users can find relevant articles based on the meaning of their search queries, not just keyword matching.\\n\\nStep-by-Step Workflow:\\n\\nText Data Preparation: You have a dataset of articles, and each article consists of multiple paragraphs.\\n\\nEmbedding Generation: Use a pre-trained language model (e.g., BERT, Sentence Transformers) to generate vector embeddings for each paragraph in the articles. These embeddings represent the semantic meaning of the text.\\n\\nIndexing in Pinecone: Push these embeddings into a Pinecone index, where they are stored as vectors. You can also associate metadata with each vector, such as the article title, publication date, or paragraph ID.\\n\\nUser Query Handling: When a user performs a search query, convert the query into a vector embedding using the same language model.\\n\\nQuerying Pinecone for Similarity Search: Use Pinecone to find the most similar vectors to the query vector. This search retrieves the paragraphs that are semantically closest to the user\\'s input.\\n\\nFiltering and Ranking: Optionally, apply filters to limit the results based on metadata (e.g., only search within certain categories). Pinecone ranks the results based on similarity scores.\\n\\nPresenting the Results: Display the retrieved articles or paragraphs to the user in a ranked list.\\n\\nTechnical Details Behind the Pinecone:\\n\\nIndexing Techniques: Pinecone uses ANN (Approximate Nearest Neighbor) search techniques, such as HNSW (Hierarchical Navigable Small World), to quickly find vectors that are close to the query vector.\\n\\nDistance Metrics: Common metrics used include cosine similarity, Euclidean distance, and dot product. These metrics measure how \"close\" vectors are in the high-dimensional space.\\n\\nCosine Similarity:\\n\\nCosine similarity measures the cosine of the angle between two non-zero vectors in a high-dimensional space. It is a measure of the directional similarity between vectors, regardless of their magnitudes. The resulting similarity value ranges from -1 to 1:\\n\\n1 means the vectors are identical in direction.\\n\\n0 means the vectors are orthogonal (no similarity).\\n\\n1 means the vectors are diametrically opposite in direction.\\n\\nFormula: \\n\\n\\n\\nUse Case:\\n\\nCosine similarity is often used in text analysis (e.g., comparing the similarity of documents) because it captures the similarity in the direction of word embeddings without being affected by the length of the documents.\\n\\nExample:\\n\\nEuclidean Distance:\\n\\nEuclidean distance is a measure of the straight-line distance between two points in a multi-dimensional space. It quantifies the magnitude of the difference between the two vectors. The smaller the distance, the more similar the vectors are.\\n\\nFormula:\\n\\nUse Case: \\n\\nEuclidean distance is often used in clustering algorithms (e.g., K-means) and nearest-neighbor search to measure how far apart data points are in a feature space.\\n\\nExample:\\n\\n\\n\\nDot Product:\\n\\nThe dot product (or inner product) of two vectors measures the extent to which the vectors point in the same direction. It is the sum of the products of the corresponding components of the vectors.\\n\\nFormula:\\n\\nUse case:\\n\\nThe dot product is commonly used in machine learning and vector mathematics to project one vector onto another and to compute cosine similarity. It also helps in understanding whether two vectors are aligned or perpendicular (if the dot product is zero).\\n\\nExample:\\n\\nScaling Capabilities: Pinecone automatically manages sharding and replication across servers, making it highly scalable for large datasets.\\n\\nFiltering Capabilities: Pinecone supports metadata-based filtering; allowing searches to be constrained by various conditions (e.g., time range, document type).\\n\\nIndexing Algorithms:\\n\\nANN (Approximate Nearest Neighbor):\\n\\nApproximate Nearest Neighbor (ANN) algorithms aim to efficiently find the nearest neighbors of a given query point in a high-dimensional space. Unlike exact nearest neighbor search, which guarantees finding the closest points with 100% accuracy, ANN algorithms trade off some accuracy for faster search times and lower computational costs. This makes ANN techniques suitable for large-scale and high-dimensional data where exact search is computationally infeasible.\\n\\nWhy Use ANN?\\n\\nHigh-dimensional data: In high dimensions, exact nearest neighbor search becomes computationally expensive due to the \"curse of dimensionality.\" The time complexity increases significantly as the number of dimensions grows.\\n\\nLarge datasets: For datasets containing millions or billions of points, exact search is time-consuming. ANN algorithms provide a way to find neighbors quickly with acceptable accuracy.\\n\\nHow ANN Works: General Approach\\n\\nData Structure Creation: ANN algorithms pre-process the data to create an index (e.g., trees, graphs, hash tables) that allows for efficient querying.\\n\\n\\n\\nQuerying: Given a query point, the algorithm uses the index to find a set of candidate neighbors, which are then evaluated to approximate the nearest neighbors.\\n\\nResult Refinement: Depending on the method used, the algorithm may further refine the results to improve accuracy. However, the goal is not to guarantee finding the exact nearest neighbor but to provide a good approximation.\\n\\nCommon ANN Algorithms:\\n\\nLSH (Locality-Sensitive Hashing): Hashes input data so that similar points are mapped to the same bucket with high probability.\\n\\nHNSW (Hierarchical Navigable Small World): A graph-based approach that uses a hierarchical structure to find nearest neighbors efficiently.\\n\\nHNSW  (Hierarchical Navigate Small World):\\n\\nLSH (Locality-Sensitive Hashing):\\n\\nPinecone References Docs (Important Insights):\\n\\nIndexes:\\n\\nIn Pinecone, an index is the primary structure where vector data is stored, managed, and queried. Each index organizes vectors and provides mechanisms for nearest neighbor search, allowing efficient retrieval of similar vectors based on similarity metrics (e.g., cosine similarity, Euclidean distance, dot product).\\n\\nKey Characteristics Indexes:\\n\\nVector Storage: Indexes store vectors along with optional metadata (tags, identifiers) associated with each vector.\\n\\nIndex Types: Pinecone supports different types of indexes optimized for various retrieval methods, including Approximate Nearest Neighbor (ANN) techniques.\\n\\nSimilarity Metrics: When creating an index, you specify the metric (e.g., cosine similarity, Euclidean distance) used to measure vector similarity.\\n\\nExample Usage:\\n\\nIn a semantic search application, an index might be used to store document embeddings. When a query is provided, the index retrieves the most similar document vectors based on cosine similarity.\\n\\nPods:\\n\\nPods in Pinecone refer to the compute units that provide the processing power and storage capacity for an index. Each pod consists of resources (CPU, memory, storage) that manage the data operations, such as vector insertion, indexing, and querying.\\n\\nKey Characteristics of Pods in Pinecone:\\n\\nScaling: Pods can be scaled horizontally to accommodate larger datasets or to improve query throughput. Multiple pods allow for distributed storage and processing.\\n\\nReplication: You can increase the number of replicas for fault tolerance and to handle higher read workloads.\\n\\n\\n\\nPod Size Variants: Different pod size options are available (e.g., s1, s2, etc.), where larger pod sizes provide more computational resources and storage capacity.\\n\\nExample Usage:\\n\\nA small application with a few thousand vectors may use a single pod. For a large-scale recommendation system handling billions of vectors, multiple pods with replication might be used.\\n\\nServerless:\\n\\nServerless in Pinecone refers to the platform\\'s ability to automatically manage infrastructure, allowing developers to focus on building applications without worrying about scaling, provisioning, or maintaining servers.\\n\\nKey Characteristics of Serverless in Pinecone:\\n\\nAutomatic Scaling: The platform scales compute resources up or down based on the workload. This ensures efficient resource usage and cost management.\\n\\nNo Manual Infrastructure Management: Users don\\'t have to manually provision or manage servers; Pinecone takes care of the underlying infrastructure.\\n\\nPay-per-Use: Costs are based on the actual usage, such as the number of queries or the amount of data stored, making it cost-effective for various workloads.\\n\\nExample Usage:\\n\\nFor an application with unpredictable traffic patterns (e.g., seasonal e-commerce), Serverless architecture can automatically adjust resources to meet the demand during peak hours and reduce costs during off-peak times.\\n\\nVarious Data Operations:\\n\\nPinecone supports a variety of data operations that allow you to manage vector data in an index. These operations include:\\n\\nInserting Vectors:\\n\\nAdd new vectors to the index along with any associated metadata.\\n\\nSupports batch inserts to optimize the performance when dealing with large datasets.\\n\\nUpdating Vectors:\\n\\nModify existing vectors by updating their values or associated metadata.\\n\\nUseful for applications where data is continually evolving (e.g., user preferences in a recommendation system).\\n\\nDeleting Vectors:\\n\\nRemove vectors from the index when they are no longer needed.\\n\\nCan also be used to delete specific metadata fields without removing the entire vector.\\n\\nQuerying Vectors:\\n\\nPerform similarity search to find vectors that are closest to a given query vector.\\n\\nSupports filtering by metadata, allowing for conditional queries (e.g., search within specific categories).\\n\\nInference:\\n\\nInference in Pinecone typically involves using pre-trained models to convert raw data (text, images, audio) into vector embeddings. These embeddings are then stored in Pinecone for efficient querying.\\n\\n\\n\\nSteps for Inference with Pinecone:\\n\\nData Preparation: Use machine learning models (e.g., BERT for text, ResNet for images) to generate vector embeddings from raw input.\\n\\nVector Storage: Insert these embeddings into a Pinecone index.\\n\\nQuery and Search: When a new input is provided, generate its embedding and use Pinecone to find the nearest neighbors from the stored vectors.\\n\\nExample:\\n\\nIn a chatbot, user queries are converted into vector embeddings using a language model. Pinecone retrieves the most relevant responses from a database of stored embeddings.\\n\\nAssistance:\\n\\n\\n\\nAdvanced Retrievers:\\n\\nContextual Compressor Retriever:\\n\\nThe Contextual Compressor Retriever is a retrieval technique designed to improve information retrieval by focusing on the most relevant parts of documents based on the user\\'s query. It aims to reduce the size of the retrieved information by compressing the context, thereby retaining only the most important content. This technique enhances the quality of responses and makes it easier for models to work with limited context windows (e.g., when generating responses).\\n\\nHow Contextual Compressor Retriever Works:\\n\\nBasic Retrieval: The process begins with a standard retrieval step, where the initial set of documents or passages relevant to the user\\'s query is retrieved. This could be done using methods like BM25, Dense Vector Retrieval, or Hybrid Retrieval.\\n\\nContextual Compression: After the initial retrieval, a compression step is applied. The retrieved documents are further analysed to identify and retain only the most relevant parts of the documents based on the original query.\\n\\nThe compression can be done using techniques like:\\n\\nExtractive summarization: Selecting key sentences or paragraphs.\\n\\nQuery-focused summarization: Compressing the document by focusing on content directly related to the query.\\n\\nRefined Retrieval with Compressed Context: The compressed documents are then used to perform a more focused retrieval step, ensuring that the most relevant information is considered in subsequent processes, such as answer generation.\\n\\nFinal Output: The results from the refined retrieval are presented as the final output, providing concise and contextually rich information.\\n\\nExample of Contextual Compressor Retriever:\\n\\nUser Query: “What are the benefits of multi-head attention in Transformers?”\\n\\nStep-by-Step Process: \\n\\nInitial Retrieval: Suppose the query retrieves several documents related to “multi-head attention” and “Transformer models.” Each document may contain multiple sections discussing various aspects, such as the architecture, different types of attention, and training techniques.\\n\\nContextual Compression: The retriever then compresses each document to keep only the parts that are most relevant to the benefits of multi-head attention. For example:\\n\\nA passage discussing how multi-head attention allows for parallel processing and captures different patterns in data might be kept.\\n\\nIrrelevant sections, such as implementation details or other types of attention mechanisms, are discarded.\\n\\nRefined Retrieval with Compressed Documents: The compressed documents (containing only the relevant sections) are used to conduct a second round of retrieval, refining the results to ensure they directly address the original query.\\n\\nFinal Output: The final retrieved documents contain concise, focused information about the benefits of multi-head attention, such as its ability to capture various relationships in the data and enhance model performance.\\n\\nBenefits of Contextual Compressor Retriever:\\n\\nReduces Irrelevant Information: By compressing documents to focus on relevant content, the technique ensures that the retrieved results are concise and useful.\\n\\n\\n\\nHandles Long Documents Better: For very long documents, retrieving and compressing the most relevant sections makes it easier to work within limited context windows (e.g., models with limited token input).\\n\\nImproved Retrieval Accuracy: The double-layered retrieval (initial and refined with compressed content) enhances accuracy by filtering out unnecessary details and focusing on the core information.\\n\\nSelf-Query Retriever:\\n\\nA Self-Query Retriever is an advanced mechanism used in Retrieval-Augmented Generation (RAG) to improve the retrieval process by generating better and more contextually relevant queries. It transforms the original input query into a more refined or structured set of sub-queries or expansions. These refined queries are then used to search the knowledge base, ensuring more accurate retrieval of relevant documents or passages. “Self-Query retriever is works when you have metadata information in your knowledgebase or vector database”. \\n\\nExample: How Self Query Retriever Works:\\n\\nScenario: \\n\\nInput Query: “How does Transformer models work?”\\n\\nStep-by-Step Process:\\n\\nOriginal Query: The user\\'s input query is “How does Transformer models work?”\\n\\nGenerating Refined Sub-Queries: The Self-Query Retriever generates several sub-queries:\\n\\n“What is self-attention in Transformer models?”\\n\\n\\n\\n“What role does positional encoding play in Transformers?”\\n\\n“How do Transformer models differ from LSTM models?”\\n\\nRetrieving Relevant Documents: Each of the sub-queries is used to search the knowledge base, retrieving documents that cover these specific aspects of Transformer models.\\n\\nCombining the Results: The relevant information retrieved from the various sub-queries is combined and synthesized to provide a comprehensive answer to the original query.\\n\\nFinal Response: The system generates a final answer by integrating information about self-attention, positional encoding, and comparisons to other models, providing a detailed explanation of how Transformer models work.\\n\\nWhen to Use Self Query Retrieve:\\n\\nA Self-Query Retriever is particularly useful in scenarios where:\\n\\nComplex or Ambiguous Queries: The original query is either too broad or lacks specificity. The Self-Query Retriever can break down the query into more specific sub-queries.\\n\\nMulti-Faced Information Needs: The user’s question may contain multiple aspects or dimensions that need to be addressed separately. For example, a question like \"What are the advantages of multi-head attention in Transformers?\" may involve aspects such as computational efficiency, interpretability, and modelling power.\\n\\nImproving Recall and Precision in Retrieval: When traditional keyword-based searches fail to capture the context or meaning of the query, refined sub-queries generated by a Self-Query Retriever can enhance the retrieval process by targeting different angles of the question.\\n\\nProblems with Basic RAG Workflow:\\n\\nIn a basic RAG workflow, there are some challenges that the Self-Query Retriever can help overcome:\\n\\nPoor Query Representation:\\n\\nIn a standard RAG setup, the original input query is used directly to search the knowledge base. If the query is ambiguous, incomplete, or too broad, the retrieval may not find the most relevant documents.\\n\\nSolution: The Self-Query Retriever generates refined sub-queries that are more targeted and align better with the knowledge base.\\n\\nHandling Complex or Multi-Part Queries:\\n\\nBasic RAG workflows may struggle to handle queries that require multiple pieces of information or cover different subtopics. For example, \"Explain how transformers and LSTMs differ in handling sequential data.\"\\n\\nSolution: The Self-Query Retriever can break the query into multiple sub-queries, addressing each aspect separately, thus improving the completeness of the response.\\n\\nLimited Recall:\\n\\nIn a basic RAG workflow, if the original query does not match well with the content in the knowledge base, relevant documents may be missed.\\n\\nSolution: By using multiple refined sub-queries, the Self-Query Retriever increases the chances of retrieving relevant documents, thereby improving recall.\\n\\nUnstructured Queries:\\n\\nBasic RAG may struggle with queries that are not well-structured or follow natural language conventions, leading to poor retrieval.\\n\\nSolution: The Self-Query Retriever converts such queries into more structured sub-queries, making it easier for the system to retrieve relevant information.\\n\\n\\n\\nParent Document Retriever:\\n\\nThe Parent Document Retriever is a retrieval strategy used in information retrieval and RAG (Retrieval-Augmented Generation) workflows. It aims to improve the quality of retrieved information by leveraging the relationship between smaller document chunks (children) and their larger parent documents. This method ensures that the context of the information is preserved and provides more comprehensive answers to user queries.\\n\\nHow Parent Document Retrieval Works:\\n\\nThe Parent Document Retriever works by first retrieving relevant child documents (smaller chunks of text) and then identifying their corresponding parent documents (the larger documents from which the chunks were derived). This process helps maintain the original context and provides richer information.\\n\\nSteps-by-Steps Workflow of the Parent Document Retriever:\\n\\nData Preparation and Chunking: The original documents (parent documents) are divided into smaller chunks or sections (child documents) for indexing. For example, a research paper could be broken down into sections or paragraphs.\\n\\nIndexing the Child Documents: The smaller chunks (child documents) are indexed in a vector database, allowing them to be retrieved based on their semantic similarity to a given query.\\n\\nQuery Processing and Initial Retrieval: When a user submits a query, the retrieval process starts by finding the top-ranked child documents (chunks) that are most similar to the query using similarity metrics such as cosine similarity or Euclidean distance.\\n\\nMapping to Parent Documents: Once the relevant child documents are retrieved, the Parent Document Retriever maps these child documents to their corresponding parent documents. This involves identifying the larger original documents that contain the retrieved chunks.\\n\\nCombining the Results and Ranking Parent Documents: The retrieved parent documents are aggregated and ranked based on the relevance of the child documents retrieved. This ensures that the most relevant parent documents are presented to the user.\\n\\nFinal Output: The system returns the parent documents (or sections of the parent documents) as the final retrieval result, providing a more complete and contextually accurate response.\\n\\nAdvantages Parent Document Retrieval:\\n\\nMaintains Context: By mapping child documents back to their parent documents, the approach ensures that the retrieved information retains its original context, leading to more accurate and comprehensive responses.\\n\\n\\n\\nImproved Retrieval Accuracy: Parent Document Retrieval reduces the chances of providing isolated or out-of-context information by aggregating related content from the same source.\\n\\nBetter Coverage of Multi-Part Queries: When a query requires multiple pieces of information, this approach ensures that various relevant sections from the same parent document can be retrieved together.\\n\\nWhen to Use Parent Document Retrieval:\\n\\nLong Documents or Structured Content: It is beneficial when dealing with long documents (e.g., books, research papers, manuals) that are broken down into smaller sections. Parent Document Retrieval ensures the relevant sections are tied back to the full document.\\n\\nPreserving Context: It is useful when the context of the information is important, such as legal documents or technical reports, where individual sections may not make sense without the larger context.\\n\\nMulti-Step Information Retrieval: Suitable for use cases where the retrieval needs to be refined from coarse-grained (parent level) to fine-grained (child level), or vice versa.\\n\\nSentence Window Retriever:\\n\\nSentence Window Retrieval is a retrieval technique used to enhance the information extraction process by considering not just a single sentence but also a specified number of surrounding sentences (context) around it. The main idea is to improve the quality of retrieval by capturing the broader context in which a sentence appears.\\n\\nHow Sentence Window Retrieval Works:\\n\\nData Preparation: A text document is divided into individual sentences.\\n\\nWindow Size Definition: The window size is defined (e.g., window=3) which specifies how many surrounding sentences (both before and after the target sentence) should be considered together as a single unit for retrieval.\\n\\nCreating Sentence Windows:\\n\\nFor each sentence in the document, a “window” is formed by taking the sentence itself and a number of surrounding sentences (before and after) based on the window size.\\n\\nThese sentence windows become the basic units used for retrieval.\\n\\nIndexing Sentence Windows: The sentence windows are indexed in a vector database or search engine. This allows for efficient similarity search when performing retrieval.\\n\\nQuery Processing and Retrieval:\\n\\nWhen a query is issued, it is matched against the indexed sentence windows.\\n\\nThe model retrieves the most relevant sentence windows based on the similarity of the query to the combined context in each window.\\n\\nExample of Sentence Window retrieval:\\n\\nSuppose we have the following text divided into sentences:\\n\\nSentence 1: “Transformers are powerful models for NLP.”\\n\\nSentence 2: “They use self-attention mechanisms.”\\n\\nSentence 3: “The architecture allows for parallel processing.”\\n\\nSentence 4: “Training requires large amounts of data.”\\n\\nSentence 5: “Fine-tuning improves the model\\'s performance.”\\n\\n\\n\\nWindow=3 Examples:\\n\\nWindow Size: Window=3 means that for each sentence, we take the target sentence plus one preceding and one following sentence.\\n\\nCreating Sentence Windows:\\n\\nFor Sentence 2 (“They use Self-Attention mechanisms.”):\\n\\nThe surrounding context would include Sentence 1, Sentence 2, and Sentence 3:\\n\\nCombined Window: “Transformers are powerful models for NLP. They use self-attention mechanisms. The architecture allows for parallel processing.”\\n\\nFor Sentence 3 (“The architecture allows for parallel processing.”)\\n\\nThe surrounding context would include Sentence 2, Sentence 3, and Sentence 4:\\n\\nCombined Window: “They use self-attention mechanisms. The architecture allows for parallel processing. Training requires large amounts of data.”\\n\\nIn this example, each window combines three sentences to form a richer context, providing better retrieval results than using individual sentences alone.\\n\\nSurrounding Context:\\n\\nThe surrounding context refers to the additional sentences included around the target sentence when forming a sentence window. This context helps to capture more information about the topic and maintains the continuity of the narrative, making the retrieval more accurate and meaningful.\\n\\nBenefits of Sentence Window Retrieval:\\n\\nImproved Contextual Understanding: By including surrounding sentences, the model can better understand the context and retrieve more relevant information.\\n\\nEnhanced Retrieval Accuracy: It reduces the chance of retrieving isolated sentences that lack meaningful context.\\n\\nSuitable for Complex Queries: When a query requires detailed understanding or involves multiple aspects, using sentence windows helps provide more comprehensive responses.\\n\\nSentence Window Retrieval enhances traditional sentence-level retrieval by incorporating surrounding context to form a \"window\" of sentences. The window size determines how many neighbouring sentences are included, and this technique is especially useful when trying to capture broader contextual information to improve the accuracy and quality of retrieval results. For “window=3” the model uses the target sentence plus one preceding and one following sentence, creating a three-sentence window for better context.\\n\\nHypothetical Document Embedding (HYDE):\\n\\nHypothetical Document Embedding (HYDE) is a technique used in information retrieval to improve the quality of query-based document retrieval. Instead of relying solely on a given query to perform a search, HYDE generates hypothetical documents based on the query. These documents are then used as proxy queries to find relevant documents in the database. By doing so, HYDE can help retrieve more contextually relevant results.\\n\\nHow HYDE Works:\\n\\nInitial Query Processing: Start with a user-provided query. For example, let\\'s say the query is: “What are the benefits of multi-head attention in Transformers?”\\n\\n\\n\\nGenerating Hypothetical Documents:\\n\\nUse a language model (such as GPT-3 or BERT) to generate a hypothetical document or passage based on the input query. This document serves as a possible answer or elaboration related to the query.\\n\\nFor the example query, a hypothetical document could be: “Multi-head attention allows Transformers to jointly attend to information from different representation subspaces at different positions. This enhances the model\\'s ability to capture more nuanced relationships in the data.”\\n\\nEmbedding the Hypothetical Documents: The generated hypothetical document is embedded into a vector space using a pre-trained embedding model (such as BERT embeddings or Sentence Transformers).\\n\\nRetrieval using the Embedded Hypothetical Document: The hypothetical document embedding is used as a proxy query to search the vector database. This helps retrieve documents that are contextually similar to the hypothetical document, thereby addressing the original query.\\n\\nCombining Results: The documents retrieved using the hypothetical document embeddings are presented as the final results.\\n\\nAdvantages of HYDE:\\n\\nEnhances Retrieval Quality: By generating a hypothetical document, HYDE provides more contexts to the search process, improving the relevance of the retrieved results.\\n\\nReduces Ambiguity: The technique helps in addressing ambiguous queries by generating a more detailed hypothetical representation of the user\\'s intent.\\n\\nSuitable for Complex Queries: HYDE works well when dealing with complex or broad queries, where direct retrieval using the original query may not be effective.\\n\\nAuto Merger Retriever:\\n\\nThe Auto Merger Retriever is an approach used in information retrieval to combine multiple retrieval strategies and merge their results to improve the overall quality of retrieval. The idea is to use different retrieval techniques or configurations to obtain candidate documents and then automatically merge these results to get a final ranked list.\\n\\nHow Auto Merger Retriever Works:\\n\\nMultiple Retriever Methods:\\n\\nThe Auto Merger Retriever employs several different retrieval methods or configurations. For example, it might use:\\n\\nBM25: A traditional keyword-based retrieval method.\\n\\nDense Vector Retriever: Using embeddings to retrieve documents based on semantic similarity.\\n\\nHybrid Retrieval: Combining both sparse (BM25) and dense vector retrieval.\\n\\nCandidate Document Retrieval: Each retrieval method produces its own set of candidate documents based on the given query. The same query is run through all the configured retrieval techniques, resulting in multiple ranked lists of documents.\\n\\nMerging the Results: The results from all the different retrieval methods are automatically merged using a merging strategy. This could involve:\\n\\nCombining the ranks of documents from each list.\\n\\nWeighted voting based on the confidence score from each retrieval method.\\n\\n\\n\\nLearning-to-rank techniques to adjust the merged ranking based on training data.\\n\\nFinal Ranked List Presentation: The merged list is then presented as the final set of retrieved documents.\\n\\nExample of Auto Merger Retriever:\\n\\nUser Query: “What are the applications of Transformer models in NLP?”\\n\\nStep-by-Step Process: \\n\\nUsing Multiple Retrieval Methods:\\n\\nBM25 Retrieval might return documents containing exact keyword matches for “Transformer models” and “NLP applications.”\\n\\nDense vector Retrieval might return documents that semantically match the query, even if they don\\'t contain the exact keywords.\\n\\nHybrid Retrieval might combine keyword and semantic search results.\\n\\nMerge the Results: Suppose each method returns a list of top 10 documents. The Auto Merger Retriever takes these lists and merges them based on a merging strategy (e.g., average rank, weighted score).\\n\\nGenerate the Final Ranked List: The merged list contains documents from different sources that have been re-ranked to ensure the most relevant documents appear at the top.\\n\\nPresent the Merged Results: The final merged ranked list is presented to the user, providing a more comprehensive set of results.\\n\\nAdvantages of Auto Merger Retriever:\\n\\nCombines Strengths of Different Methods: It leverages the advantages of different retrieval strategies, such as traditional keyword matching and modern vector-based retrieval.\\n\\nImproved Converge and Diversity: The approach ensures that relevant documents are not missed due to the limitations of a single retrieval method.\\n\\nEnhances Retrieval Accuracy: Merging results from different methods often leads to better ranking and more relevant results.\\n\\nBasically, Auto Merger Retriever combines results from multiple retrieval methods to generate a more comprehensive and accurate final list of documents.')]"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["len(retriever_docs[0].page_content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rl59e08OshRl","executionInfo":{"status":"ok","timestamp":1730195684510,"user_tz":-330,"elapsed":865,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"01245d39-477f-4549-df1e-69bef887b5eb"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["60402"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":[],"metadata":{"id":"qc5y-vTlsvFO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Retriever 2:**"],"metadata":{"id":"ZVb17ymBs_x0"}},{"cell_type":"code","source":["\n","# It should create documents smaller than the parent\n","child_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n","\n","\n","# This text splitter is used to create the parent documents\n","parent_splitter = RecursiveCharacterTextSplitter(chunk_size=5000)"],"metadata":{"id":"_KNS7wyAtBit","executionInfo":{"status":"ok","timestamp":1730195892563,"user_tz":-330,"elapsed":542,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["# The storage layer for the parent documents\n","store1 = InMemoryStore()"],"metadata":{"id":"BD__taxXtRtS","executionInfo":{"status":"ok","timestamp":1730195893482,"user_tz":-330,"elapsed":2,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["vectorstore1 = Chroma(\n","    collection_name=\"full_documents\", embedding_function=embedding_model\n",")\n","\n","\n","retriever2 = ParentDocumentRetriever(\n","    vectorstore=vectorstore1,\n","    docstore=store1,\n","    child_splitter=child_splitter,\n","    parent_splitter=parent_splitter,\n",")\n","\n","retriever2.add_documents(parent_docs, ids=None)"],"metadata":{"id":"SNnYtRDFtSAq","executionInfo":{"status":"ok","timestamp":1730195940609,"user_tz":-330,"elapsed":25584,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["print(\"Store 1: \", len(list(store1.yield_keys())))\n","print(\"Store 2: \", len(list(store.yield_keys())))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eDn2neNTtSC5","executionInfo":{"status":"ok","timestamp":1730195961415,"user_tz":-330,"elapsed":9,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"cfc82932-5819-4b38-a660-07ba255046e3"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Store 1:  23\n","Store 2:  2\n"]}]},{"cell_type":"code","source":["retrieved_docs2 = retriever2.invoke(\"Tell me something about Transformer\")\n","retrieved_docs2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j1ZZNl5rtSFy","executionInfo":{"status":"ok","timestamp":1730196026465,"user_tz":-330,"elapsed":552,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"f44b27c5-1994-4bde-acb8-fe0039e73c55"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'source': '/content/ai_ml_docs.docx'}, page_content='Handling Long Sequences: Self-attention allows the model to capture long-range dependencies in a single step, making it better at handling long sentences and complex relationships.\\n\\nScalability: The Transformer is highly scalable, allowing for the training of larger models that achieve superior performance on a variety of tasks.\\n\\nThis model laid the groundwork for subsequent advancements in NLP, most notably BERT, GPT, and other Transformer-based models that now dominate the field of natural language processing.\\n\\n\\n\\nKey Components of the Transformer Model:\\n\\nSelf-Attention Mechanism:\\n\\nThe core innovation of the Transformer is the self-attention mechanism, which allows each token in a sequence to attend to all other tokens, making it possible to capture long-range dependencies.\\n\\nSelf-attention: computes a representation of a sequence by focusing on different parts of the sequence. For each word in the input, the model calculates how much \"attention\" to give to each other word in the sequence.\\n\\nScaled Dot-Product Attention: Given queries ‘Q’, keys ‘K’, and values ‘V’, self-attention is computed as:\\n\\nWhere,\\n\\nQ (query) represents the word being processed.\\n\\nK (key) represents the words being attended to.\\n\\nV (value) is the data associated with each key.\\n\\ndk is the dimensionality of the keys, and the scaling factor ‘root(dk)’ prevents extremely large values from pushing the SoftMax into regions with small gradients.\\n\\nMulti-Headed Attention:\\n\\nTo further improve the model’s ability to focus on different parts of the input sequence, the Transformer employs multi-head attention. Instead of performing a single attention function, the input is projected into multiple smaller spaces, and several attention mechanisms (called heads) are applied in parallel. This allows the model to attend to different aspects of the input at the same time.\\n\\nThe outputs from these heads are then concatenated and projected back into the original space. The advantage is that the model can jointly attend to information from different parts of the sequence more effectively.\\n\\nEach head computes attention independently:\\n\\nPositional Encoding:\\n\\nSince the Transformer lacks the inherent sequential structure of RNNs, it uses positional encodings to inject information about the position of tokens in the sequence. Without positional information, the model would treat all tokens as if they were unordered, which is problematic for tasks like translation, where word order is critical.\\n\\nThe paper introduces sinusoidal positional encodings, where the position of each token is represented by a combination of sine and cosine functions:\\n\\nThese encodings are added to the token embeddings to retain positional information. The choice of sine and cosine ensures that the model can generalize to sequences longer than those seen during training.\\n\\nFeed-Forward Networks (FFN):\\n\\n\\n\\nEach layer of the Transformer’s encoder and decoder also contains a feed-forward neural network (FFN). This is a simple fully-connected network that is applied to each position in the sequence independently.\\n\\nThe FFN is composed of two linear transformations with a ReLU activation between them:\\n\\nThe purpose of the FFN is to introduce non-linearities and learn complex transformations for each token in the sequence.\\n\\nResidual Connections and Layer Normalization:\\n\\nTo facilitate gradient flow and improve training stability, residual connections are added around each sub-layer in both the encoder and decoder. This means the input to a sub-layer is added to its output:\\n\\nWhere Sublayer(x) represents either the multi-head attention or feed-forward sub-layer. After applying the residual connection, the result is normalized using layer normalization.\\n\\nWhy Transformer is Important:\\n\\nThe Transformer model revolutionized NLP and sequence-based tasks due to its key advantages:\\n\\nParallelization: By removing recurrence, the Transformer can process sequences in parallel, drastically reducing training times and allowing it to scale too much larger datasets.\\n\\nHandling Long-Range Dependencies: Self-attention allows the model to capture dependencies between distant tokens in a single step, making it far more effective at handling long sequences than RNNs or CNNs, which require multiple layers or recurrent steps to do so.\\n\\nScalability: The Transformer’s efficiency and ability to parallelize operations made it highly scalable, allowing researchers to train massive models like BERT and GPT on enormous datasets, leading to dramatic improvements in tasks like text generation, question answering, and language understanding.\\n\\nSimpler Architecture: The Transformer’s architecture is simpler than RNNs and CNNs, relying only on self-attention and feed-forward networks. This simplicity, combined with its effectiveness, made it a popular choice for a wide range of sequence transduction tasks.\\n\\nImpact and Legacy:')]"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["len(retrieved_docs2[0].page_content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g6kuPjKvt9WP","executionInfo":{"status":"ok","timestamp":1730196611532,"user_tz":-330,"elapsed":2,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"7ccd7910-4809-4544-cdb8-851704336602"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4880"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","source":["## **Generate Response:**"],"metadata":{"id":"yUcwm5dGwUma"}},{"cell_type":"code","source":["from operator import itemgetter\n","from langchain.prompts import ChatPromptTemplate\n","from langchain.schema.output_parser import StrOutputParser\n","from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n","from IPython.display import display, Markdown\n","\n","template = \"\"\"\n","Answer the question in these format based only on the following context:\n","{context}\n","Defination:\n","Key Points:\n","Additional Information:\n","\n","Question: {question}\n","\"\"\"\n","\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","chain = (\n","    {\"context\": retriever2, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n"],"metadata":{"id":"SgAE4ukKuNJ8","executionInfo":{"status":"ok","timestamp":1730196782957,"user_tz":-330,"elapsed":569,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["ans = chain.invoke(\"Tell me something about Transformer\")\n","display(Markdown(ans))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"iFtLcNxHwTEL","executionInfo":{"status":"ok","timestamp":1730196788525,"user_tz":-330,"elapsed":4957,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"a1e73a12-798f-4dbe-9383-a843a798c11e"},"execution_count":58,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Defination: \nThe Transformer is a neural network architecture that revolutionized natural language processing (NLP) and sequence-based tasks. It was introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017.\n\nKey Points: \n- The Transformer model is based on the concept of self-attention, which allows each token in a sequence to attend to all other tokens, making it possible to capture long-range dependencies.\n- The Transformer is highly scalable, allowing for the training of larger models that achieve superior performance on a variety of tasks.\n- The Transformer's architecture is simpler than RNNs and CNNs, relying only on self-attention and feed-forward networks.\n\nAdditional Information: \n- The Transformer model has been used to achieve state-of-the-art results on a wide range of NLP tasks, including machine translation, question answering, and text generation.\n- The Transformer model has also been used to develop new NLP models, such as BERT and GPT, which have further advanced the field of NLP."},"metadata":{}}]},{"cell_type":"code","source":["ans = chain.invoke(\"Tell me something about Self-Query Retriever\")\n","display(Markdown(ans))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":227},"id":"NoXBsknPwl3Q","executionInfo":{"status":"ok","timestamp":1730196812905,"user_tz":-330,"elapsed":4092,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"ad398ca4-bc53-4564-901e-1dba06d6aa84"},"execution_count":59,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Defination: \nA Self-Query Retriever is an advanced mechanism used in Retrieval-Augmented Generation (RAG) to improve the retrieval process by generating better and more contextually relevant queries. It transforms the original input query into a more refined or structured set of sub-queries or expansions. These refined queries are then used to search the knowledge base, ensuring more accurate retrieval of relevant documents or passages. “Self-Query retriever is works when you have metadata information in your knowledgebase or vector database”. \n\nKey Points: \n- Breaks down complex or ambiguous queries into more specific sub-queries.\n- Addresses multi-faceted information needs by targeting different aspects of the query.\n- Improves recall and precision in retrieval by generating refined sub-queries.\n\nAdditional Information: \n- Helps overcome challenges in basic RAG workflow, such as poor query representation, handling complex queries, limited recall, and unstructured queries."},"metadata":{}}]},{"cell_type":"code","source":["ans = chain.invoke(\"Tell me something about Parent Document Retriever\")\n","display(Markdown(ans))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"id":"HDSW3zOmxCD4","executionInfo":{"status":"ok","timestamp":1730196838631,"user_tz":-330,"elapsed":6679,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"22676ca1-394b-4791-a402-98f65dc47f56"},"execution_count":60,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Defination: \nThe Parent Document Retriever is a retrieval strategy used in information retrieval and RAG (Retrieval-Augmented Generation) workflows. It aims to improve the quality of retrieved information by leveraging the relationship between smaller document chunks (children) and their larger parent documents. This method ensures that the context of the information is preserved and provides more comprehensive answers to user queries.\n\nKey Points: \n- Maintains Context: By mapping child documents back to their parent documents, the approach ensures that the retrieved information retains its original context, leading to more accurate and comprehensive responses.\n- Improved Retrieval Accuracy: Parent Document Retrieval reduces the chances of providing isolated or out-of-context information by aggregating related content from the same source.\n- Better Coverage of Multi-Part Queries: When a query requires multiple pieces of information, this approach ensures that various relevant sections from the same parent document can be retrieved together.\n\nAdditional Information: \n- When to Use Parent Document Retrieval:\n  - Long Documents or Structured Content: It is beneficial when dealing with long documents (e.g., books, research papers, manuals) that are broken down into smaller sections. Parent Document Retrieval ensures the relevant sections are tied back to the full document.\n  - Preserving Context: It is useful when the context of the information is important, such as legal documents or technical reports, where individual sections may not make sense without the larger context.\n  - Multi-Step Information Retrieval: Suitable for use cases where the retrieval needs to be refined from coarse-grained (parent level) to fine-grained (child level), or vice versa."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"2gXr-j68xHua"},"execution_count":null,"outputs":[]}]}