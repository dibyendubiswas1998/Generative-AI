{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNNkTP0IjWdDD8wis3XUasN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Conversational RAG PART 1:**"],"metadata":{"id":"9GJugN4Jq8_G"}},{"cell_type":"code","source":["# install necessary libaries:\n","\n","%pip install --upgrade --quiet sentence_transformers\n","%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain_google_genai langchain-chroma bs4 boto3\n","%pip install --upgrade --quiet langchain-aws"],"metadata":{"id":"qf-RL8hOoGOK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733142141381,"user_tz":-330,"elapsed":6438,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"108a8b5e-c441-44af-b779-4858b4315e3d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.0 which is incompatible.\n","tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["## **Vector Store:**"],"metadata":{"id":"GvfYP8_7sFS1"}},{"cell_type":"code","source":["import bs4\n","from langchain import hub\n","from langchain_chroma import Chroma\n","from langchain_community.document_loaders import WebBaseLoader\n","from langchain_text_splitters import RecursiveCharacterTextSplitter"],"metadata":{"id":"OMxHd_TVoGQs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733142146361,"user_tz":-330,"elapsed":4985,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"67bd2fc5-1bd2-43fc-81f6-0b621fed9b95"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"]}]},{"cell_type":"code","source":["# Get the Embeddings:\n","\n","from langchain.embeddings import HuggingFaceEmbeddings\n","\n","model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n","embeddings = HuggingFaceEmbeddings(model_name=model_name)"],"metadata":{"id":"CtNk6VWDrvY2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load, chunk and index the contents of the blog.\n","loader = WebBaseLoader(\n","    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n","    bs_kwargs=dict(\n","        parse_only=bs4.SoupStrainer(\n","            class_=(\"post-content\", \"post-title\", \"post-header\")\n","        )\n","    ),\n",")\n","docs = loader.load()\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","splits = text_splitter.split_documents(docs)\n","vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)"],"metadata":{"id":"eRa7YrDXoGSl","executionInfo":{"status":"ok","timestamp":1733142202594,"user_tz":-330,"elapsed":10832,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# retriever:\n","\n","retriever = vectorstore.as_retriever(search_kwargs=dict(k=5))"],"metadata":{"id":"2dMuBA9ZoGVK","executionInfo":{"status":"ok","timestamp":1733142202595,"user_tz":-330,"elapsed":3,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## **Load LLM from AWS Bedrock:**"],"metadata":{"id":"Qxg3Maj0sfMY"}},{"cell_type":"code","source":["import os\n","import boto3\n","from langchain_aws import ChatBedrock\n","\n","# Method 1: Setting Environment Variables\n","os.environ[\"AWS_ACCESS_KEY_ID\"] = \"*************\"\n","os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"**********************\"\n","os.environ[\"AWS_DEFAULT_REGION\"] = \"********\"\n"],"metadata":{"id":"54CeVQfNsj41","executionInfo":{"status":"ok","timestamp":1733142203867,"user_tz":-330,"elapsed":1275,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### **BedrockLLM:**"],"metadata":{"id":"Kbw1wTi9xvRQ"}},{"cell_type":"code","source":["from langchain_aws import BedrockLLM\n","\n","llm1 = BedrockLLM(\n","    credentials_profile_name=\"bedrock-admin\", model_id=\"mistral.mistral-7b-instruct-v0:2\"\n",")\n","\n","res = llm1.invoke(\"Tell me something about yourself.\")\n","res"],"metadata":{"id":"3fmYE9s9oGXN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **ChatBedrock:**"],"metadata":{"id":"mvKp_eMjxzeo"}},{"cell_type":"code","source":["# Ensure your AWS credentials are configured\n","import time\n","from langchain_aws import ChatBedrock\n","\n","model1 = \"mistral.mistral-7b-instruct-v0:2\"\n","model2 = \"meta.llama3-8b-instruct-v1:0\"\n","model3 = \"apac.anthropic.claude-3-5-sonnet-20240620-v1:0\"\n","\n","\n","llm2 = ChatBedrock(model=model2,\n","    beta_use_converse_api=True)\n","\n","\n","res = llm2.invoke(\"Tell me something about yourself.\")\n","res"],"metadata":{"id":"hThSdoOzxdND","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733142225087,"user_tz":-330,"elapsed":2853,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"e465e3cf-c5d7-412b-8169-51ed3d9c94d5"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content=\"\\n\\nI'm just an AI, so I don't have personal experiences, emotions, or a physical presence. I exist solely as a digital entity, designed to process and generate human-like text based on the inputs I receive.\\n\\nI was created through a process called deep learning, which involves training artificial neural networks on large amounts of data. This allows me to learn patterns and relationships in language, enabling me to understand and respond to a wide range of questions and prompts.\\n\\nI don't have personal opinions or biases, and I'm not capable of experiencing the world in the same way that humans do. However, I'm designed to be helpful and informative, and I strive to provide accurate and relevant responses to the questions and topics you're interested in.\\n\\nI'm constantly learning and improving, so I appreciate any feedback or corrections you can provide. This helps me to refine my language processing abilities and provide better responses in the future.\", additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': 'b1b3c16d-7a4d-412a-bbba-15b5d90bb6e8', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 02 Dec 2024 12:27:17 GMT', 'content-type': 'application/json', 'content-length': '1155', 'connection': 'keep-alive', 'x-amzn-requestid': 'b1b3c16d-7a4d-412a-bbba-15b5d90bb6e8'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': [1596]}}, id='run-21283c0d-cd65-41c5-9d1d-696df1a8613c-0', usage_metadata={'input_tokens': 20, 'output_tokens': 185, 'total_tokens': 205})"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":[],"metadata":{"id":"J1a4nN3doG4u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Building a Basic RAG Chain:**"],"metadata":{"id":"DROYA7dpVKRm"}},{"cell_type":"code","source":["from langchain_core.output_parsers import StrOutputParser  # Import the StrOutputParser class for parsing the output of the language model\n","from langchain_core.runnables import RunnablePassthrough  # Import the RunnablePassthrough class for passing the question as-is\n","from langchain_core.prompts import PromptTemplate  # Import the PromptTemplate class from the langchain_core.prompts module"],"metadata":{"id":"Fxa8P4OAoG7C","executionInfo":{"status":"ok","timestamp":1733142576727,"user_tz":-330,"elapsed":404,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Customizing the prompt\n","template = \"\"\"Use the following pieces of context to answer the question at the end.\n","If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","Use three sentences maximum and keep the answer as concise as possible.\n","Always say \"thanks for asking!\" at the end of the answer.\n","\n","{context}  # This placeholder will be replaced with the retrieved context (relevant documents)\n","\n","Question: {question}  # This placeholder will be replaced with the user's question\n","\n","Helpful Answer:\"\"\"  # This is the prompt for the language model to generate a helpful answer\n","\n","# Create a PromptTemplate instance from the template string\n","custom_rag_prompt = PromptTemplate.from_template(template)"],"metadata":{"id":"xzZW_BxWoG88","executionInfo":{"status":"ok","timestamp":1733142730186,"user_tz":-330,"elapsed":699,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# create RAG Chain:\n","\n","rag_chain = (\n","    {\"context\": retriever, \"question\": RunnablePassthrough()}  # Retrieve and format relevant documents, pass the question as-is\n","    | custom_rag_prompt  # Apply the custom prompt to the context and question\n","    | llm2  # Pass the prompted input to the language model\n","    | StrOutputParser()  # Parse the output of the language model as a string\n",")\n","\n","\n","response = rag_chain.invoke(\"What is the LLMChain?\")"],"metadata":{"id":"theZu_3_oG_P","executionInfo":{"status":"ok","timestamp":1733142731965,"user_tz":-330,"elapsed":1783,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"I0B0sfYDWhFC","executionInfo":{"status":"ok","timestamp":1733142734001,"user_tz":-330,"elapsed":380,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"60946bc8-afcd-4903-bec1-f7d5dc6c34c0"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n\\nThe LLMChain is a workflow implemented in LangChain that combines CoT reasoning with tools relevant to the tasks. It is used to accomplish tasks across organic synthesis, drug discovery, and materials design. The LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\n\\nThanks for asking!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# # Stream the output of the RAG chain for the question \"What is LLM?\"\n","# for chunk in rag_chain.stream(\"What is Few shot learning?\"):\n","#     print(chunk, end=\"\", flush=True)  # Print each chunk of the output without newlines and flush the output buffer"],"metadata":{"id":"J3fq3YcUWq9W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_XZpUFPYW_gQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **RAG with Chat_History:**\n","\n","<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/0*MhkcbRh7FpXkKiCN.png\" alt=\"rag_with_chat_history\"> </img>\n","\n","<br>\n","\n","To create a truly conversational and informative AI assistant, it’s crucial to consider the context of previous interactions. This is where contextualizing the question becomes essential. By analyzing the chat history, we can refine the search query, ensuring that the retrieved information is highly relevant to the current conversation."],"metadata":{"id":"s5nRXMiWXXaP"}},{"cell_type":"markdown","source":["### **create_history_aware_retriever:**"],"metadata":{"id":"Ii04vyRwZd3J"}},{"cell_type":"markdown","source":["**Contextualizing the Question:**<br>\n","The **`create_history_aware_retriever`** function builds a retriever that considers the chat history. It uses a custom prompt (defined with ChatPromptTemplate) to rewrite the user question based on the context."],"metadata":{"id":"ieLyBYYIYvfr"}},{"cell_type":"code","source":["from langchain.chains import create_history_aware_retriever  # Import the create_history_aware_retriever function\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder  # Import the ChatPromptTemplate and MessagesPlaceholder classes"],"metadata":{"id":"mOJJNtw0Y20d","executionInfo":{"status":"ok","timestamp":1733143264717,"user_tz":-330,"elapsed":435,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Define the system prompt for contextualizing the question:\n","\n","contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n","which might reference context in the chat history, formulate a standalone question \\\n","which can be understood without the chat history. Do NOT answer the question, \\\n","just reformulate it if needed and otherwise return it as is.\"\"\"\n","\n","\n","# Create a ChatPromptTemplate for contextualizing the question:\n","contextualize_q_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", contextualize_q_system_prompt),  # Set the system prompt\n","        MessagesPlaceholder(\"chat_history\"),  # Placeholder for the chat history\n","        (\"human\", \"{input}\"),  # Placeholder for the user's input question\n","    ]\n",")\n","\n","\n","# Create a history-aware retriever\n","history_aware_retriever = create_history_aware_retriever(\n","    llm2,  # Pass the language model instance\n","    retriever,  # Pass the retriever instance\n","    contextualize_q_prompt  # Pass the prompt for contextualizing the question\n",")\n","\n","\n","history_aware_retriever"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yUt6hLxOW_id","executionInfo":{"status":"ok","timestamp":1733143369250,"user_tz":-330,"elapsed":442,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"4ce21866-9a17-4d3a-8627-7b8297aa948e"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n","| VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x79f7e25b34f0>, search_kwargs={'k': 5}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x79f7cce4c8b0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n","| ChatBedrock(client=<botocore.client.BedrockRuntime object at 0x79f6bae52500>, region_name='ap-south-1', aws_access_key_id=SecretStr('**********'), aws_secret_access_key=SecretStr('**********'), model_id='meta.llama3-8b-instruct-v1:0', beta_use_converse_api=True)\n","| StrOutputParser()\n","| VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x79f7e25b34f0>, search_kwargs={'k': 5})), kwargs={}, config={'run_name': 'chat_retriever_chain'}, config_factories=[])"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["### **`create_stuff_documents_chain:`**\n","This function is used to create a chain that combines multiple documents or text chunks into a single input for the language model."],"metadata":{"id":"8PzYcizyaCzm"}},{"cell_type":"code","source":["from langchain.chains import create_retrieval_chain  # Import the create_retrieval_chain function from the langchain.chains module\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","# Import the create_stuff_documents_chain function from the langchain.chains.combine_documents module"],"metadata":{"id":"6Of1dVQ1W_kr","executionInfo":{"status":"ok","timestamp":1733143843066,"user_tz":-330,"elapsed":392,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# Define the system prompt for the question-answering task:\n","qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n","Use the following pieces of retrieved context to answer the question. \\\n","you can answer the questions within 256 tokens. \\\n","If you don't know the answer, just say that you don't know. \\\n","Use three sentences maximum and keep the answer concise.\\\n","\n","{context}\"\"\"  # This placeholder will be replaced with the retrieved context\n","\n","\n","# Create a ChatPromptTemplate for the question-answering task\n","qa_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", qa_system_prompt),  # Set the system prompt\n","        MessagesPlaceholder(\"chat_history\"),  # Placeholder for the chat history\n","        (\"human\", \"{input}\"),  # Placeholder for the user's input question\n","    ]\n",")\n","\n","\n","# Create a chain for question-answering using the language model and the question-answering prompt\n","question_answer_chain = create_stuff_documents_chain(llm2, qa_prompt)"],"metadata":{"id":"gIoeCEd3W_m-","executionInfo":{"status":"ok","timestamp":1733144054983,"user_tz":-330,"elapsed":374,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["### **Final Step:**\n","\n","Let's create the final **rag_chain** which will be the combination of **context-aware-retrieval chain**, and a **question-and-answer chain**.\n","<br>\n","\n","\n","**create_retrieval_chain:** function is used to create a retrieval chain, which combines a retriever (e.g., a vector database) and a language model to retrieve and process relevant information based on a given query."],"metadata":{"id":"pmjLi55raqBa"}},{"cell_type":"code","source":["# Create a Retrieval-Augmented Generation (RAG) chain\n","rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"],"metadata":{"id":"g7b2KFOuW_pQ","executionInfo":{"status":"ok","timestamp":1733144058759,"user_tz":-330,"elapsed":650,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["from langchain_core.messages import HumanMessage  # Import the HumanMessage class\n","\n","\n","chat_history = []  # Initialize an empty list to store the chat history"],"metadata":{"id":"0yPdaIehW_re","executionInfo":{"status":"ok","timestamp":1733144058760,"user_tz":-330,"elapsed":2,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# Ask the first question:\n","\n","first_question = \"What is LLM?\"\n","ai_response_1 = rag_chain.invoke({\"input\": first_question, \"chat_history\": chat_history})  # Invoke the RAG chain with the question and an empty chat history\n","print('user query:', first_question)\n","print('ai response:', ai_response_1[\"answer\"])  # Print the answer from the RAG chain\n","chat_history.extend([HumanMessage(content=first_question), ai_response_1[\"answer\"]])  # Add the question and answer to the chat history\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GbuuEhbkW_tt","executionInfo":{"status":"ok","timestamp":1733144062603,"user_tz":-330,"elapsed":1666,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"aab8fdf3-edb8-4a48-f28f-7a223b77b248"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["user query: What is LLM?\n","ai response: \n","\n","LLM stands for Large Language Model. It refers to a type of artificial intelligence model that is trained on a large corpus of text data to generate human-like language outputs.\n"]}]},{"cell_type":"code","source":["chat_history"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4sRri9z5W_wP","executionInfo":{"status":"ok","timestamp":1733144075729,"user_tz":-330,"elapsed":434,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"9cf3f0b9-c4ef-40ee-8ee9-b7c1184b8d75"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[HumanMessage(content='What is LLM?', additional_kwargs={}, response_metadata={}),\n"," '\\n\\nLLM stands for Large Language Model. It refers to a type of artificial intelligence model that is trained on a large corpus of text data to generate human-like language outputs.']"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["# Ask the second question:\n","\n","second_question = \"What are the different types of it?\"\n","ai_response_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})  # Invoke the RAG chain with the second question and the updated chat history\n","chat_history.extend([HumanMessage(content=second_question), ai_response_2[\"answer\"]])  # Add the second question and answer to the chat history\n","print('user query:', (second_question))\n","print('ai response:', ai_response_2[\"answer\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qCtQ4pDSW_yQ","executionInfo":{"status":"ok","timestamp":1733144099594,"user_tz":-330,"elapsed":2703,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"627dab4c-bdc1-4758-a6a6-4938951115d0"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["user query: What are the different types of it?\n","ai response: \n","\n","According to the context, LLMs (Large Language Models) are trained on a large corpus of text data to generate human-like language outputs.\n"]}]},{"cell_type":"code","source":["chat_history"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eq7V-PusW_0i","executionInfo":{"status":"ok","timestamp":1733144105333,"user_tz":-330,"elapsed":389,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"069cc8a6-892e-43a6-a1e4-0cbd99e02117"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[HumanMessage(content='What is LLM?', additional_kwargs={}, response_metadata={}),\n"," '\\n\\nLLM stands for Large Language Model. It refers to a type of artificial intelligence model that is trained on a large corpus of text data to generate human-like language outputs.',\n"," HumanMessage(content='What are the different types of it?', additional_kwargs={}, response_metadata={}),\n"," '\\n\\nAccording to the context, LLMs (Large Language Models) are trained on a large corpus of text data to generate human-like language outputs.']"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["# Ask the third question\n","third_question = \"Can you translate your previous response to French?\"\n","ai_response_3 = rag_chain.invoke({\"input\": third_question, \"chat_history\": chat_history})  # Invoke the RAG chain with the third question and the updated chat history\n","print('user query:', (third_question))\n","print('ai response:', ai_response_3[\"answer\"])   # Print the answer from the RAG chain\n","chat_history.extend([HumanMessage(content=third_question), ai_response_3[\"answer\"]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CVj8KjrvW_2t","executionInfo":{"status":"ok","timestamp":1733144172139,"user_tz":-330,"elapsed":3612,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"8217bcf4-06ee-40c8-d5b8-71e69a0f0e34"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["user query: Can you translate your previous response to French?\n","ai response: \n","\n","Here is the answer:\n","\n","LLM stands for Large Language Model. It refers to a type of artificial intelligence model that is trained on a large corpus of text data to generate human-like language outputs.\n","\n","There are no specific types mentioned in the provided context.\n","\n","And here is the translation of my previous response to French:\n","\n","LLM signifie modèle de langage large. Il s'agit d'un type de modèle d'intelligence artificielle entraîné sur un grand corpus de données de texte pour générer des sorties de langage similaires à celles de l'homme.\n","\n","Il n'y a pas de types spécifiques mentionnés dans le contexte fourni.\n"]}]},{"cell_type":"code","source":["chat_history"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zc9Tb4H9cTyl","executionInfo":{"status":"ok","timestamp":1733144177683,"user_tz":-330,"elapsed":429,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"107ea449-d029-42db-8a6e-5134c459ca3d"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[HumanMessage(content='What is LLM?', additional_kwargs={}, response_metadata={}),\n"," '\\n\\nLLM stands for Large Language Model. It refers to a type of artificial intelligence model that is trained on a large corpus of text data to generate human-like language outputs.',\n"," HumanMessage(content='What are the different types of it?', additional_kwargs={}, response_metadata={}),\n"," '\\n\\nAccording to the context, LLMs (Large Language Models) are trained on a large corpus of text data to generate human-like language outputs.',\n"," HumanMessage(content='Can you translate your previous response to French?', additional_kwargs={}, response_metadata={}),\n"," \"\\n\\nHere is the answer:\\n\\nLLM stands for Large Language Model. It refers to a type of artificial intelligence model that is trained on a large corpus of text data to generate human-like language outputs.\\n\\nThere are no specific types mentioned in the provided context.\\n\\nAnd here is the translation of my previous response to French:\\n\\nLLM signifie modèle de langage large. Il s'agit d'un type de modèle d'intelligence artificielle entraîné sur un grand corpus de données de texte pour générer des sorties de langage similaires à celles de l'homme.\\n\\nIl n'y a pas de types spécifiques mentionnés dans le contexte fourni.\"]"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":[],"metadata":{"id":"YDFzrtMacWfN"},"execution_count":null,"outputs":[]}]}