{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPDsGRCNkNZzBQa8gq4Ktzw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Conversational RAG PART 2:**"],"metadata":{"id":"e763-uJsetAb"}},{"cell_type":"code","source":["# install necessary libaries:\n","\n","%pip install --upgrade --quiet sentence_transformers\n","%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-google-genai langchain-chroma bs4 boto3\n","%pip install --upgrade --quiet langchain-aws"],"metadata":{"id":"BNFDvmiQdsJp","executionInfo":{"status":"ok","timestamp":1733147002938,"user_tz":-330,"elapsed":16855,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# Load the Tokens:\n","\n","from google.colab import userdata\n","import os\n","\n","GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n","HF_TOKEN = userdata.get('HF_TOKEN')\n","\n","os.environ['GOOGLE_API_KEY'] = GEMINI_API_KEY\n","os.environ['HF_TOKEN'] = HF_TOKEN"],"metadata":{"id":"Url-f98ndsMm","executionInfo":{"status":"ok","timestamp":1733147142141,"user_tz":-330,"elapsed":3538,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["## **Load Model/LLM:**"],"metadata":{"id":"4omCYx1OhKpd"}},{"cell_type":"code","source":["from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","model = ChatGoogleGenerativeAI(\n","    model=\"gemini-1.5-pro\",\n","    temperature=0.4,\n","    max_tokens=512,\n","    timeout=None,\n","    max_retries=2,\n","    # other params...\n",")\n","\n","print(model.invoke(\"hi\").content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bA1y6kMgdsPT","executionInfo":{"status":"ok","timestamp":1733147142589,"user_tz":-330,"elapsed":454,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"8f832bc2-d764-4248-8003-34a0b419898a"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Hi there! How can I help you today?\n","\n"]}]},{"cell_type":"markdown","source":["## **Load Embeddings:**"],"metadata":{"id":"ESYhIpg2hXyV"}},{"cell_type":"code","source":["# Get the Embeddings:\n","\n","from langchain.embeddings import HuggingFaceEmbeddings\n","\n","model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n","embeddings = HuggingFaceEmbeddings(model_name=model_name)"],"metadata":{"id":"T1Qj-Qe4dsTh","executionInfo":{"status":"ok","timestamp":1733147146645,"user_tz":-330,"elapsed":1764,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["## **Vector Store:**"],"metadata":{"id":"TPPcFWDZhqw5"}},{"cell_type":"code","source":["import bs4\n","from langchain import hub\n","from langchain_chroma import Chroma\n","from langchain_community.document_loaders import WebBaseLoader\n","from langchain_text_splitters import RecursiveCharacterTextSplitter"],"metadata":{"id":"LwaEalEndsWC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load, chunk and index the contents of the blog.\n","loader = WebBaseLoader(\n","    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n","    bs_kwargs=dict(\n","        parse_only=bs4.SoupStrainer(\n","            class_=(\"post-content\", \"post-title\", \"post-header\")\n","        )\n","    ),\n",")\n","docs = loader.load()\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","splits = text_splitter.split_documents(docs)\n","vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)"],"metadata":{"id":"_bd3Vr_qdsYh","executionInfo":{"status":"ok","timestamp":1733147162730,"user_tz":-330,"elapsed":12251,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["# retriever:\n","\n","retriever = vectorstore.as_retriever(search_kwargs=dict(k=5))"],"metadata":{"id":"UJrrNUKVdsa6","executionInfo":{"status":"ok","timestamp":1733147162731,"user_tz":-330,"elapsed":11,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["retriever.get_relevant_documents(query=\"What is LLM?\")"],"metadata":{"id":"qsB8uU3tdsdY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Basic RAG Work Flow:**"],"metadata":{"id":"uwoO0Xr2iIbr"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate"],"metadata":{"id":"hvWvZVmqdsfs","executionInfo":{"status":"ok","timestamp":1733147166774,"user_tz":-330,"elapsed":426,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# Define System Prompt:\n","\n","system_prompt = (\n","    \"You are an assistant for question-answering tasks. \"\n","    \"Use the following pieces of retrieved context to answer the question \"\n","    \"If you don't know the answer, say that you don't know.\"\n","    \"Use three sentences maximum and keep the answer concise.\"\n","    \"\\n\\n\"\n","    \"{context}\"\n",")\n","\n","chat_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", system_prompt),\n","        (\"human\", \"{input}\"),\n","    ]\n",")"],"metadata":{"id":"YyGtHc4ZdsiK","executionInfo":{"status":"ok","timestamp":1733147167216,"user_tz":-330,"elapsed":3,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["from langchain.chains.combine_documents import create_stuff_documents_chain\n","from langchain.chains import create_retrieval_chain"],"metadata":{"id":"FfM0VUkcdski","executionInfo":{"status":"ok","timestamp":1733147236698,"user_tz":-330,"elapsed":411,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["# Question-Answering Chains\n","question_answering_chain=create_stuff_documents_chain(model, chat_prompt)\n","\n","# Retriever Chain:\n","rag_chain = create_retrieval_chain(retriever, question_answering_chain)"],"metadata":{"id":"ilTMFwwNdsnG","executionInfo":{"status":"ok","timestamp":1733147252347,"user_tz":-330,"elapsed":424,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["response = rag_chain.invoke({\"input\":\"what is MRKL?\"})\n","response[\"answer\"]"],"metadata":{"id":"PxPz1avndspw","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1733147270724,"user_tz":-330,"elapsed":2407,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"fcd86d6e-230a-47bc-a5fc-3fb0de052fe2"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'MRKL (Modular Reasoning, Knowledge and Language) is a neuro-symbolic architecture for autonomous agents.  It uses a collection of expert modules, and a large language model (LLM) acts as a router to direct inquiries to the appropriate module.  These modules can be neural networks or symbolic systems like calculators or APIs.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":[],"metadata":{"id":"jLjJIx0jdssT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **RAG Based Chatbot With Memory(Chat History):**"],"metadata":{"id":"itAEhD-0pY1-"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain.chains import create_history_aware_retriever\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","from langchain.chains import create_retrieval_chain"],"metadata":{"id":"CPnzRVM-dsu7","executionInfo":{"status":"ok","timestamp":1733148120485,"user_tz":-330,"elapsed":415,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":["### **Step 1: Create History-Aware-Retriever**"],"metadata":{"id":"OulTSRNDrCnm"}},{"cell_type":"code","source":["# Create History Aware Retriever:\n","\n","retriever_prompt = (\n","    \"Given a chat history and the latest user question which might reference context in the chat history,\"\n","    \"formulate a standalone question which can be understood without the chat history.\"\n","    \"Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",")\n","\n","\n","contextualize_q_prompt  = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", retriever_prompt),\n","        MessagesPlaceholder(variable_name=\"chat_history\"),\n","        (\"human\", \"{input}\"),\n","\n","\n","     ]\n",")\n","\n","history_aware_retriever = create_history_aware_retriever(model, retriever, contextualize_q_prompt)"],"metadata":{"id":"ljo2vEqtdsxo","executionInfo":{"status":"ok","timestamp":1733148146236,"user_tz":-330,"elapsed":401,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":["### **Step 2: Define the Custom System Prompts and Create Question-Aware-Chain**"],"metadata":{"id":"QPPTDNhnrk-W"}},{"cell_type":"code","source":["# Define the Custom System Prompts and Create Question-Aware-Chain:\n","\n","system_prompt = (\n","    \"You are an assistant for question-answering tasks. \"\n","    \"Use the following pieces of retrieved context to answer the question \"\n","    \"If you don't know the answer, say that you don't know.\"\n","    \"Use three sentences maximum and keep the answer concise.\"\n","    \"\\n\\n\"\n","    \"{context}\"\n",")\n","\n","qa_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", system_prompt),\n","        MessagesPlaceholder(variable_name=\"chat_history\"),\n","        (\"human\", \"{input}\"),\n","    ]\n",")\n","\n","\n","question_answer_chain = create_stuff_documents_chain(model, qa_prompt)"],"metadata":{"id":"b0YljeLDds0G","executionInfo":{"status":"ok","timestamp":1733148270568,"user_tz":-330,"elapsed":418,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":["### **Step 3: Create Rag-Chain using history_aware_retriever and question_answer_chain**"],"metadata":{"id":"GkuzS7AFsDoo"}},{"cell_type":"code","source":["# Create Rag-Chain using history_aware_retriever and question_answer_chain:\n","\n","rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"],"metadata":{"id":"M53FAxCpds25","executionInfo":{"status":"ok","timestamp":1733148391805,"user_tz":-330,"elapsed":438,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":["### **Step 4: Generate Response and Manually add the Conversation (Human & AI) to chat_history:**"],"metadata":{"id":"vJdUARP_shKp"}},{"cell_type":"code","source":["from langchain_core.messages import HumanMessage, AIMessage\n","\n","# Define or Initialized the chat_hostory\n","chat_history = []"],"metadata":{"id":"srONsG-ods5W","executionInfo":{"status":"ok","timestamp":1733148448366,"user_tz":-330,"elapsed":432,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["# Generation 01:\n","question = \"what is Task Decomposition?\"\n","response = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n","\n","print(\"Question:\\n\", question)\n","print(\"Answer:\\n\", response[\"answer\"])\n","\n","\n","\n","# Add to chat_hostory (question and response)\n","chat_history.extend(\n","    [\n","        HumanMessage(content=question),\n","        AIMessage(content=response[\"answer\"]),\n","    ]\n",")"],"metadata":{"id":"yQ2zNwkFds74","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733148733228,"user_tz":-330,"elapsed":2211,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"e3758843-8a0a-4905-a4eb-b8c438677b89"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Question:\n"," what is Task Decomposition?\n","Answer:\n"," Task decomposition is a technique used to break down complex tasks into smaller, more manageable steps.  Chain of thought (CoT) prompting encourages this decomposition by instructing the model to \"think step by step\".  This approach improves model performance and offers insights into the model's reasoning process.\n","\n"]}]},{"cell_type":"code","source":["chat_history"],"metadata":{"id":"-3A0DeeRds-j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733148751541,"user_tz":-330,"elapsed":2,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"cb7f70b5-44a4-4d94-a9bf-a12589e1b2f8"},"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[HumanMessage(content='what is Task Decomposition?', additional_kwargs={}, response_metadata={}),\n"," AIMessage(content='Task decomposition is a technique used to break down complex tasks into smaller, more manageable steps.  Chain of thought (CoT) prompting encourages this decomposition by instructing the model to \"think step by step\".  This approach improves model performance and offers insights into the model\\'s reasoning process.\\n', additional_kwargs={}, response_metadata={})]"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["# Generation 02:\n","question = \"What are common ways of doing it?\"\n","response = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n","\n","print(\"Question:\\n\",question)\n","print(\"Answer:\\n\",response[\"answer\"])\n","\n","\n","\n","# Add to chat_hostory (question and response)\n","chat_history.extend(\n","    [\n","        HumanMessage(content=question),\n","        AIMessage(content=response[\"answer\"]),\n","    ]\n",")"],"metadata":{"id":"IQpGXliedtBJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733148824508,"user_tz":-330,"elapsed":3227,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"84fcd591-75bd-401c-9e42-a47ca10bd183"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Question:\n"," What are common ways of doing it?\n","Answer:\n"," Task decomposition can be achieved by prompting LLMs with simple instructions to list steps or subgoals, using task-specific instructions (e.g., \"Write a story outline\"), or through direct human input.  Tree of Thoughts further extends this by exploring multiple reasoning possibilities at each step, creating a tree of potential solutions.\n","\n"]}]},{"cell_type":"code","source":["chat_history"],"metadata":{"id":"lNTnK3aVdtDu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733148866390,"user_tz":-330,"elapsed":425,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"6b862d90-6c5b-4f66-c4fb-ef2968e98cf9"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[HumanMessage(content='what is Task Decomposition?', additional_kwargs={}, response_metadata={}),\n"," AIMessage(content='Task decomposition is a technique used to break down complex tasks into smaller, more manageable steps.  Chain of thought (CoT) prompting encourages this decomposition by instructing the model to \"think step by step\".  This approach improves model performance and offers insights into the model\\'s reasoning process.\\n', additional_kwargs={}, response_metadata={}),\n"," HumanMessage(content='What are common ways of doing it?', additional_kwargs={}, response_metadata={}),\n"," AIMessage(content='Task decomposition can be achieved by prompting LLMs with simple instructions to list steps or subgoals, using task-specific instructions (e.g., \"Write a story outline\"), or through direct human input.  Tree of Thoughts further extends this by exploring multiple reasoning possibilities at each step, creating a tree of potential solutions.\\n', additional_kwargs={}, response_metadata={})]"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":[" ### **Note:**\n","\n"," Here we pass the **`chat_history`** manually, means manually pass/add the chat history (**HumanMessages** & **AIMessages**).<br>\n","\n"," <u>This is not a correct approach, so LangChain provides some modules to do automated way.</u>"],"metadata":{"id":"VkW4QUeDuVIs"}},{"cell_type":"code","source":[],"metadata":{"id":"cd5-Q74WdtGL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7Gbm6FxKdtI3"},"execution_count":null,"outputs":[]}]}