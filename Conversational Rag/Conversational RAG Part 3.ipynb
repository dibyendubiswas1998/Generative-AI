{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPDZLbCcQ5LoSkFWCC9jcpW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Conversational RAG Part 3:**\n","\n","Here we use,\n","\n","* BaseChatMessageHistory,\n","* InMemoryChatMessageHistory,\n","* RunnableWithMessageHistory,\n","* trim_messages"],"metadata":{"id":"0EY2Ng2_8pDz"}},{"cell_type":"code","source":["# install necessary libaries:\n","\n","%pip install --upgrade --quiet sentence_transformers\n","%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-google-genai langchain-chroma bs4 boto3\n","%pip install --upgrade --quiet langchain-aws"],"metadata":{"id":"rq-aaLlC8kle","executionInfo":{"status":"ok","timestamp":1733303991670,"user_tz":-330,"elapsed":3,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Load the Tokens:\n","\n","from google.colab import userdata\n","import os\n","\n","os.environ['GOOGLE_API_KEY'] = userdata.get('GEMINI_API_KEY')\n","os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"],"metadata":{"id":"2iiaTqe3-SW3","executionInfo":{"status":"ok","timestamp":1733305186614,"user_tz":-330,"elapsed":5877,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["## **Load LLM:**"],"metadata":{"id":"6-LuA6tm-Aoo"}},{"cell_type":"code","source":["from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","model = ChatGoogleGenerativeAI(\n","    model=\"gemini-1.5-pro\",\n","    temperature=0.4,\n","    max_tokens=512,\n","    timeout=None,\n","    max_retries=2,\n","    # other params...\n",")\n","\n","print(model.invoke(\"hi\").content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mUi-u0ey8kn2","executionInfo":{"status":"ok","timestamp":1733305188126,"user_tz":-330,"elapsed":1517,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"36856faf-310c-495a-e0b1-c7352327bc74"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Hi there! How can I help you today?\n","\n"]}]},{"cell_type":"markdown","source":["## **Load Embeddings from HF:**"],"metadata":{"id":"p_rtnS1B_XqB"}},{"cell_type":"code","source":["# Get the Embeddings:\n","\n","from langchain.embeddings import HuggingFaceEmbeddings\n","\n","model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n","embeddings = HuggingFaceEmbeddings(model_name=model_name)"],"metadata":{"id":"EsYqTL-X8kqm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Vector Store:**"],"metadata":{"id":"LBuXSBze_gsy"}},{"cell_type":"code","source":["import bs4\n","from langchain import hub\n","from langchain_chroma import Chroma\n","from langchain_community.document_loaders import WebBaseLoader\n","from langchain_text_splitters import RecursiveCharacterTextSplitter"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RERObcXt8ktP","executionInfo":{"status":"ok","timestamp":1733304426732,"user_tz":-330,"elapsed":2123,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"54abd653-185d-42b8-fad0-9ff305037aa4"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"]}]},{"cell_type":"code","source":["# Load, chunk and index the contents of the blog.\n","loader = WebBaseLoader(\n","    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n","    bs_kwargs=dict(\n","        parse_only=bs4.SoupStrainer(\n","            class_=(\"post-content\", \"post-title\", \"post-header\")\n","        )\n","    ),\n",")\n","docs = loader.load()\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","splits = text_splitter.split_documents(docs)\n","vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)"],"metadata":{"id":"hYhc-VOf8kvm","executionInfo":{"status":"ok","timestamp":1733304439529,"user_tz":-330,"elapsed":10228,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# retriever:\n","\n","retriever = vectorstore.as_retriever(search_kwargs=dict(k=5))"],"metadata":{"id":"7ThimKSL8kx2","executionInfo":{"status":"ok","timestamp":1733304442161,"user_tz":-330,"elapsed":487,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["retriever.get_relevant_documents(query=\"What is LLM?\")"],"metadata":{"id":"0Qsx-dA58kz0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Build Conversational AI Bot:**"],"metadata":{"id":"6pLS2Bxk_50V"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain.chains import create_history_aware_retriever\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","from langchain.chains import create_retrieval_chain\n","from langchain_core.messages import HumanMessage, AIMessage"],"metadata":{"id":"hKez-hfe8k2X","executionInfo":{"status":"ok","timestamp":1733306082574,"user_tz":-330,"elapsed":556,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":["### **Step 1: Create History-Aware-Retriever**"],"metadata":{"id":"DqRmCYN6E87Z"}},{"cell_type":"code","source":["# Create History Aware Retriever:\n","\n","retriever_prompt = (\n","    \"Given a chat history and the latest user question which might reference context in the chat history,\"\n","    \"formulate a standalone question which can be understood without the chat history.\"\n","    \"Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",")\n","\n","\n","contextualize_q_prompt  = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", retriever_prompt),\n","        MessagesPlaceholder(variable_name=\"chat_history\"),\n","        (\"human\", \"{input}\"),\n","\n","\n","     ]\n",")\n","\n","history_aware_retriever = create_history_aware_retriever(model, retriever, contextualize_q_prompt)"],"metadata":{"id":"eZ94R70Y8k4v","executionInfo":{"status":"ok","timestamp":1733305192990,"user_tz":-330,"elapsed":377,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["### **Step 2: Define the Custom System Prompts and Create Question-Aware-Chain**"],"metadata":{"id":"U4EiWaoVE_3i"}},{"cell_type":"code","source":["# Define the Custom System Prompts and Create Question-Aware-Chain:\n","\n","system_prompt = (\n","    \"You are an assistant for question-answering tasks. \"\n","    \"Use the following pieces of retrieved context to answer the question \"\n","    \"If you don't know the answer, say that you don't know.\"\n","    \"Use three sentences maximum and keep the answer concise.\"\n","    \"\\n\\n\"\n","    \"{context}\"\n",")\n","\n","qa_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", system_prompt),\n","        MessagesPlaceholder(variable_name=\"chat_history\"),\n","        (\"human\", \"{input}\"),\n","    ]\n",")\n","\n","\n","question_answer_chain = create_stuff_documents_chain(model, qa_prompt)"],"metadata":{"id":"mLrq5hK98k6-","executionInfo":{"status":"ok","timestamp":1733305196866,"user_tz":-330,"elapsed":389,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["### **Step 3: Create Rag-Chain using history_aware_retriever and question_answer_chain**"],"metadata":{"id":"JTiKdHXIFMFf"}},{"cell_type":"code","source":["# Create Rag-Chain using history_aware_retriever and question_answer_chain:\n","\n","rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"],"metadata":{"id":"B3ckSWsK8k9X","executionInfo":{"status":"ok","timestamp":1733305200482,"user_tz":-330,"elapsed":656,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["### **Step 4: Use the Memory and Session to store the current conversation**\n","\n","* ChatMessageHistory,\n","* BaseChatMessageHistory,\n","* RunnableWithMessageHistory"],"metadata":{"id":"t_kgwd8UD5ro"}},{"cell_type":"code","source":["from langchain_community.chat_message_histories import ChatMessageHistory\n","from langchain_core.chat_history import BaseChatMessageHistory\n","from langchain_core.runnables.history import RunnableWithMessageHistory\n"],"metadata":{"id":"XCXDJ48VCKWw","executionInfo":{"status":"ok","timestamp":1733305415123,"user_tz":-330,"elapsed":404,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":["#### **Create a Session to Store Conversation:**"],"metadata":{"id":"LrhdFNGZFhfx"}},{"cell_type":"code","source":["store = {}"],"metadata":{"id":"_SgBEI2XDTM3","executionInfo":{"status":"ok","timestamp":1733305485628,"user_tz":-330,"elapsed":392,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["def get_session_history(session_id: str) -> BaseChatMessageHistory:\n","    if session_id not in store:\n","        store[session_id] = ChatMessageHistory()\n","    return store[session_id]"],"metadata":{"id":"gmpHE39xDTPP","executionInfo":{"status":"ok","timestamp":1733305486066,"user_tz":-330,"elapsed":2,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":["#### **Create Conversational Rag Chain using memory and rag_chain**"],"metadata":{"id":"Eh6pddgOFmek"}},{"cell_type":"code","source":["# conversational_rag_chain:\n","\n","conversational_rag_chain = RunnableWithMessageHistory(\n","    rag_chain,\n","    get_session_history,\n","    input_messages_key=\"input\",\n","    history_messages_key=\"chat_history\",\n","    output_messages_key=\"answer\",\n",")"],"metadata":{"id":"893XXxu9DTRj","executionInfo":{"status":"ok","timestamp":1733305487005,"user_tz":-330,"elapsed":2,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}}},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":["#### **Generate Response and store the current user session:**"],"metadata":{"id":"clEAQzaVFuIp"}},{"cell_type":"code","source":["conversational_rag_chain.invoke(\n","    {\"input\": \"What is Chain of Thought Prompt?\"},\n","    config={\n","        \"configurable\": {\"session_id\": \"abc123\"}\n","    },  # constructs a key \"abc123\" in `store`.\n",")[\"answer\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"qpCPifn2DTTt","executionInfo":{"status":"ok","timestamp":1733305513620,"user_tz":-330,"elapsed":2471,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"0f3b87ea-057d-41fe-e78b-f200828e02b5"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Chain-of-Thought (CoT) prompting encourages large language models to generate intermediate reasoning steps before arriving at a final answer.  This is done by providing a few examples of question-answer pairs with reasoning steps included in the prompt.  CoT improves reasoning and performance on complex tasks.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["store"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"alswj8llDnVG","executionInfo":{"status":"ok","timestamp":1733305586857,"user_tz":-330,"elapsed":448,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"400cf23d-ecb7-45d4-d8d2-2a529c925849"},"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'abc123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is Chain of Thaught Prompt?', additional_kwargs={}, response_metadata={}), AIMessage(content='Chain-of-Thought (CoT) prompting encourages large language models to generate intermediate reasoning steps before arriving at a final answer.  This is done by providing a few examples of question-answer pairs with reasoning steps included in the prompt.  CoT improves reasoning and performance on complex tasks.\\n', additional_kwargs={}, response_metadata={})])}"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["conversational_rag_chain.invoke(\n","    {\"input\": \"What are common ways of doing it?\"},\n","    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",")[\"answer\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"I4L15UNEEEyd","executionInfo":{"status":"ok","timestamp":1733305613895,"user_tz":-330,"elapsed":3213,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"f4d334ad-3ad2-4730-ee47-28b4c942aacf"},"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'One common way is to simply add \"Let\\'s think step by step\" to the prompt.  More generally, providing a few examples of question-answer pairs that include the reasoning steps in the prompt can elicit this behavior.  There are also more advanced techniques like Tree of Thoughts, which explores multiple reasoning possibilities.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["store"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Grcl9bKqEK8a","executionInfo":{"status":"ok","timestamp":1733305626193,"user_tz":-330,"elapsed":471,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"db88b2d0-9c3d-4ee6-a6d3-c41b95bcb8ec"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'abc123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is Chain of Thaught Prompt?', additional_kwargs={}, response_metadata={}), AIMessage(content='Chain-of-Thought (CoT) prompting encourages large language models to generate intermediate reasoning steps before arriving at a final answer.  This is done by providing a few examples of question-answer pairs with reasoning steps included in the prompt.  CoT improves reasoning and performance on complex tasks.\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content='What are common ways of doing it?', additional_kwargs={}, response_metadata={}), AIMessage(content='One common way is to simply add \"Let\\'s think step by step\" to the prompt.  More generally, providing a few examples of question-answer pairs that include the reasoning steps in the prompt can elicit this behavior.  There are also more advanced techniques like Tree of Thoughts, which explores multiple reasoning possibilities.\\n', additional_kwargs={}, response_metadata={})])}"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["conversational_rag_chain.invoke(\n","    {\"input\": \"What are the questions I asked before?\"},\n","    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",")[\"answer\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"F6boGgvgELeS","executionInfo":{"status":"ok","timestamp":1733305662073,"user_tz":-330,"elapsed":2125,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"d3caf0e4-af90-422d-a5c8-18d8c15ad3d2"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'You asked about Chain of Thought prompting, specifically what it is and common ways to implement it.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":[],"metadata":{"id":"632Ivt7lELgn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for message in store[\"abc123\"].messages:\n","    if isinstance(message, AIMessage):\n","        prefix = \"AI\"\n","    else:\n","        prefix = \"User\"\n","\n","    print(f\"{prefix}: {message.content}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sU-I9DAeELit","executionInfo":{"status":"ok","timestamp":1733305701151,"user_tz":-330,"elapsed":409,"user":{"displayName":"Dibyendu Biswas.","userId":"13544031185281536447"}},"outputId":"155490e2-75a4-44b4-e440-16160c1400c3"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["User: What is Chain of Thaught Prompt?\n","\n","AI: Chain-of-Thought (CoT) prompting encourages large language models to generate intermediate reasoning steps before arriving at a final answer.  This is done by providing a few examples of question-answer pairs with reasoning steps included in the prompt.  CoT improves reasoning and performance on complex tasks.\n","\n","\n","User: What are common ways of doing it?\n","\n","AI: One common way is to simply add \"Let's think step by step\" to the prompt.  More generally, providing a few examples of question-answer pairs that include the reasoning steps in the prompt can elicit this behavior.  There are also more advanced techniques like Tree of Thoughts, which explores multiple reasoning possibilities.\n","\n","\n","User: What are the questions I asked before?\n","\n","AI: You asked about Chain of Thought prompting, specifically what it is and common ways to implement it.\n","\n","\n"]}]},{"cell_type":"markdown","source":["## **Note:**\n","Create an simple bot application and use the user authentication method using Flask."],"metadata":{"id":"1KQGltdoGAkW"}},{"cell_type":"code","source":[],"metadata":{"id":"sPMG076mEg7c"},"execution_count":null,"outputs":[]}]}